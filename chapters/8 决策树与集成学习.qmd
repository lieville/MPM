---
title: "决策树与集成学习"
format: html
editor: visual
---

## 本章导读

在前面的学习中，我们分别建立了回归分析框架（针对连续型响应变量）和分类分析框架（针对类别型响应变量）。决策树方法为这两类问题提供了统一的建模视角，它通过直观的树状结构将复杂的预测问题分解为一系列简单的决策规则。集成学习则进一步通过模型组合的策略，显著提升了单一模型的预测性能。本章将从回归和分类的双重角度，系统阐述决策树方法的理论基础和实践应用。

## 8.1 决策树的基本框架

### 8.1.1 统一视角下的树模型

决策树的核心思想是通过递归地划分特征空间来构建预测模型，这种思想既适用于回归问题，也适用于分类问题。

树结构的通用组成： - 根节点：包含完整的训练数据集 - 内部节点：基于某个特征的决策规则 - 分支：决策规则的可能结果 - 叶节点：最终的预测输出

关键区别： - 回归树：叶节点输出连续数值预测 - 分类树：叶节点输出类别标签或概率

### 8.1.2 特征空间的划分策略

决策树通过轴平行分割将特征空间划分为矩形区域，每个区域对应一个叶节点。

数学表述： 对于 $p$ 维特征空间，决策树构建了一个划分： $$
R_1, R_2, \ldots, R_M \quad \text{满足} \quad \bigcup_{m=1}^M R_m = \mathbb{R}^p
$$

其中每个区域 $R_m$ 对应树的一个叶节点。

## 8.2 回归树：连续响应的建模

### 8.2.1 回归树的预测机制

叶节点预测值： 对于落入区域 $R_m$ 的观测，回归树给出该区域内所有训练样本响应值的均值作为预测： $$
\hat{y}_{R_m} = \frac{1}{N_m} \sum_{x_i \in R_m} y_i = \bar{y}_{R_m}
$$

其中 $N_m$ 是区域 $R_m$ 中包含的训练样本数。

模型表示： 回归树模型可以表示为： $$
f(x) = \sum_{m=1}^M \bar{y}_{R_m} \cdot I(x \in R_m)
$$

### 8.2.2 回归树的分裂准则

目标函数： 选择分裂特征 $j$ 和分裂点 $s$ 来最小化加权平方误差： $$
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \right]
$$

其中： - $R_1(j,s) = \{X | X_j \leq s\}$（左子节点） - $R_2(j,s) = \{X | X_j > s\}$（右子节点） - $c_1, c_2$ 分别是左右子节点的最优预测值（即样本均值）

分裂增益： 分裂带来的平方误差减少量为： $$
\Delta SSE = SSE_{\text{parent}} - (SSE_{\text{left}} + SSE_{\text{right}})
$$

其中 $SSE = \sum (y_i - \bar{y})^2$

### 8.2.3 回归树的复杂度控制

预剪枝策略： - 最大树深度 - 叶节点最小样本数 - 分裂所需最小改进量

后剪枝策略： 代价复杂度剪枝，最小化： $$
C_\alpha(T) = \sum_{m=1}^{|T|} N_m \cdot MSE(R_m) + \alpha |T|
$$

其中 $MSE(R_m) = \frac{1}{N_m} \sum_{x_i \in R_m} (y_i - \bar{y}_{R_m})^2$

## 8.3 分类树：类别响应的建模

### 8.3.1 分类树的预测机制

叶节点预测： 对于落入区域 $R_m$ 的观测，分类树给出该区域内最频繁的类别： $$
\hat{y}_{R_m} = \arg\max_k \left( \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k) \right)
$$

或者输出类别概率： $$
\hat{p}_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)
$$

### 8.3.2 分类树的不纯度度量

不纯度度量用于评估节点内类别的混杂程度，理想的不纯度函数应满足： 1. 在均匀分布时取得最大值 2. 在纯节点时取得最小值（0） 3. 是对称的凹函数

Gini不纯度： $$
\text{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^K p_k^2
$$

-   直观解释：从节点中随机抽取两个样本，它们属于不同类别的概率
-   取值范围：\[0, 1-1/K\]
-   对类别分布变化敏感

信息熵： $$
\text{Entropy}(p) = -\sum_{k=1}^K p_k \log p_k
$$

-   基于信息论的概念
-   在机器学习中广泛使用
-   计算相对复杂

误分类误差： $$
\text{Error}(p) = 1 - \max_k p_k
$$

-   直观但不够敏感
-   不适合作为分裂准则，常用于剪枝

### 8.3.3 分类树的分裂准则

不纯度减少量： 选择使子节点不纯度加权和减少最多的分裂： $$
\Delta I = I(\text{parent}) - \left( \frac{N_{\text{left}}}{N} I(\text{left}) + \frac{N_{\text{right}}}{N} I(\text{right}) \right)
$$

其中 $I(\cdot)$ 是选择的不纯度函数。

信息增益（使用熵不纯度时）： $$
\text{Information Gain} = \text{Entropy}(p) - \sum_{j=1}^2 \frac{N_j}{N} \text{Entropy}(p_j)
$$

## 8.4 树模型的比较与诊断

### 8.4.1 回归树与分类树的统一视角

共同特点： 1. 非参数方法：不对数据分布做强假设 2. 特征选择：自动选择重要特征 3. 处理混合类型数据：能同时处理数值型和类别型特征 4. 稳健性：对异常值和缺失值相对稳健

主要差异：

| 方面       | 回归树              | 分类树                 |
|------------|---------------------|------------------------|
| 响应变量   | 连续型              | 类别型                 |
| 叶节点预测 | 均值                | 众数或概率             |
| 分裂准则   | 平方误差减少        | 不纯度减少             |
| 模型评估   | MSE, R²             | 准确率, AUC            |
| 复杂度控制 | 基于MSE的代价复杂度 | 基于不纯度的代价复杂度 |

### 8.4.2 树模型的优势与局限

主要优势： 1. 直观易懂：决策规则可解释性强 2. 无需预处理：对特征缩放不敏感 3. 处理非线性：自动捕捉变量间的交互效应 4. 变量重要性：提供特征重要性的自然度量

主要局限： 1. 不稳定性：数据微小变化可能导致树结构巨大改变 2. 贪婪性：每一步选择局部最优，未必全局最优 3. 预测面不光滑：分段常数预测 4. 外推能力差：无法预测训练数据范围外的值

## 8.5 集成学习理论基础

### 8.5.1 集成学习的统计原理

集成学习通过组合多个基学习器来改善预测性能，其理论基础可以从偏差-方差分解的角度理解。

回归问题的偏差-方差分解： $$
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2(\hat{f}(x)) + \text{Var}(\hat{f}(x)) + \sigma_\varepsilon^2
$$

分类问题的误差分解： 虽然分类问题没有精确的偏差-方差分解，但类似概念仍然适用： - 偏差：模型系统性错误 - 方差：模型对训练数据变化的敏感性

### 8.5.2 集成方法的分类

根据基学习器的生成方式和组合策略，集成方法主要分为：

1.  并行方法：基学习器独立生成，如Bagging、随机森林
2.  序列方法：基学习器顺序生成，如Boosting
3.  异质集成：组合不同类型的学习器，如Stacking

## 8.6 Bagging与随机森林

### 8.6.1 Bagging的回归应用

Bootstrap采样： 从训练集中有放回地抽取 $B$ 个自助样本，每个样本大小与原始训练集相同。

回归预测聚合： $$
\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x)
$$

方差减少机制： 假设基学习器方差为 $\sigma^2$，相关系数为 $\rho$，则Bagging集成方差为： $$
\text{Var}(\hat{f}_{\text{bag}}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2
$$

### 8.6.2 Bagging的分类应用

多数投票： $$
\hat{y}_{\text{bag}}(x) = \arg\max_k \sum_{b=1}^B I(\hat{f}^{*b}(x) = k)
$$

概率平均： $$
\hat{p}_k(x) = \frac{1}{B} \sum_{b=1}^B \hat{p}^{*b}_k(x)
$$

### 8.6.3 随机森林的改进

随机森林在Bagging的基础上引入特征随机选择，进一步降低基学习器间的相关性。

特征采样策略： - 分类问题：$m \approx \sqrt{p}$ - 回归问题：$m \approx p/3$

特征重要性： 基于不纯度减少的平均值： $$
\text{Importance}(X_j) = \frac{1}{B} \sum_{b=1}^B \sum_{t \in T_b} \Delta I(t, X_j)
$$

## 8.7 Boosting方法

### 8.7.1 回归问题的梯度提升

加性模型： $$
f_M(x) = \sum_{m=1}^M \beta_m h_m(x)
$$

梯度下降视角： 在函数空间中进行梯度下降，每一步拟合当前模型的负梯度： $$
r_{im} = -\left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f=f_{m-1}}
$$

对于平方损失： $$
r_{im} = y_i - f_{m-1}(x_i)
$$

算法步骤： 1. 初始化 $f_0(x) = \bar{y}$ 2. 对于 $m = 1$ 到 $M$： - 计算伪残差 $r_{im} = y_i - f_{m-1}(x_i)$ - 用决策树拟合 $\{(x_i, r_{im})\}$ 得到 $h_m(x)$ - 更新 $f_m(x) = f_{m-1}(x) + \nu \cdot h_m(x)$

其中 $\nu$ 是学习率。

### 8.7.2 分类问题的AdaBoost

AdaBoost通过调整样本权重来关注难以分类的样本。

指数损失函数： $$
L(y, f(x)) = \exp(-y f(x))
$$

其中 $y \in \{-1, +1\}$

算法流程： 1. 初始化权重 $w_i = 1/N$ 2. 对于 $m = 1$ 到 $M$： - 训练弱分类器 $G_m(x)$ 最小化加权误差 - 计算误差率 $\text{err}_m = \frac{\sum_{i=1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^N w_i}$ - 计算分类器权重 $\alpha_m = \frac{1}{2} \log\left(\frac{1-\text{err}_m}{\text{err}_m}\right)$ - 更新样本权重 $w_i \leftarrow w_i \exp(-\alpha_m y_i G_m(x_i))$ - 权重归一化 3. 输出最终分类器 $\text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x)\right)$

## 8.8 集成学习的高级应用

### 8.8.1 回归与分类的性能差异

回归问题的集成： - 主要降低方差 - 对基学习器的准确性要求相对较低 - 容易实现预测区间估计

分类问题的集成： - 同时影响偏差和方差 - 对基学习器的多样性要求更高 - 类别不平衡时需要特殊处理

### 8.8.2 模型解释性工具

特征重要性： 无论回归还是分类，都可以通过特征在集成中的总不纯度减少来评估重要性。

部分依赖图： 显示某个特征对预测值的边际效应，适用于回归和概率预测。

SHAP值： 统一框架下的特征贡献度分析，为每个预测提供可解释的分解。

## 8.9 案例分析

## 本章总结

核心公式回顾

1.  回归树预测：$\hat{y}_{R_m} = \frac{1}{N_m} \sum_{x_i \in R_m} y_i$
2.  分类树Gini不纯度：$\text{Gini}(p) = 1 - \sum_{k=1}^K p_k^2$
3.  Bagging回归：$\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x)$
4.  梯度提升伪残差：$r_{im} = y_i - f_{m-1}(x_i)$
5.  AdaBoost分类器权重：$\alpha_m = \frac{1}{2} \log\left(\frac{1-\text{err}_m}{\text{err}_m}\right)$

回归与分类的对比总结

| 方面     | 回归树        | 分类树                 |
|----------|---------------|------------------------|
| 响应变量 | 连续数值      | 离散类别               |
| 预测输出 | 叶节点均值    | 叶节点众数或概率       |
| 损失函数 | 平方误差      | Gini/熵不纯度          |
| 集成目标 | 主要降低方差  | 平衡偏差与方差         |
| 评估指标 | MSE, RMSE, R² | 准确率, 精确率, 召回率 |

方法选择指南

| 问题类型     | 推荐方法   | 关键考虑                 |
|--------------|------------|--------------------------|
| 需要强解释性 | 单棵决策树 | 深度限制，剪枝强度       |
| 回归预测精度 | 梯度提升树 | 学习率，树深度，迭代次数 |
| 分类平衡数据 | 随机森林   | 树数量，特征采样比例     |
| 类别不平衡   | AdaBoost   | 关注困难样本的能力       |
| 计算效率优先 | Bagging    | 可并行化，内存需求       |

重要概念体系

1.  递归划分：通过特征测试构建层次决策结构
2.  不纯度度量：量化节点内响应变量的混杂程度
3.  模型集成：通过组合多个基学习器提升预测性能
4.  偏差-方差权衡：在不同集成方法中的差异化表现
5.  特征重要性：基于树模型的特征贡献度评估
6.  稳健预测：通过模型平均降低方差和过拟合风险

决策树和集成学习方法为回归和分类问题提供了强大而灵活的解决方案。从单一树的直观解释到集成方法的高精度预测，这一方法体系涵盖了从探索性分析到生产部署的全流程需求。理解这些方法在回归和分类背景下的共性与差异，有助于在实际问题中选择最适合的技术路线。
