---
title: "11 支持向量机"
format: html
editor: source
---

## 本章导读

支持向量机（Support Vector Machine, SVM）是一类强大的监督学习算法，在线性分类和回归问题中表现出色。本章将从几何直观和数学优化两个角度，系统介绍SVM的基本原理、核方法以及在实际问题中的应用。SVM的核心思想是通过寻找最优分离超平面来实现分类，并通过核技巧处理非线性问题。

## 11.1 线性支持向量分类器

### 11.1.1 最大间隔分类器

**基本概念**： 对于线性可分的二分类问题，存在无数个超平面可以将两类样本分开。最大间隔分类器选择那个具有最大间隔的超平面。

**数学表述**： 考虑训练数据 $\{(x_i, y_i)\}_{i=1}^n$，其中 $y_i \in \{-1, +1\}$。分离超平面可以表示为： $$
w^T x + b = 0
$$

**函数间隔**： 样本点 $(x_i, y_i)$ 到超平面的函数间隔定义为： $$
\hat{\gamma}_i = y_i(w^T x_i + b)
$$

**几何间隔**： 几何间隔是函数间隔除以权重向量的范数： $$
\gamma_i = \frac{y_i(w^T x_i + b)}{\|w\|} = \frac{\hat{\gamma}_i}{\|w\|}
$$

### 11.1.2 优化问题 formulation

**最大间隔优化问题**： 寻找超平面使得最小几何间隔最大化： $$
\begin{aligned}
&\max_{w,b} \gamma \\
&\text{满足 } \frac{y_i(w^T x_i + b)}{\|w\|} \geq \gamma, \quad i = 1, \ldots, n
\end{aligned}
$$

**等价形式**： 通过缩放约束，可以得到等价的凸优化问题： $$
\begin{aligned}
&\min_{w,b} \frac{1}{2} \|w\|^2 \\
&\text{满足 } y_i(w^T x_i + b) \geq 1, \quad i = 1, \ldots, n
\end{aligned}
$$

### 11.1.3 支持向量的概念

**支持向量**： 是那些恰好满足 $y_i(w^T x_i + b) = 1$ 的样本点，它们定义了间隔的边界，并完全决定了最优超平面。

**关键性质**： - 支持向量是距离分离超平面最近的样本点 - 移除非支持向量不会影响模型 - 支持向量的数量通常很少，使得SVM具有稀疏性

## 11.2 软间隔支持向量机

### 11.2.1 线性不可分情况

在实际问题中，数据往往不是线性可分的。软间隔SVM通过引入松弛变量来处理这种情况。

**松弛变量**： 对于每个样本引入 $\xi_i \geq 0$，允许某些样本违反间隔约束： $$
y_i(w^T x_i + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n
$$

**优化问题**： $$
\begin{aligned}
&\min_{w,b,\xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&\text{满足 } y_i(w^T x_i + b) \geq 1 - \xi_i, \quad i = 1, \ldots, n \\
&\xi_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

### 11.2.2 参数C的解释

**正则化参数**： - $C$ 控制间隔宽度与分类错误之间的权衡 - $C \to \infty$：硬间隔SVM，不允许任何分类错误 - $C \to 0$：最大化间隔，容忍更多分类错误

**实际选择**： $C$ 通常通过交叉验证选择，平衡模型的复杂度和训练误差。

### 11.2.3 损失函数视角

**合页损失**： 软间隔SVM等价于最小化合页损失： $$
\min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(w^T x_i + b))
$$

**与其他方法的比较**： - 合页损失对正确分类且置信度高的样本损失为0 - 与逻辑回归的交叉熵损失形成对比

## 11.3 对偶问题与核方法

### 11.3.1 拉格朗日对偶

**原始问题**： $$
\begin{aligned}
&\min_{w,b,\xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\
&\text{满足 } y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{aligned}
$$

**拉格朗日函数**： $$
L(w,b,\xi,\alpha,\mu) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i[y_i(w^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i \xi_i
$$

**对偶问题**： $$
\begin{aligned}
&\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
&\text{满足 } 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, n \\
&\sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

### 11.3.2 核技巧

**特征映射**： 通过非线性映射 $\phi: \mathcal{X} \to \mathcal{H}$ 将输入空间映射到高维特征空间。

**核函数**： 核函数 $K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle$ 计算特征空间中的内积，而无需显式计算特征映射。

**对偶问题的核化形式**： $$
\begin{aligned}
&\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) \\
&\text{满足 } 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

### 11.3.3 常用核函数

**线性核**： $$
K(x_i, x_j) = x_i^T x_j
$$

**多项式核**： $$
K(x_i, x_j) = (\gamma x_i^T x_j + r)^d
$$

**高斯径向基核**： $$
K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)
$$

**Sigmoid核**： $$
K(x_i, x_j) = \tanh(\gamma x_i^T x_j + r)
$$

## 11.4 支持向量回归

### 11.4.1 $\epsilon$-不敏感损失

**回归问题设定**： 对于回归问题 $y_i \in \mathbb{R}$，SVM的扩展称为支持向量回归。

$\epsilon$-不敏感损失： $$
L_\epsilon(y, f(x)) = \begin{cases}
0 & \text{if } |y - f(x)| \leq \epsilon \\
|y - f(x)| - \epsilon & \text{otherwise}
\end{cases}
$$

### 11.4.2 SVR优化问题

**原始问题**： $$
\begin{aligned}
&\min_{w,b,\xi,\xi^*} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*) \\
&\text{满足 } y_i - (w^T x_i + b) \leq \epsilon + \xi_i \\
&(w^T x_i + b) - y_i \leq \epsilon + \xi_i^* \\
&\xi_i, \xi_i^* \geq 0, \quad i = 1, \ldots, n
\end{aligned}
$$

**对偶问题**： $$
\begin{aligned}
&\max_{\alpha,\alpha^*} -\epsilon \sum_{i=1}^n (\alpha_i + \alpha_i^*) + \sum_{i=1}^n y_i (\alpha_i - \alpha_i^*) \\
&- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) K(x_i, x_j) \\
&\text{满足 } 0 \leq \alpha_i, \alpha_i^* \leq C, \quad \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0
\end{aligned}
$$

### 11.4.3 预测函数

**SVR预测**： $$
f(x) = \sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + b
$$

**支持向量**： 只有那些满足 $|\alpha_i - \alpha_i^*| > 0$ 的样本才是支持向量。

## 11.5 模型选择与评估

### 11.5.1 超参数调优

**关键超参数**： - 正则化参数 $C$ - 核参数（如RBF核的 $\gamma$） - $\epsilon$（对于SVR）

**网格搜索**： 在超参数空间中进行系统搜索，使用交叉验证评估性能。

**交叉验证策略**： - $k$-折交叉验证 - 分层交叉验证（用于分类） - 时间序列交叉验证（用于时间相关数据）

### 11.5.2 核函数选择

**核选择指南**：

| 数据类型   | 推荐核函数 | 理由                   |
|------------|------------|------------------------|
| 线性可分   | 线性核     | 简单，计算高效         |
| 文本数据   | 线性核     | 高维稀疏数据           |
| 非线性问题 | RBF核      | 通用性强，适应各种模式 |
| 先验知识   | 自定义核   | 利用领域特定知识       |

### 11.5.3 模型评估指标

**分类问题**： - 准确率、精确率、召回率、F1分数 - ROC曲线和AUC - 混淆矩阵

**回归问题**： - 均方误差（MSE） - 平均绝对误差（MAE） - $R^2$ 决定系数

## 11.6 SVM的扩展变体

### 11.6.1 多类SVM

**一对多方法**： 为每个类别训练一个二分类SVM，将该类与其他所有类别区分。

**一对一方法**： 为每对类别训练一个二分类SVM，然后通过投票决定最终类别。

**多类SVM的统一公式**： 直接扩展SVM到多类情况，使用单一优化问题。

### 11.6.2 概率输出

**Platt缩放**： 将SVM输出通过sigmoid函数转换为概率估计： $$
P(y = 1 | x) = \frac{1}{1 + \exp(A f(x) + B)}
$$

其中 $A, B$ 通过最大似然估计得到。

### 11.6.3 大规模SVM

**挑战**： 标准SVM算法的时间复杂度为 $O(n^3)$，空间复杂度为 $O(n^2)$，难以处理大规模数据。

**解决方案**： - 序列最小优化（SMO）算法 - 随机梯度下降 - 近似核方法

## 11.7 SVM与其他方法的比较

### 11.7.1 与逻辑回归的比较

**相似点**： - 都是线性分类器 - 都可以通过核技巧处理非线性问题 - 都有正则化项控制模型复杂度

**不同点**： - 损失函数：合页损失 vs 交叉熵损失 - 输出：决策函数 vs 概率估计 - 支持向量：稀疏解 vs 稠密解

### 11.7.2 与神经网络的比较

**优势**： - 理论基础坚实 - 全局最优解 - 对高维数据有效

**劣势**： - 核函数选择困难 - 大规模数据计算成本高 - 特征工程相对重要

## 11.8 案例分析

## 本章总结

核心公式回顾

1.  最大间隔优化：$\min_{w,b} \frac{1}{2} \|w\|^2$ s.t. $y_i(w^T x_i + b) \geq 1$
2.  软间隔SVM：$\min_{w,b,\xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i$
3.  对偶问题：$\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j K(x_i, x_j)$
4.  RBF核：$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
5.  SVR预测：$f(x) = \sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + b$

方法选择指南

| 问题类型     | 推荐方法  | 关键参数                  | 注意事项               |
|------------------|------------------|--------------------|------------------|
| 线性可分分类 | 硬间隔SVM | 无                        | 数据需严格线性可分     |
| 一般分类     | 软间隔SVM | $C$, 核参数               | 通过交叉验证选择参数   |
| 非线性分类   | 核SVM     | $C$, $\gamma$             | 小心过拟合             |
| 回归问题     | SVR       | $C$, $\epsilon$, $\gamma$ | $\epsilon$控制预测精度 |
| 大规模数据   | 线性SVM   | $C$                       | 使用随机梯度下降       |

重要概念体系

1.  **最大间隔原理**：基于几何间隔最大化的分类准则
2.  **支持向量**：决定分类边界的关键样本点
3.  **核技巧**：通过核函数隐式实现非线性映射
4.  **对偶理论**：将原问题转化为更易求解的对偶问题
5.  **软间隔**：通过松弛变量处理线性不可分情况
6.  $\epsilon$-**不敏感损失**：SVR中使用的稳健损失函数

实践指导原则

1.  **数据预处理**：SVM对特征缩放敏感，建议标准化数据
2.  **核选择**：从线性核开始，必要时升级到RBF核
3.  **参数调优**：使用网格搜索和交叉验证系统优化超参数
4.  **模型解释**：线性SVM的权重向量提供特征重要性
5.  **计算考虑**：对于大规模数据，考虑线性SVM或近似方法

支持向量机提供了坚实的理论基础和优秀的实践性能，特别适合中小规模的高维数据问题。理解SVM的几何直观和优化理论，有助于在实际应用中更好地使用和解释这一强大工具。