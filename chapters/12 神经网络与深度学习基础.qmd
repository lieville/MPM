---
title: "12 神经网络与深度学习基础"
format: html
editor: source
---

## 本章导读

神经网络是预测建模领域的重要方法，它通过模拟人脑神经元的工作方式，为回归和分类问题提供了统一的解决方案。本章将从预测建模的本质出发，介绍神经网络的基本原理、网络设计、训练方法，并简要探讨深度学习的概念，帮助理解这一强大工具在回归和分类任务中的应用。

## 12.1 神经网络：回归与分类的统一框架

### 12.1.1 从线性模型到神经网络

线性模型的回顾： - 线性回归：$y = w^T x + b$（连续输出） - 逻辑回归：$P(y=1|x) = \sigma(w^T x + b)$（概率输出）

神经网络的扩展思路： 单一神经元可以看作一个广义线性模型，通过组合多个神经元并引入非线性激活函数，神经网络能够学习更复杂的模式。

基本神经元模型： $$
\begin{aligned}
z &= w^T x + b \quad \text{(线性组合)} \\
a &= \sigma(z) \quad \text{(非线性变换)}
\end{aligned}
$$

### 12.1.2 为什么需要神经网络？

传统方法的局限性： - 线性模型只能捕捉线性关系 - 多项式回归需要手动设计特征组合 - 对复杂非线性模式建模能力有限

神经网络的优势： - 自动学习特征组合 - 通过层次化结构学习抽象特征 - 统一处理回归和分类问题

## 12.2 网络架构设计

### 12.2.1 输出层设计

回归问题的输出层： - 神经元数量：1个（单输出）或对应输出维度 - 激活函数：恒等函数（无激活） - 输出解释：连续数值预测

二分类问题的输出层： - 神经元数量：1个 - 激活函数：sigmoid - 输出解释：属于正类的概率

多分类问题的输出层： - 神经元数量：类别数K - 激活函数：softmax - 输出解释：每个类别的概率分布

### 12.2.2 隐藏层设计

隐藏层的作用： - 学习输入特征的中间表示 - 通过非线性变换增强模型表达能力 - 自动进行特征工程

网络深度与宽度： - 浅层网络：1-2个隐藏层，适合简单问题 - 深层网络：多个隐藏层，适合复杂模式 - 宽度：每层神经元数，影响模型容量

## 12.3 激活函数与损失函数

### 12.3.1 常用激活函数

Sigmoid函数： $$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$ - 将输出压缩到(0,1) - 适合输出层，直观的概率解释 - 梯度消失问题

ReLU函数： $$
\text{ReLU}(z) = \max(0, z)
$$ - 计算简单，训练高效 - 缓解梯度消失 - 现代神经网络的首选

Tanh函数： $$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$ - 输出范围(-1,1)，零中心化 - 比sigmoid有更好的梯度特性

### 12.3.2 损失函数选择

回归任务： 均方误差损失： $$
L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

二分类任务： 交叉熵损失： $$
L = -\frac{1}{n} \sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
$$

多分类任务： 多类交叉熵损失： $$
L = -\frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K y_{ik} \log(\hat{y}_{ik})
$$

## 12.4 神经网络训练

### 12.4.1 前向传播

计算过程： 从输入层开始，逐层计算直到输出层： $$
\begin{aligned}
z^{[l]} &= W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} &= \sigma^{[l]}(z^{[l]})
\end{aligned}
$$

预测输出： 最终层的激活值即为模型预测。

### 12.4.2 反向传播

梯度计算： 利用链式法则计算损失函数对每个参数的梯度： $$
\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial z^{[l]}} (a^{[l-1]})^T
$$

参数更新： 使用梯度下降算法更新参数： $$
W^{[l]} := W^{[l]} - \alpha \frac{\partial L}{\partial W^{[l]}}
$$

### 12.4.3 优化算法

批量梯度下降： 使用全部训练数据计算梯度，收敛稳定但计算量大。

小批量梯度下降： 折中方案，兼顾收敛速度和稳定性。

Adam优化器： 自适应学习率，结合动量项，实践中效果良好。

## 12.5 过拟合与正则化

### 12.5.1 过拟合问题

神经网络的特点： - 参数众多，模型容量大 - 容易过拟合训练数据 - 需要正则化技术控制复杂度

检测方法： - 训练误差持续下降，验证误差开始上升 - 学习曲线分析 - 早停法

### 12.5.2 正则化技术

L2正则化： 在损失函数中加入权重惩罚： $$
L_{\text{reg}} = L + \frac{\lambda}{2} \sum \|W\|^2
$$

Dropout： 训练时随机丢弃部分神经元，提高泛化能力。

数据增强： 通过变换生成更多训练样本。

## 12.6 深度学习简介

深度学习模型可以形式化地定义为：

$$
f(\mathbf{x}; \theta) = f_L(f_{L-1}(\cdots f_1(\mathbf{x}; \theta_1) \cdots; \theta_{L-1}); \theta_L)
$$

其中： - $\mathbf{x}$：输入向量 - $\theta = \{\theta_1, \theta_2, \dots, \theta_L\}$：模型参数 - $f_l$：第$l$层的变换函数 - $L$：网络总层数（深度）

与浅层学习的对比

| 特征         | 浅层学习     | 深度学习          |
|--------------|--------------|-------------------|
| **层数**     | 1-2层隐藏层  | 多层（通常\>3层） |
| **特征工程** | 需要手动设计 | 自动学习特征      |
| **数据需求** | 相对较少     | 大量数据          |
| **计算需求** | 相对较低     | 高计算需求        |
| **可解释性** | 较好         | 较差（黑箱问题）  |

### 12.6.2 卷积神经网络（CNN）

CNN的基本思想

卷积神经网络专门处理具有网格结构的数据（如图像），通过局部连接和权值共享大幅减少参数数量。

卷积操作

二维离散卷积： $$
S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i+m, j+n)K(m,n)
$$

其中$I$为输入图像，$K$为卷积核。

卷积层特性

1.  **局部连接**：每个神经元只连接输入图像的局部区域
2.  **权值共享**：同一卷积核在输入的不同位置使用相同权重
3.  **平移不变性**：能够检测输入任何位置的特征

CNN的基本组件

卷积层

输出特征图大小计算： $$
\text{输出高度} = \frac{H - F_H + 2P}{S} + 1
$$ $$
\text{输出宽度} = \frac{W - F_W + 2P}{S} + 1
$$

其中： - $H, W$：输入高度和宽度 - $F_H, F_W$：卷积核高度和宽度 - $P$：填充（padding）大小 - $S$：步长（stride）

池化层

1.  **最大池化**： $$
    \text{output} = \max(\text{window})
    $$

2.  **平均池化**： $$
    \text{output} = \text{mean}(\text{window})
    $$

池化作用：降维、减少过拟合、增加平移不变性。

全连接层

在CNN末端，将特征图展平后连接全连接层进行分类。

经典CNN架构

LeNet-5（1998）

-   输入：32×32灰度图像
-   结构：2个卷积层 + 2个池化层 + 3个全连接层
-   应用：手写数字识别

AlexNet（2012）

-   创新：ReLU激活函数、Dropout、数据增强
-   结构：5个卷积层 + 3个池化层 + 3个全连接层
-   成就：ImageNet竞赛冠军

VGGNet（2014）

-   特点：使用小卷积核（3×3），增加网络深度
-   VGG16：13个卷积层 + 3个全连接层
-   VGG19：16个卷积层 + 3个全连接层

ResNet（2015）

-   创新：残差连接，解决梯度消失问题
-   残差块： $$
    \mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
    $$
-   深度：ResNet-50, ResNet-101, ResNet-152

------------------------------------------------------------------------

### 12.6.3 循环神经网络（RNN）

序列数据处理

循环神经网络用于处理序列数据，具有时间维度上的记忆能力。

RNN基本结构

$$
\mathbf{h}_t = \phi(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$ $$
\mathbf{y}_t = \psi(\mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y)
$$

其中： - $\mathbf{h}_t$：时刻$t$的隐藏状态 - $\mathbf{x}_t$：时刻$t$的输入 - $\mathbf{y}_t$：时刻$t$的输出

不同类型的RNN

1.  **一对一**：标准神经网络
2.  **一对多**：图像描述生成
3.  **多对一**：情感分析
4.  **多对多（等长）**：词性标注
5.  **多对多（不等长）**：机器翻译

**长短期记忆网络（LSTM）**

LSTM通过门控机制解决长期依赖问题。

LSTM单元结构

1.  **遗忘门**： $$
    \mathbf{f}_t = \sigma(\mathbf{W}_f[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)
    $$

2.  **输入门**： $$
    \mathbf{i}_t = \sigma(\mathbf{W}_i[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)
    $$ $$
    \tilde{\mathbf{C}}_t = \tanh(\mathbf{W}_C[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_C)
    $$

3.  **细胞状态更新**： $$
    \mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t
    $$

4.  **输出门**： $$
    \mathbf{o}_t = \sigma(\mathbf{W}_o[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)
    $$ $$
    \mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{C}_t)
    $$

门控循环单元（GRU）

GRU是LSTM的简化版本，合并遗忘门和输入门： $$
\mathbf{z}_t = \sigma(\mathbf{W}_z[\mathbf{h}_{t-1}, \mathbf{x}_t])
$$ $$
\mathbf{r}_t = \sigma(\mathbf{W}_r[\mathbf{h}_{t-1}, \mathbf{x}_t])
$$ $$
\tilde{\mathbf{h}}_t = \tanh(\mathbf{W}[\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t])
$$ $$
\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
$$

### 12.6.5 注意力机制

缩放点积注意力

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

其中： - $Q$：查询矩阵 - $K$：键矩阵 - $V$：值矩阵 - $d_k$：键的维度

多头注意力

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
$$ $$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

**Transformer架构**

编码器-解码器结构

**编码器**（$N$层）： 1. 多头自注意力 2. 前馈神经网络 3. 残差连接 + 层归一化

**解码器**（$N$层）： 1. 掩码多头自注意力（防止看到未来信息） 2. 多头编码器-解码器注意力 3. 前馈神经网络 4. 残差连接 + 层归一化

位置编码

为序列添加位置信息： $$
PE_{(pos, 2i)} = \sin(pos/10000^{2i/d})
$$ $$
PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d})
$$

**BERT与GPT**

BERT（双向编码器表示）

-   预训练任务：掩码语言模型 + 下一句预测
-   特点：双向上下文理解
-   应用：文本分类、问答、命名实体识别

GPT（生成式预训练Transformer）

-   架构：仅解码器的Transformer
-   预训练：自回归语言建模
-   特点：强大的生成能力

## 12.7 案例分析

## 本章总结

核心概念回顾

1.  统一框架：神经网络为回归和分类提供统一建模框架
2.  架构设计：根据问题类型设计输出层，回归用线性输出，分类用sigmoid/softmax输出
3.  激活函数：ReLU适合隐藏层，根据任务选择输出层激活函数
4.  训练原理：前向传播计算预测，反向传播计算梯度，梯度下降更新参数
5.  正则化：使用L2正则化、Dropout等技术防止过拟合

实践应用指南

数据预处理要求： - 特征标准化：加速收敛，提高数值稳定性 - 数据量要求：神经网络通常需要较多训练数据 - 计算资源：深层网络需要较强的计算能力

与传统方法的选择：

| 考虑因素   | 选择传统方法 | 选择神经网络   |
|------------|--------------|----------------|
| 数据量     | 小样本       | 大样本         |
| 问题复杂度 | 简单线性关系 | 复杂非线性关系 |
| 可解释性   | 要求高       | 要求低         |
| 计算资源   | 有限         | 充足           |
| 特征工程   | 手动设计     | 自动学习       |

深度学习扩展

成功应用领域： - 计算机视觉：图像分类、目标检测 - 自然语言处理：文本分类、机器翻译 - 语音识别：语音转文本、声纹识别 - 推荐系统：个性化推荐

实践建议： 1. 从基础开始：先掌握浅层神经网络，再学习深度学习 2. 理解原理：不仅要会使用，更要理解背后的数学原理 3. 项目驱动：通过实际项目加深理解 4. 持续学习：深度学习领域发展迅速，需要持续跟进

与前面章节的联系

神经网络不是孤立的方法，而是前面学习内容的自然延伸：

-   单层神经网络 = 广义线性模型
-   多层神经网络 = 多个广义线性模型的堆叠+非线性变换
-   深度学习 = 更深层次的特征学习

通过本教材的学习，希望读者能够建立从传统统计方法到现代机器学习的完整知识体系，在实际问题中选择合适的建模方法。

*全书总结：从线性回归到神经网络，我们建立了一套完整的预测建模方法体系。每种方法都有其适用场景和局限性，在实际工作中需要根据具体问题选择合适的方法，理解其假设和限制，才能构建出有效的预测模型。*