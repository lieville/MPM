---
title: "神经网络与深度学习基础"
format: html
editor: source
---

## 本章导读
神经网络是预测建模领域的重要方法，它通过模拟人脑神经元的工作方式，为回归和分类问题提供了统一的解决方案。本章将从预测建模的本质出发，介绍神经网络的基本原理、网络设计、训练方法，并简要探讨深度学习的概念，帮助理解这一强大工具在回归和分类任务中的应用。

## 10.1 神经网络：回归与分类的统一框架

### 10.1.1 从线性模型到神经网络

线性模型的回顾：
- 线性回归：$y = w^T x + b$（连续输出）
- 逻辑回归：$P(y=1|x) = \sigma(w^T x + b)$（概率输出）

神经网络的扩展思路：
单一神经元可以看作一个广义线性模型，通过组合多个神经元并引入非线性激活函数，神经网络能够学习更复杂的模式。

基本神经元模型：
$$
\begin{aligned}
z &= w^T x + b \quad \text{(线性组合)} \\
a &= \sigma(z) \quad \text{(非线性变换)}
\end{aligned}
$$

### 10.1.2 为什么需要神经网络？

传统方法的局限性：
- 线性模型只能捕捉线性关系
- 多项式回归需要手动设计特征组合
- 对复杂非线性模式建模能力有限

神经网络的优势：
- 自动学习特征组合
- 通过层次化结构学习抽象特征
- 统一处理回归和分类问题

## 10.2 网络架构设计

### 10.2.1 输出层设计

回归问题的输出层：
- 神经元数量：1个（单输出）或对应输出维度
- 激活函数：恒等函数（无激活）
- 输出解释：连续数值预测

二分类问题的输出层：
- 神经元数量：1个
- 激活函数：sigmoid
- 输出解释：属于正类的概率

多分类问题的输出层：
- 神经元数量：类别数K
- 激活函数：softmax
- 输出解释：每个类别的概率分布

### 10.2.2 隐藏层设计

隐藏层的作用：
- 学习输入特征的中间表示
- 通过非线性变换增强模型表达能力
- 自动进行特征工程

网络深度与宽度：
- 浅层网络：1-2个隐藏层，适合简单问题
- 深层网络：多个隐藏层，适合复杂模式
- 宽度：每层神经元数，影响模型容量

## 10.3 激活函数与损失函数

### 10.3.1 常用激活函数

Sigmoid函数：
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
- 将输出压缩到(0,1)
- 适合输出层，直观的概率解释
- 梯度消失问题

ReLU函数：
$$
\text{ReLU}(z) = \max(0, z)
$$
- 计算简单，训练高效
- 缓解梯度消失
- 现代神经网络的首选

Tanh函数：
$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$
- 输出范围(-1,1)，零中心化
- 比sigmoid有更好的梯度特性

### 10.3.2 损失函数选择

回归任务：
均方误差损失：
$$
L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

二分类任务：
交叉熵损失：
$$
L = -\frac{1}{n} \sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
$$

多分类任务：
多类交叉熵损失：
$$
L = -\frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K y_{ik} \log(\hat{y}_{ik})
$$

## 10.4 神经网络训练

### 10.4.1 前向传播

计算过程：
从输入层开始，逐层计算直到输出层：
$$
\begin{aligned}
z^{[l]} &= W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} &= \sigma^{[l]}(z^{[l]})
\end{aligned}
$$

预测输出：
最终层的激活值即为模型预测。

### 10.4.2 反向传播

梯度计算：
利用链式法则计算损失函数对每个参数的梯度：
$$
\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial z^{[l]}} (a^{[l-1]})^T
$$

参数更新：
使用梯度下降算法更新参数：
$$
W^{[l]} := W^{[l]} - \alpha \frac{\partial L}{\partial W^{[l]}}
$$

### 10.4.3 优化算法

批量梯度下降：
使用全部训练数据计算梯度，收敛稳定但计算量大。

小批量梯度下降：
折中方案，兼顾收敛速度和稳定性。

Adam优化器：
自适应学习率，结合动量项，实践中效果良好。

## 10.5 过拟合与正则化

### 10.5.1 过拟合问题

神经网络的特点：
- 参数众多，模型容量大
- 容易过拟合训练数据
- 需要正则化技术控制复杂度

检测方法：
- 训练误差持续下降，验证误差开始上升
- 学习曲线分析
- 早停法

### 10.5.2 正则化技术

L2正则化：
在损失函数中加入权重惩罚：
$$
L_{\text{reg}} = L + \frac{\lambda}{2} \sum \|W\|^2
$$

Dropout：
训练时随机丢弃部分神经元，提高泛化能力。

数据增强：
通过变换生成更多训练样本。

## 10.6 深度学习简介

### 10.6.1 从神经网络到深度学习

深度学习定义：
具有多个隐藏层的神经网络，能够学习数据的层次化表示。

深度学习的优势：
- 自动学习特征层次
- 端到端学习
- 在图像、语音、文本等领域表现突出

### 10.6.2 常用深度学习架构

卷积神经网络(CNN)：
- 专为图像数据处理设计
- 卷积层提取局部特征
- 池化层降低维度
- 在计算机视觉任务中广泛应用

循环神经网络(RNN)：
- 处理序列数据
- 具有记忆功能
- 适用于时间序列分析、自然语言处理

## 10.7 案例分析

## 本章总结

核心概念回顾

1. 统一框架：神经网络为回归和分类提供统一建模框架
2. 架构设计：根据问题类型设计输出层，回归用线性输出，分类用sigmoid/softmax输出
3. 激活函数：ReLU适合隐藏层，根据任务选择输出层激活函数
4. 训练原理：前向传播计算预测，反向传播计算梯度，梯度下降更新参数
5. 正则化：使用L2正则化、Dropout等技术防止过拟合

实践应用指南

数据预处理要求：
- 特征标准化：加速收敛，提高数值稳定性
- 数据量要求：神经网络通常需要较多训练数据
- 计算资源：深层网络需要较强的计算能力

与传统方法的选择：

| 考虑因素 | 选择传统方法 | 选择神经网络 |
|---------|-------------|-------------|
| 数据量 | 小样本 | 大样本 |
| 问题复杂度 | 简单线性关系 | 复杂非线性关系 |
| 可解释性 | 要求高 | 要求低 |
| 计算资源 | 有限 | 充足 |
| 特征工程 | 手动设计 | 自动学习 |

深度学习扩展

成功应用领域：
- 计算机视觉：图像分类、目标检测
- 自然语言处理：文本分类、机器翻译
- 语音识别：语音转文本、声纹识别
- 推荐系统：个性化推荐

实践建议：
1. 从基础开始：先掌握浅层神经网络，再学习深度学习
2. 理解原理：不仅要会使用，更要理解背后的数学原理
3. 项目驱动：通过实际项目加深理解
4. 持续学习：深度学习领域发展迅速，需要持续跟进

与前面章节的联系

神经网络不是孤立的方法，而是前面学习内容的自然延伸：

- 单层神经网络 = 广义线性模型
- 多层神经网络 = 多个广义线性模型的堆叠+非线性变换
- 深度学习 = 更深层次的特征学习

通过本教材的学习，希望读者能够建立从传统统计方法到现代机器学习的完整知识体系，在实际问题中选择合适的建模方法。


*全书总结：从线性回归到神经网络，我们建立了一套完整的预测建模方法体系。每种方法都有其适用场景和局限性，在实际工作中需要根据具体问题选择合适的方法，理解其假设和限制，才能构建出有效的预测模型。*