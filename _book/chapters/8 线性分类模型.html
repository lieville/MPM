<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-Hans" xml:lang="zh-Hans"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8 线性分类模型 – 现代预测建模：从回归到机器学习</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/9 非线性回归与样条回归.html" rel="next">
<link href="../chapters/7 广义线性模型.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-70a47bd5681a7291082a5b9f83d58762.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="science-textbook.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-2-introduction.html">II 扩展——线性分析方法</a></li><li class="breadcrumb-item"><a href="../chapters/8 线性分类模型.html"><span class="chapter-title">8 线性分类模型</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">现代预测建模：从回归到机器学习</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">前言</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">index.html</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">I 基础-线性回归模型</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-1-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-1-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/1 预测模型与评估.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">1 预测模型与评估</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/2 线性回归.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">2 线性回归</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/3 模型诊断.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">3 模型诊断</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/4 时间序列分析初步.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">4 时间序列分析初步</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">II 扩展——线性分析方法</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-2-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-2-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/5 降维.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">5 降维</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/6 正则化.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">6 正则化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/7 广义线性模型.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">7 广义线性模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/8 线性分类模型.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">8 线性分类模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/9 非线性回归与样条回归.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">9 非线性回归与样条回归</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">III 进阶——超越线性的机器学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-3-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-3-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10 决策树与集成学习.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">10 决策树与集成学习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11 支持向量机.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11 支持向量机</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12 神经网络与深度学习基础.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12 神经网络与深度学习基础</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">IV 实践——综合案例分析</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-4-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-4-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13 回归任务与基准测试.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13 回归任务与基准测试</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14 分类任务.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14 分类任务</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15 案例综合分析.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15 案例综合分析</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#本章导读" id="toc-本章导读" class="nav-link active" data-scroll-target="#本章导读">本章导读</a></li>
  <li><a href="#线性判别分析与二次判别分析" id="toc-线性判别分析与二次判别分析" class="nav-link" data-scroll-target="#线性判别分析与二次判别分析">8.1 线性判别分析与二次判别分析</a>
  <ul class="collapse">
  <li><a href="#lda的几何解释" id="toc-lda的几何解释" class="nav-link" data-scroll-target="#lda的几何解释">8.1.2 LDA的几何解释</a></li>
  </ul></li>
  <li><a href="#二次判别分析qda" id="toc-二次判别分析qda" class="nav-link" data-scroll-target="#二次判别分析qda">8.2 二次判别分析（QDA）</a>
  <ul class="collapse">
  <li><a href="#lda与qda的比较" id="toc-lda与qda的比较" class="nav-link" data-scroll-target="#lda与qda的比较">8.2.2 LDA与QDA的比较</a></li>
  <li><a href="#正则化判别分析rda" id="toc-正则化判别分析rda" class="nav-link" data-scroll-target="#正则化判别分析rda">8.2.4 正则化判别分析（RDA）</a></li>
  </ul></li>
  <li><a href="#朴素贝叶斯分类器" id="toc-朴素贝叶斯分类器" class="nav-link" data-scroll-target="#朴素贝叶斯分类器">8.3 朴素贝叶斯分类器</a>
  <ul class="collapse">
  <li><a href="#不同特征类型的概率估计" id="toc-不同特征类型的概率估计" class="nav-link" data-scroll-target="#不同特征类型的概率估计">8.3.2不同特征类型的概率估计</a></li>
  <li><a href="#算法实现" id="toc-算法实现" class="nav-link" data-scroll-target="#算法实现">8.3.3 算法实现</a></li>
  <li><a href="#与判别分析的比较" id="toc-与判别分析的比较" class="nav-link" data-scroll-target="#与判别分析的比较">8.3.4 与判别分析的比较</a></li>
  </ul></li>
  <li><a href="#模型比较与选择" id="toc-模型比较与选择" class="nav-link" data-scroll-target="#模型比较与选择">8.5 模型比较与选择</a>
  <ul class="collapse">
  <li><a href="#分类性能评估" id="toc-分类性能评估" class="nav-link" data-scroll-target="#分类性能评估">8.5.1 分类性能评估</a></li>
  <li><a href="#roc曲线与auc" id="toc-roc曲线与auc" class="nav-link" data-scroll-target="#roc曲线与auc">8.5.2 ROC曲线与AUC</a></li>
  <li><a href="#方法比较" id="toc-方法比较" class="nav-link" data-scroll-target="#方法比较">8.5.3 方法比较</a></li>
  <li><a href="#模型诊断" id="toc-模型诊断" class="nav-link" data-scroll-target="#模型诊断">8.5.4 模型诊断</a></li>
  </ul></li>
  <li><a href="#案例分析" id="toc-案例分析" class="nav-link" data-scroll-target="#案例分析">8.6 案例分析</a></li>
  <li><a href="#本章总结" id="toc-本章总结" class="nav-link" data-scroll-target="#本章总结">本章总结</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-2-introduction.html">II 扩展——线性分析方法</a></li><li class="breadcrumb-item"><a href="../chapters/8 线性分类模型.html"><span class="chapter-title">8 线性分类模型</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">8 线性分类模型</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="本章导读" class="level2">
<h2 class="anchored" data-anchor-id="本章导读">本章导读</h2>
<p>把线性分析进行扩展</p>
</section>
<section id="线性判别分析与二次判别分析" class="level2">
<h2 class="anchored" data-anchor-id="线性判别分析与二次判别分析">8.1 线性判别分析与二次判别分析</h2>
<p>判别分析（Discriminant Analysis）是一种经典的统计分类方法，主要用于解决分类问题和降维问题。其核心思想是寻找能够最佳区分不同类别的特征组合。</p>
<p>假设我们有： - <span class="math inline">\(K\)</span>个类别：<span class="math inline">\(C_1, C_2, \dots, C_K\)</span> - <span class="math inline">\(p\)</span>维特征向量：<span class="math inline">\(\mathbf{x} = (x_1, x_2, \dots, x_p)^\top\)</span> - 训练数据集：<span class="math inline">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span>，其中<span class="math inline">\(y_i \in \{1, 2, \dots, K\}\)</span></p>
<p>判别分析的目标是基于贝叶斯定理构建分类规则：</p>
<p><span class="math display">\[
P(Y=k | \mathbf{x}) = \frac{\pi_k f_k(\mathbf{x})}{\sum_{l=1}^K \pi_l f_l(\mathbf{x})}
\]</span></p>
<p>其中： - <span class="math inline">\(\pi_k = P(Y=k)\)</span>：类别<span class="math inline">\(k\)</span>的先验概率 - <span class="math inline">\(f_k(\mathbf{x})\)</span>：类别<span class="math inline">\(k\)</span>中<span class="math inline">\(\mathbf{x}\)</span>的概率密度函数</p>
<p>基本假设</p>
<p>LDA基于以下关键假设：</p>
<ol type="1">
<li><p>多元正态性：每个类别的特征向量服从多元正态分布 <span class="math display">\[
\mathbf{x} | Y=k \sim N_p(\pmb{\mu}_k, \pmb{\Sigma})
\]</span></p></li>
<li><p>同方差性：所有类别共享相同的协方差矩阵<span class="math inline">\(\pmb{\Sigma}\)</span></p></li>
<li><p>协方差矩阵非奇异：<span class="math inline">\(\pmb{\Sigma}\)</span>是可逆的</p></li>
</ol>
<p>判别函数推导</p>
<p>根据多元正态分布的概率密度函数：</p>
<p><span class="math display">\[
f_k(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\pmb{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\pmb{\mu}_k)^\top \pmb{\Sigma}^{-1} (\mathbf{x}-\pmb{\mu}_k)\right)
\]</span></p>
<p>代入贝叶斯公式，忽略常数项，得到判别函数：</p>
<p><span class="math display">\[
\begin{aligned}
\delta_k(\mathbf{x}) &amp;= \log(\pi_k) + \log(f_k(\mathbf{x})) \\
&amp;= \log(\pi_k) - \frac{1}{2}\pmb{\mu}_k^\top \pmb{\Sigma}^{-1}\pmb{\mu}_k + \mathbf{x}^\top \pmb{\Sigma}^{-1}\pmb{\mu}_k - \frac{1}{2}\mathbf{x}^\top \pmb{\Sigma}^{-1}\mathbf{x}
\end{aligned}
\]</span></p>
<p>由于<span class="math inline">\(\mathbf{x}^\top \pmb{\Sigma}^{-1}\mathbf{x}\)</span>与<span class="math inline">\(k\)</span>无关，可以简化为线性判别函数：</p>
<p><span class="math display">\[
\delta_k(\mathbf{x}) = \mathbf{x}^\top \pmb{\Sigma}^{-1}\pmb{\mu}_k - \frac{1}{2}\pmb{\mu}_k^\top \pmb{\Sigma}^{-1}\pmb{\mu}_k + \log(\pi_k)
\]</span></p>
<p>分类规则</p>
<p>将观测<span class="math inline">\(\mathbf{x}\)</span>分配到使判别函数最大的类别：</p>
<p><span class="math display">\[
\hat{y}(\mathbf{x}) = \arg\max_{k=1,\dots,K} \delta_k(\mathbf{x})
\]</span></p>
<p>决策边界是线性的，因为判别函数是<span class="math inline">\(\mathbf{x}\)</span>的线性函数。</p>
<p>参数估计</p>
<p>从训练数据中估计参数：</p>
<ol type="1">
<li><p>先验概率： <span class="math display">\[
\hat{\pi}_k = \frac{n_k}{n}, \quad n_k = \sum_{i=1}^n I(y_i = k)
\]</span></p></li>
<li><p>均值向量： <span class="math display">\[
\hat{\pmb{\mu}}_k = \frac{1}{n_k} \sum_{i: y_i = k} \mathbf{x}_i
\]</span></p></li>
<li><p>协方差矩阵： <span class="math display">\[
\hat{\pmb{\Sigma}} = \frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i = k} (\mathbf{x}_i - \hat{\pmb{\mu}}_k)(\mathbf{x}_i - \hat{\pmb{\mu}}_k)^\top
\]</span> 这是合并协方差矩阵（pooled covariance matrix）。</p></li>
</ol>
<section id="lda的几何解释" class="level3">
<h3 class="anchored" data-anchor-id="lda的几何解释">8.1.2 LDA的几何解释</h3>
<p>马氏距离</p>
<p>LDA的分类规则等价于将<span class="math inline">\(\mathbf{x}\)</span>分配到具有最小马氏距离（Mahalanobis distance）的类别：</p>
<p><span class="math display">\[
D_k^2(\mathbf{x}) = (\mathbf{x} - \pmb{\mu}_k)^\top \pmb{\Sigma}^{-1} (\mathbf{x} - \pmb{\mu}_k)
\]</span></p>
<p>考虑先验概率调整后的距离：</p>
<p><span class="math display">\[
D_k^2(\mathbf{x}) - 2\log(\pi_k)
\]</span></p>
<p>降维视角</p>
<p>LDA可以视为寻找最佳投影方向以最大化类间散度与类内散度的比值：</p>
<p><span class="math display">\[
J(\mathbf{w}) = \frac{\mathbf{w}^\top \mathbf{S}_B \mathbf{w}}{\mathbf{w}^\top \mathbf{S}_W \mathbf{w}}
\]</span></p>
<p>其中： - 类间散度矩阵：<span class="math inline">\(\mathbf{S}_B = \sum_{k=1}^K n_k (\pmb{\mu}_k - \bar{\pmb{\mu}})(\pmb{\mu}_k - \bar{\pmb{\mu}})^\top\)</span> - 类内散度矩阵：<span class="math inline">\(\mathbf{S}_W = \sum_{k=1}^K \sum_{i: y_i = k} (\mathbf{x}_i - \pmb{\mu}_k)(\mathbf{x}_i - \pmb{\mu}_k)^\top\)</span> - 总体均值：<span class="math inline">\(\bar{\pmb{\mu}} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i\)</span></p>
<p>最优投影方向是<span class="math inline">\(\mathbf{S}_W^{-1}\mathbf{S}_B\)</span>的特征向量，最多有<span class="math inline">\(\min(p, K-1)\)</span>个有效判别方向。</p>
</section>
</section>
<section id="二次判别分析qda" class="level2">
<h2 class="anchored" data-anchor-id="二次判别分析qda">8.2 二次判别分析（QDA）</h2>
<p>基本假设</p>
<p>QDA放宽了LDA的同方差性假设：</p>
<ol type="1">
<li><p>多元正态性：每个类别的特征向量服从多元正态分布 <span class="math display">\[
\mathbf{x} | Y=k \sim N_p(\pmb{\mu}_k, \pmb{\Sigma}_k)
\]</span></p></li>
<li><p>异方差性：每个类别有自己的协方差矩阵<span class="math inline">\(\pmb{\Sigma}_k\)</span></p></li>
</ol>
<p>判别函数推导</p>
<p>对于多元正态分布，类别<span class="math inline">\(k\)</span>的概率密度函数为：</p>
<p><span class="math display">\[
f_k(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\pmb{\Sigma}_k|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\pmb{\mu}_k)^\top \pmb{\Sigma}_k^{-1} (\mathbf{x}-\pmb{\mu}_k)\right)
\]</span></p>
<p>取对数并忽略常数项，得到二次判别函数：</p>
<p><span class="math display">\[
\begin{aligned}
\delta_k(\mathbf{x}) &amp;= \log(\pi_k) - \frac{1}{2}\log|\pmb{\Sigma}_k| \\
&amp;\quad - \frac{1}{2}(\mathbf{x} - \pmb{\mu}_k)^\top \pmb{\Sigma}_k^{-1} (\mathbf{x} - \pmb{\mu}_k)
\end{aligned}
\]</span></p>
<p>分类规则</p>
<p>同样，将观测<span class="math inline">\(\mathbf{x}\)</span>分配到使判别函数最大的类别：</p>
<p><span class="math display">\[
\hat{y}(\mathbf{x}) = \arg\max_{k=1,\dots,K} \delta_k(\mathbf{x})
\]</span></p>
<p>由于判别函数包含<span class="math inline">\(\mathbf{x}\)</span>的二次项，决策边界是二次曲面（椭圆、双曲线或抛物线）。</p>
<p>参数估计</p>
<p>从训练数据中估计参数：</p>
<ol type="1">
<li><p>先验概率： <span class="math display">\[
\hat{\pi}_k = \frac{n_k}{n}
\]</span></p></li>
<li><p>均值向量： <span class="math display">\[
\hat{\pmb{\mu}}_k = \frac{1}{n_k} \sum_{i: y_i = k} \mathbf{x}_i
\]</span></p></li>
<li><p>协方差矩阵： <span class="math display">\[
\hat{\pmb{\Sigma}}_k = \frac{1}{n_k - 1} \sum_{i: y_i = k} (\mathbf{x}_i - \hat{\pmb{\mu}}_k)(\mathbf{x}_i - \hat{\pmb{\mu}}_k)^\top
\]</span></p></li>
</ol>
<p>QDA的二次项展开</p>
<p>将QDA的判别函数展开，可以更清楚地看到其二次本质：</p>
<p><span class="math display">\[
\begin{aligned}
\delta_k(\mathbf{x}) &amp;= \log(\pi_k) - \frac{1}{2}\log|\pmb{\Sigma}_k| - \frac{1}{2}\pmb{\mu}_k^\top \pmb{\Sigma}_k^{-1}\pmb{\mu}_k \\
&amp;\quad + \mathbf{x}^\top \pmb{\Sigma}_k^{-1}\pmb{\mu}_k - \frac{1}{2}\mathbf{x}^\top \pmb{\Sigma}_k^{-1}\mathbf{x}
\end{aligned}
\]</span></p>
<p>令： - <span class="math inline">\(C_k = \log(\pi_k) - \frac{1}{2}\log|\pmb{\Sigma}_k| - \frac{1}{2}\pmb{\mu}_k^\top \pmb{\Sigma}_k^{-1}\pmb{\mu}_k\)</span>（常数项） - <span class="math inline">\(\pmb{\beta}_k = \pmb{\Sigma}_k^{-1}\pmb{\mu}_k\)</span>（线性系数） - <span class="math inline">\(\pmb{\Omega}_k = -\frac{1}{2}\pmb{\Sigma}_k^{-1}\)</span>（二次系数）</p>
<p>则判别函数可写为：</p>
<p><span class="math display">\[
\delta_k(\mathbf{x}) = C_k + \mathbf{x}^\top \pmb{\beta}_k + \mathbf{x}^\top \pmb{\Omega}_k \mathbf{x}
\]</span></p>
<hr>
<section id="lda与qda的比较" class="level3">
<h3 class="anchored" data-anchor-id="lda与qda的比较">8.2.2 LDA与QDA的比较</h3>
<p>理论比较</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>LDA</th>
<th>QDA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>协方差矩阵</td>
<td><span class="math inline">\(\pmb{\Sigma}_k = \pmb{\Sigma}\)</span>（相同）</td>
<td><span class="math inline">\(\pmb{\Sigma}_k\)</span>（不同）</td>
</tr>
<tr class="even">
<td>判别函数</td>
<td>线性函数</td>
<td>二次函数</td>
</tr>
<tr class="odd">
<td>决策边界</td>
<td>线性超平面</td>
<td>二次曲面</td>
</tr>
<tr class="even">
<td>参数数量</td>
<td><span class="math inline">\(Kp + p(p+1)/2 + (K-1)\)</span></td>
<td><span class="math inline">\(Kp + K \cdot p(p+1)/2 + (K-1)\)</span></td>
</tr>
<tr class="odd">
<td>假设强度</td>
<td>较强</td>
<td>较弱</td>
</tr>
</tbody>
</table>
<p>参数复杂度分析</p>
<p>LDA的参数数量： - 均值向量：<span class="math inline">\(K \times p\)</span>个参数 - 协方差矩阵：<span class="math inline">\(p(p+1)/2\)</span>个参数 - 先验概率：<span class="math inline">\(K-1\)</span>个参数（因为<span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>）</p>
<p>总计：<span class="math inline">\(Kp + \frac{p(p+1)}{2} + (K-1)\)</span></p>
<p>QDA的参数数量： - 均值向量：<span class="math inline">\(K \times p\)</span>个参数 - 协方差矩阵：<span class="math inline">\(K \times p(p+1)/2\)</span>个参数 - 先验概率：<span class="math inline">\(K-1\)</span>个参数</p>
<p>总计：<span class="math inline">\(Kp + K \cdot \frac{p(p+1)}{2} + (K-1)\)</span></p>
<p>偏差-方差权衡</p>
<p>LDA的优势： 1. 参数更少，方差更小 2. 在样本量较小时更稳定 3. 当同方差性假设成立时，分类效果更优</p>
<p>QDA的优势： 1. 更灵活，可以捕捉类别间的协方差差异 2. 当异方差性明显时，分类精度更高 3. 对模型假设的依赖较小</p>
<p>样本量要求</p>
<p>QDA需要更大的样本量来准确估计每个类别的协方差矩阵。经验法则： - LDA：每个类别至少需要<span class="math inline">\(p+1\)</span>个样本 - QDA：每个类别至少需要<span class="math inline">\(p(p+3)/2 + 1\)</span>个样本</p>
</section>
<section id="正则化判别分析rda" class="level3">
<h3 class="anchored" data-anchor-id="正则化判别分析rda">8.2.4 正则化判别分析（RDA）</h3>
<p>RDA的基本思想</p>
<p>RDA是LDA和QDA的折中方案，通过正则化协方差矩阵来平衡偏差和方差：</p>
<p><span class="math display">\[
\hat{\pmb{\Sigma}}_k(\alpha, \gamma) = \alpha \hat{\pmb{\Sigma}}_k + (1-\alpha) \hat{\pmb{\Sigma}}
\]</span></p>
<p>其中： - <span class="math inline">\(\hat{\pmb{\Sigma}}_k\)</span>：第<span class="math inline">\(k\)</span>类的样本协方差矩阵 - <span class="math inline">\(\hat{\pmb{\Sigma}}\)</span>：合并协方差矩阵 - <span class="math inline">\(\alpha \in [0, 1]\)</span>：控制个体协方差与合并协方差的混合比例</p>
<p>进一步进行特征值收缩：</p>
<p><span class="math display">\[
\hat{\pmb{\Sigma}}_k(\alpha, \gamma) = \gamma \hat{\pmb{\Sigma}}_k(\alpha) + (1-\gamma) \frac{\text{tr}(\hat{\pmb{\Sigma}}_k(\alpha))}{p} \mathbf{I}_p
\]</span></p>
<p>其中<span class="math inline">\(\gamma \in [0, 1]\)</span>控制收缩强度。</p>
<p>RDA的参数选择</p>
<p>通过交叉验证选择最优的<span class="math inline">\((\alpha, \gamma)\)</span>组合：</p>
<ol type="1">
<li>在训练集上拟合不同<span class="math inline">\((\alpha, \gamma)\)</span>组合的RDA模型</li>
<li>在验证集上评估分类准确率</li>
<li>选择使验证集准确率最高的参数组合</li>
</ol>
<p>与逻辑回归的比较</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方面</th>
<th>LDA/QDA</th>
<th>逻辑回归</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>假设</td>
<td>特征服从多元正态分布</td>
<td>无分布假设</td>
</tr>
<tr class="even">
<td>建模对象</td>
<td>联合分布<span class="math inline">\(P(X,Y)\)</span></td>
<td>条件分布<span class="math inline">\(P(Y\|X)\)</span></td>
</tr>
<tr class="odd">
<td>参数估计</td>
<td>有解析解</td>
<td>迭代最大似然估计</td>
</tr>
<tr class="even">
<td>样本效率</td>
<td>当假设成立时更高效</td>
<td>更稳健</td>
</tr>
<tr class="odd">
<td>多分类</td>
<td>直接扩展</td>
<td>需要多项逻辑回归</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="朴素贝叶斯分类器" class="level2">
<h2 class="anchored" data-anchor-id="朴素贝叶斯分类器">8.3 朴素贝叶斯分类器</h2>
<p>朴素贝叶斯基本思想</p>
<p>朴素贝叶斯（Naïve Bayes）是一种基于贝叶斯定理的简单而高效的分类算法。其”朴素”（naïve）之处在于假设<strong>特征之间相互条件独立</strong>，即给定类别时，各个特征之间没有依赖关系。尽管这一假设在现实中很少完全成立，但朴素贝叶斯在许多实际应用中仍表现出色。</p>
<p>贝叶斯定理是朴素贝叶斯分类器的理论基础：</p>
<p><span class="math display">\[
P(Y=k | \mathbf{x}) = \frac{P(Y=k) \cdot P(\mathbf{x} | Y=k)}{P(\mathbf{x})}
\]</span></p>
<p>其中： - <span class="math inline">\(P(Y=k | \mathbf{x})\)</span>：后验概率（给定特征<span class="math inline">\(\mathbf{x}\)</span>时属于类别<span class="math inline">\(k\)</span>的概率） - <span class="math inline">\(P(Y=k)\)</span>：先验概率（类别<span class="math inline">\(k\)</span>在训练集中的比例） - <span class="math inline">\(P(\mathbf{x} | Y=k)\)</span>：似然（类别<span class="math inline">\(k\)</span>中观察到特征<span class="math inline">\(\mathbf{x}\)</span>的概率） - <span class="math inline">\(P(\mathbf{x})\)</span>：证据（边际概率，作为归一化常数）</p>
<p>朴素贝叶斯假设</p>
<p>对于<span class="math inline">\(p\)</span>维特征向量<span class="math inline">\(\mathbf{x} = (x_1, x_2, \dots, x_p)^\top\)</span>，朴素贝叶斯假设：</p>
<p><span class="math display">\[
P(\mathbf{x} | Y=k) = P(x_1, x_2, \dots, x_p | Y=k) = \prod_{j=1}^p P(x_j | Y=k)
\]</span></p>
<p>即给定类别<span class="math inline">\(k\)</span>时，各个特征<span class="math inline">\(x_1, x_2, \dots, x_p\)</span>相互条件独立。</p>
<p>分类规则</p>
<p>将观测<span class="math inline">\(\mathbf{x}\)</span>分配到后验概率最大的类别：</p>
<p><span class="math display">\[
\hat{y}(\mathbf{x}) = \arg\max_{k=1,\dots,K} P(Y=k | \mathbf{x})
\]</span></p>
<p>根据贝叶斯定理和朴素假设：</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}(\mathbf{x}) &amp;= \arg\max_{k} P(Y=k) \cdot P(\mathbf{x} | Y=k) \\
&amp;= \arg\max_{k} P(Y=k) \cdot \prod_{j=1}^p P(x_j | Y=k)
\end{aligned}
\]</span></p>
<p>由于<span class="math inline">\(P(\mathbf{x})\)</span>对所有类别相同，可以省略。</p>
<p>对数形式</p>
<p>在实际计算中，使用对数避免数值下溢：</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}(\mathbf{x}) &amp;= \arg\max_{k} \left[ \log P(Y=k) + \sum_{j=1}^p \log P(x_j | Y=k) \right]
\end{aligned}
\]</span></p>
<section id="不同特征类型的概率估计" class="level3">
<h3 class="anchored" data-anchor-id="不同特征类型的概率估计">8.3.2不同特征类型的概率估计</h3>
<p>朴素贝叶斯可以根据特征类型选择不同的概率估计方法。</p>
<p>分类特征（伯努利朴素贝叶斯）</p>
<p>对于二值特征或类别特征，使用<strong>伯努利分布</strong>：</p>
<p><span class="math display">\[
P(x_j = v | Y=k) = \theta_{kjv}
\]</span></p>
<p>其中<span class="math inline">\(v \in \{0, 1\}\)</span>或<span class="math inline">\(v \in \{1, 2, \dots, m_j\}\)</span>。</p>
<p>参数估计</p>
<p>最大似然估计：</p>
<p><span class="math display">\[
\hat{\theta}_{kjv} = \frac{N_{kjv} + \alpha}{N_k + \alpha m_j}
\]</span></p>
<p>其中： - <span class="math inline">\(N_{kjv}\)</span>：类别<span class="math inline">\(k\)</span>中特征<span class="math inline">\(j\)</span>取值为<span class="math inline">\(v\)</span>的样本数 - <span class="math inline">\(N_k\)</span>：类别<span class="math inline">\(k\)</span>的总样本数 - <span class="math inline">\(\alpha\)</span>：平滑参数（拉普拉斯平滑） - <span class="math inline">\(m_j\)</span>：特征<span class="math inline">\(j\)</span>的可能取值数</p>
<p>拉普拉斯平滑</p>
<p>当某个特征值在训练集中未出现时，最大似然估计为0，导致整个概率为0。拉普拉斯平滑（Laplace smoothing）或加一平滑解决此问题：</p>
<p><span class="math display">\[
\hat{\theta}_{kjv} = \frac{N_{kjv} + 1}{N_k + m_j}
\]</span></p>
<p>这是<span class="math inline">\(\alpha=1\)</span>时的特例。</p>
<p>连续特征（高斯朴素贝叶斯）</p>
<p>对于连续特征，假设服从<strong>高斯分布</strong>：</p>
<p><span class="math display">\[
P(x_j | Y=k) = \frac{1}{\sqrt{2\pi\sigma_{kj}^2}} \exp\left(-\frac{(x_j - \mu_{kj})^2}{2\sigma_{kj}^2}\right)
\]</span></p>
<p>参数估计</p>
<p><span class="math display">\[
\hat{\mu}_{kj} = \frac{1}{N_k} \sum_{i: y_i=k} x_{ij}
\]</span></p>
<p><span class="math display">\[
\hat{\sigma}_{kj}^2 = \frac{1}{N_k} \sum_{i: y_i=k} (x_{ij} - \hat{\mu}_{kj})^2
\]</span></p>
<p>对数似然</p>
<p>对于高斯朴素贝叶斯，对数似然为：</p>
<p><span class="math display">\[
\log P(x_j | Y=k) = -\frac{1}{2} \log(2\pi\sigma_{kj}^2) - \frac{(x_j - \mu_{kj})^2}{2\sigma_{kj}^2}
\]</span> （多项式朴素贝叶斯）</p>
<p>对于表示频次或计数的特征，使用<strong>多项式分布</strong>：</p>
<p><span class="math display">\[
P(\mathbf{x} | Y=k) = \frac{(\sum_{j=1}^p x_j)!}{\prod_{j=1}^p x_j!} \prod_{j=1}^p \theta_{kj}^{x_j}
\]</span></p>
<p>参数估计</p>
<p><span class="math display">\[
\hat{\theta}_{kj} = \frac{N_{kj} + \alpha}{\sum_{j=1}^p N_{kj} + \alpha p}
\]</span></p>
<p>其中<span class="math inline">\(N_{kj} = \sum_{i: y_i=k} x_{ij}\)</span>是类别<span class="math inline">\(k\)</span>中特征<span class="math inline">\(j\)</span>的总计数。</p>
<p>混合特征类型</p>
<p>当数据包含不同类型特征时，可以组合不同分布：</p>
<p><span class="math display">\[
P(\mathbf{x} | Y=k) = \prod_{j \in C} P_{\text{cat}}(x_j | Y=k) \cdot \prod_{j \in R} P_{\text{cont}}(x_j | Y=k) \cdot \prod_{j \in N} P_{\text{count}}(x_j | Y=k)
\]</span></p>
<p>其中<span class="math inline">\(C\)</span>、<span class="math inline">\(R\)</span>、<span class="math inline">\(N\)</span>分别表示分类、连续、计数特征的索引集。</p>
<hr>
</section>
<section id="算法实现" class="level3">
<h3 class="anchored" data-anchor-id="算法实现">8.3.3 算法实现</h3>
<p>训练阶段</p>
<p><strong>输入</strong>：训练集<span class="math inline">\(D = \{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span>，其中<span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^p\)</span>，<span class="math inline">\(y_i \in \{1, 2, \dots, K\}\)</span></p>
<p><strong>步骤</strong>： 1. 估计先验概率：<span class="math inline">\(\hat{\pi}_k = \frac{N_k + \alpha}{n + \alpha K}\)</span> 2. 对于每个类别<span class="math inline">\(k\)</span>和每个特征<span class="math inline">\(j\)</span>： - 如果特征<span class="math inline">\(j\)</span>是分类的：估计<span class="math inline">\(\hat{\theta}_{kjv}\)</span>对于所有<span class="math inline">\(v\)</span> - 如果特征<span class="math inline">\(j\)</span>是连续的：估计<span class="math inline">\(\hat{\mu}_{kj}\)</span>和<span class="math inline">\(\hat{\sigma}_{kj}^2\)</span> - 如果特征<span class="math inline">\(j\)</span>是计数的：估计<span class="math inline">\(\hat{\theta}_{kj}\)</span></p>
<p><strong>输出</strong>：所有估计参数</p>
<p>预测阶段</p>
<p><strong>输入</strong>：新样本<span class="math inline">\(\mathbf{x}^* = (x_1^*, x_2^*, \dots, x_p^*)\)</span>，训练得到的参数</p>
<p><strong>步骤</strong>： 1. 对于每个类别<span class="math inline">\(k=1,\dots,K\)</span>： <span class="math display">\[
   \text{score}_k = \log \hat{\pi}_k + \sum_{j=1}^p \log P(x_j^* | Y=k; \hat{\theta}_{kj})
   \]</span> 2. 预测类别：<span class="math inline">\(\hat{y} = \arg\max_k \text{score}_k\)</span> 3. （可选）计算后验概率： <span class="math display">\[
   P(Y=k | \mathbf{x}^*) = \frac{\exp(\text{score}_k)}{\sum_{l=1}^K \exp(\text{score}_l)}
   \]</span></p>
<p><strong>输出</strong>：预测类别<span class="math inline">\(\hat{y}\)</span>和后验概率</p>
</section>
<section id="与判别分析的比较" class="level3">
<h3 class="anchored" data-anchor-id="与判别分析的比较">8.3.4 与判别分析的比较</h3>
<p>与LDA/QDA的关系</p>
<p>朴素贝叶斯与判别分析都基于贝叶斯定理，但有重要区别：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>方面</th>
<th>朴素贝叶斯</th>
<th>LDA/QDA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>特征独立性</strong></td>
<td>条件独立假设</td>
<td>考虑特征相关性</td>
</tr>
<tr class="even">
<td><strong>分布假设</strong></td>
<td>可灵活选择</td>
<td>必须多元正态</td>
</tr>
<tr class="odd">
<td><strong>参数数量</strong></td>
<td><span class="math inline">\(O(Kp)\)</span></td>
<td>LDA: <span class="math inline">\(O(p^2)\)</span>, QDA: <span class="math inline">\(O(Kp^2)\)</span></td>
</tr>
<tr class="even">
<td><strong>决策边界</strong></td>
<td>可能是非线性</td>
<td>LDA线性，QDA二次</td>
</tr>
</tbody>
</table>
<p>等价性条件</p>
<p>当以下条件成立时，高斯朴素贝叶斯等价于LDA： 1. 所有特征服从多元正态分布 2. 特征条件独立（协方差矩阵为对角矩阵） 3. 各类别的对角协方差矩阵相等</p>
<p>在这种情况下，朴素贝叶斯的决策边界也是线性的。</p>
<p>偏差-方差权衡</p>
<p>朴素贝叶斯引入条件独立假设： - <strong>增加偏差</strong>：当特征相关时，模型假设错误 - <strong>减少方差</strong>：参数更少，估计更稳定</p>
<p>在高维小样本情况下，朴素贝叶斯通常优于更复杂的模型。</p>
<p><strong>模型变体</strong></p>
<p>伯努利朴素贝叶斯</p>
<p>适用于二值特征，如文本分类中的词袋模型（出现/不出现）。</p>
<p>概率模型： <span class="math display">\[
P(x_j | Y=k) = \theta_{kj}^{x_j} (1-\theta_{kj})^{1-x_j}
\]</span></p>
<p>多项式朴素贝叶斯</p>
<p>适用于计数数据，如文本分类中的词频。</p>
<p>概率模型： <span class="math display">\[
P(\mathbf{x} | Y=k) = \frac{(\sum_j x_j)!}{\prod_j x_j!} \prod_j \theta_{kj}^{x_j}
\]</span></p>
<p>高斯朴素贝叶斯</p>
<p>适用于连续特征，假设正态分布。</p>
<p>补充朴素贝叶斯</p>
<p>处理不平衡数据，调整先验概率：</p>
<p><span class="math display">\[
\pi_k^{\text{complement}} = \frac{\sum_{i \notin \text{class } k} \sum_j x_{ij}}{\text{总计数}}
\]</span></p>
<p>贝叶斯信念网络</p>
<p>放宽条件独立假设，允许部分特征相关：</p>
<p><span class="math display">\[
P(\mathbf{x} | Y=k) = \prod_{j=1}^p P(x_j | \text{Pa}(x_j), Y=k)
\]</span></p>
<p>其中<span class="math inline">\(\text{Pa}(x_j)\)</span>是<span class="math inline">\(x_j\)</span>的父节点集合。</p>
<hr>
<p><strong>参数估计与正则化</strong></p>
<p>最大似然估计</p>
<p>对于参数<span class="math inline">\(\theta\)</span>，最大似然估计为：</p>
<p><span class="math display">\[
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \prod_{i=1}^n P(\mathbf{x}_i, y_i | \theta)
\]</span></p>
<p>最大后验估计</p>
<p>考虑参数先验，最大后验估计：</p>
<p><span class="math display">\[
\hat{\theta}_{\text{MAP}} = \arg\max_{\theta} P(\theta) \prod_{i=1}^n P(\mathbf{x}_i, y_i | \theta)
\]</span></p>
<p>共轭先验</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>分布</th>
<th>共轭先验</th>
<th>后验分布</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>伯努利</td>
<td>Beta分布</td>
<td>Beta分布</td>
</tr>
<tr class="even">
<td>多项式</td>
<td>Dirichlet分布</td>
<td>Dirichlet分布</td>
</tr>
<tr class="odd">
<td>高斯（已知方差）</td>
<td>高斯分布</td>
<td>高斯分布</td>
</tr>
<tr class="even">
<td>高斯（已知均值）</td>
<td>Gamma分布</td>
<td>Gamma分布</td>
</tr>
</tbody>
</table>
<p>经验贝叶斯</p>
<p>从数据中估计先验分布的超参数：</p>
<p><span class="math display">\[
\hat{\alpha} = \arg\max_{\alpha} \int P(D | \theta) P(\theta | \alpha) d\theta
\]</span></p>
<p><strong>实际应用</strong></p>
<p>文本分类</p>
<p>朴素贝叶斯是文本分类的经典算法，如： - 垃圾邮件检测 - 情感分析 - 新闻分类</p>
<p><strong>特征工程</strong>： - 词袋模型（Bag-of-Words） - TF-IDF加权 - N-gram特征</p>
<p>推荐系统</p>
<p>基于内容的推荐： <span class="math display">\[
P(\text{喜欢} | \text{物品特征}) \propto P(\text{喜欢}) \prod_j P(\text{特征}_j | \text{喜欢})
\]</span></p>
<p>医学诊断</p>
<p>基于症状预测疾病： <span class="math display">\[
P(\text{疾病} | \text{症状}) \propto P(\text{疾病}) \prod_j P(\text{症状}_j | \text{疾病})
\]</span></p>
<p>异常检测</p>
<p>建模正常行为，检测低概率事件： <span class="math display">\[
P(\mathbf{x}) = \sum_{k=1}^K P(Y=k) \prod_{j=1}^p P(x_j | Y=k)
\]</span> 将<span class="math inline">\(P(\mathbf{x}) &lt; \tau\)</span>的样本标记为异常。</p>
</section>
</section>
<section id="模型比较与选择" class="level2">
<h2 class="anchored" data-anchor-id="模型比较与选择">8.5 模型比较与选择</h2>
<section id="分类性能评估" class="level3">
<h3 class="anchored" data-anchor-id="分类性能评估">8.5.1 分类性能评估</h3>
<p>混淆矩阵：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>预测正类</th>
<th>预测负类</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>实际正类</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>实际负类</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>常用指标： - 准确率：<span class="math inline">\(\frac{TP+TN}{n}\)</span> - 精确率：<span class="math inline">\(\frac{TP}{TP+FP}\)</span> - 召回率：<span class="math inline">\(\frac{TP}{TP+FN}\)</span> - F1分数：<span class="math inline">\(\frac{2\times\text{精确率}\times\text{召回率}}{\text{精确率}+\text{召回率}}\)</span></p>
</section>
<section id="roc曲线与auc" class="level3">
<h3 class="anchored" data-anchor-id="roc曲线与auc">8.5.2 ROC曲线与AUC</h3>
<p>ROC曲线： 以假正率为横轴，真正率为纵轴的曲线。</p>
<p>AUC解释： - AUC = 0.5：随机猜测 - AUC = 1.0：完美分类 - AUC &gt; 0.8：通常认为模型表现良好</p>
</section>
<section id="方法比较" class="level3">
<h3 class="anchored" data-anchor-id="方法比较">8.5.3 方法比较</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>类型</th>
<th>假设</th>
<th>优点</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>逻辑回归</td>
<td>判别式</td>
<td>线性决策边界</td>
<td>概率输出，可解释性强</td>
<td>对特征相关性敏感</td>
</tr>
<tr class="even">
<td>Probit回归</td>
<td>判别式</td>
<td>线性决策边界</td>
<td>有潜在变量解释</td>
<td>计算稍复杂</td>
</tr>
<tr class="odd">
<td>LDA</td>
<td>生成式</td>
<td>正态分布，等协方差</td>
<td>小样本表现好，多分类自然</td>
<td>假设较强</td>
</tr>
</tbody>
</table>
</section>
<section id="模型诊断" class="level3">
<h3 class="anchored" data-anchor-id="模型诊断">8.5.4 模型诊断</h3>
<p>离群值检测： - Pearson残差：<span class="math inline">\(r_i = \frac{y_i - \hat{\pi}_i}{\sqrt{\hat{\pi}_i(1-\hat{\pi}_i)}}\)</span> - Deviance残差：<span class="math inline">\(d_i = \text{sign}(y_i - \hat{\pi}_i)\sqrt{-2[y_i\ln\hat{\pi}_i + (1-y_i)\ln(1-\hat{\pi}_i)]}\)</span></p>
<p>拟合优度检验： - Hosmer-Lemeshow检验 - 皮尔逊卡方检验</p>
<p>过度离散检验： <span class="math display">\[
\frac{\sum_{i=1}^n (y_i - \hat{\pi}_i)^2}{\hat{\pi}_i(1-\hat{\pi}_i)} \sim \chi^2(n-p-1)
\]</span></p>
</section>
</section>
<section id="案例分析" class="level2">
<h2 class="anchored" data-anchor-id="案例分析">8.6 案例分析</h2>
</section>
<section id="本章总结" class="level2">
<h2 class="anchored" data-anchor-id="本章总结">本章总结</h2>
<p>核心公式回顾</p>
<ol type="1">
<li>GLM连接函数：<span class="math inline">\(g(\mu) = \mathbf{X}\pmb{\beta}\)</span></li>
<li>Logit模型：<span class="math inline">\(\ln\left(\frac{\pi}{1-\pi}\right) = \mathbf{X}\pmb{\beta}\)</span></li>
<li>Probit模型：<span class="math inline">\(\Phi^{-1}(\pi) = \mathbf{X}\pmb{\beta}\)</span></li>
<li>LDA判别函数：<span class="math inline">\(\delta_k(\mathbf{x}) = \mathbf{x}'\pmb{\Sigma}^{-1}\pmb{\mu}_k - \frac{1}{2}\pmb{\mu}_k'\pmb{\Sigma}^{-1}\pmb{\mu}_k + \ln\pi_k\)</span></li>
<li>IWLS更新：<span class="math inline">\(\pmb{\beta}^{(t+1)} = (\mathbf{X}'\mathbf{W}^{(t)}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}^{(t)}\mathbf{z}^{(t)}\)</span></li>
</ol>
<p>方法选择指南</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>问题场景</th>
<th>推荐方法</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>需要概率输出</td>
<td>逻辑回归、Probit回归</td>
<td>直接建模条件概率</td>
</tr>
<tr class="even">
<td>小样本问题</td>
<td>LDA</td>
<td>参数更少，估计更稳定</td>
</tr>
<tr class="odd">
<td>多分类问题</td>
<td>LDA、多项逻辑回归</td>
<td>天然处理多类别</td>
</tr>
<tr class="even">
<td>可解释性重要</td>
<td>逻辑回归</td>
<td>优势比有明确解释</td>
</tr>
<tr class="odd">
<td>理论一致性</td>
<td>Probit回归</td>
<td>有潜在变量理论基础</td>
</tr>
</tbody>
</table>
<p>实践建议</p>
<ol type="1">
<li>数据预处理：检查类别平衡性，必要时重采样</li>
<li>特征工程：考虑交互项和非线性变换</li>
<li>模型验证：使用交叉验证评估泛化能力</li>
<li>结果解释：结合领域知识理解系数含义</li>
<li>稳健性分析：尝试不同方法，比较结果一致性</li>
</ol>
<p>广义线性模型为处理非正态响应变量提供了统一的框架，而线性分类方法则是机器学习中最基础且重要的工具。掌握这些方法为进一步学习更复杂的非线性模型奠定了坚实基础。</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "已复制");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "已复制");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/7 广义线性模型.html" class="pagination-link" aria-label="7 广义线性模型">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">7 广义线性模型</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/9 非线性回归与样条回归.html" class="pagination-link" aria-label="9 非线性回归与样条回归">
        <span class="nav-page-text"><span class="chapter-title">9 非线性回归与样条回归</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>