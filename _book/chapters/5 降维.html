<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-Hans" xml:lang="zh-Hans"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5 降维 – 现代预测建模：从回归到机器学习</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/6 正则化.html" rel="next">
<link href="../chapters/part-2-introduction.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-70a47bd5681a7291082a5b9f83d58762.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "没有结果",
    "search-matching-documents-text": "匹配的文档",
    "search-copy-link-title": "复制搜索链接",
    "search-hide-matches-text": "隐藏其它匹配结果",
    "search-more-match-text": "更多匹配结果",
    "search-more-matches-text": "更多匹配结果",
    "search-clear-button-title": "清除",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "提交",
    "search-label": "搜索"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="science-textbook.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-2-introduction.html">II 扩展——线性分析方法</a></li><li class="breadcrumb-item"><a href="../chapters/5 降维.html"><span class="chapter-title">5 降维</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="展开或折叠侧边栏导航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="搜索" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">现代预测建模：从回归到机器学习</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜索"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">前言</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">index.html</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">I 基础-线性回归模型</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-1-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-1-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/1 预测模型与评估.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">1 预测模型与评估</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/2 线性回归.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">2 线性回归</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/3 模型诊断.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">3 模型诊断</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/4 时间序列分析初步.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">4 时间序列分析初步</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">II 扩展——线性分析方法</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-2-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-2-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/5 降维.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">5 降维</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/6 正则化.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">6 正则化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/7 广义线性模型.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">7 广义线性模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/8 线性分类模型.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">8 线性分类模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/9 非线性回归与样条回归.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">9 非线性回归与样条回归</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">III 进阶——超越线性的机器学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-3-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-3-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10 决策树与集成学习.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">10 决策树与集成学习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11 支持向量机.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11 支持向量机</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12 神经网络与深度学习基础.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12 神经网络与深度学习基础</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">IV 实践——综合案例分析</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="展开或折叠此栏">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-4-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">part-4-introduction.html</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13 回归任务与基准测试.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13 回归任务与基准测试</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14 分类任务.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14 分类任务</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15 案例综合分析.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15 案例综合分析</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#本章导读" id="toc-本章导读" class="nav-link active" data-scroll-target="#本章导读">本章导读</a></li>
  <li><a href="#多重共线性" id="toc-多重共线性" class="nav-link" data-scroll-target="#多重共线性">5.1 多重共线性</a>
  <ul class="collapse">
  <li><a href="#高维问题" id="toc-高维问题" class="nav-link" data-scroll-target="#高维问题">5.1.1 高维问题</a></li>
  <li><a href="#多重共线性原因与表现" id="toc-多重共线性原因与表现" class="nav-link" data-scroll-target="#多重共线性原因与表现">5.1.2 多重共线性原因与表现</a></li>
  <li><a href="#多重共线性的影响" id="toc-多重共线性的影响" class="nav-link" data-scroll-target="#多重共线性的影响">5.1.3 多重共线性的影响：</a></li>
  <li><a href="#多重共线性的解决方法" id="toc-多重共线性的解决方法" class="nav-link" data-scroll-target="#多重共线性的解决方法">5.1.4 多重共线性的解决方法</a></li>
  </ul></li>
  <li><a href="#自变量选择" id="toc-自变量选择" class="nav-link" data-scroll-target="#自变量选择">5.2 自变量选择</a>
  <ul class="collapse">
  <li><a href="#子集回归模型" id="toc-子集回归模型" class="nav-link" data-scroll-target="#子集回归模型">5.2.1 子集回归模型</a></li>
  <li><a href="#自变量选择标准" id="toc-自变量选择标准" class="nav-link" data-scroll-target="#自变量选择标准">5.2.2 自变量选择标准</a></li>
  <li><a href="#最优子集回归" id="toc-最优子集回归" class="nav-link" data-scroll-target="#最优子集回归">5.2.3 最优子集回归：</a></li>
  <li><a href="#逐步回归" id="toc-逐步回归" class="nav-link" data-scroll-target="#逐步回归">5.2.4 逐步回归</a></li>
  <li><a href="#调参策略" id="toc-调参策略" class="nav-link" data-scroll-target="#调参策略">5.2.6 调参策略</a></li>
  </ul></li>
  <li><a href="#主成分回归" id="toc-主成分回归" class="nav-link" data-scroll-target="#主成分回归">5.3 主成分回归</a>
  <ul class="collapse">
  <li><a href="#主成分回归原理" id="toc-主成分回归原理" class="nav-link" data-scroll-target="#主成分回归原理">5.3.1 主成分回归原理</a></li>
  <li><a href="#理论推导过程" id="toc-理论推导过程" class="nav-link" data-scroll-target="#理论推导过程">5.3.2 理论推导过程</a></li>
  </ul></li>
  <li><a href="#估计量的性质" id="toc-估计量的性质" class="nav-link" data-scroll-target="#估计量的性质">5.3.3 估计量的性质</a></li>
  <li><a href="#偏最小二乘回归" id="toc-偏最小二乘回归" class="nav-link" data-scroll-target="#偏最小二乘回归">5.4 偏最小二乘回归</a>
  <ul class="collapse">
  <li><a href="#pls的基本思想" id="toc-pls的基本思想" class="nav-link" data-scroll-target="#pls的基本思想">5.4.1 PLS的基本思想</a></li>
  <li><a href="#pls算法nipals" id="toc-pls算法nipals" class="nav-link" data-scroll-target="#pls算法nipals">5.4.2 PLS算法（NIPALS）</a></li>
  </ul></li>
  <li><a href="#案例分析" id="toc-案例分析" class="nav-link" data-scroll-target="#案例分析">5.5 案例分析</a></li>
  <li><a href="#本章总结" id="toc-本章总结" class="nav-link" data-scroll-target="#本章总结">本章总结</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-2-introduction.html">II 扩展——线性分析方法</a></li><li class="breadcrumb-item"><a href="../chapters/5 降维.html"><span class="chapter-title">5 降维</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">5 降维</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="本章导读" class="level2">
<h2 class="anchored" data-anchor-id="本章导读">本章导读</h2>
<p>随着大数据时代的到来，高维数据问题日益普遍。当自变量数目 <span class="math inline">\(p\)</span> 接近甚至超过样本量 <span class="math inline">\(n\)</span> 时，传统的最小二乘法面临严重挑战。本章将系统介绍正则化方法和降维技术这两类处理高维问题的主流方法，为您提供在“维数灾难”下建立稳定预测模型的有效工具。</p>
</section>
<section id="多重共线性" class="level2">
<h2 class="anchored" data-anchor-id="多重共线性">5.1 多重共线性</h2>
<section id="高维问题" class="level3">
<h3 class="anchored" data-anchor-id="高维问题">5.1.1 高维问题</h3>
<p>高维回归模型： <span class="math display">\[
\mathbf{y} = \mathbf{X}\pmb{\beta} + \pmb{\varepsilon}
\]</span> 其中 <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>，当 <span class="math inline">\(p \gg n\)</span> 或 <span class="math inline">\(p \approx n\)</span> 时出现高维问题。</p>
<p>OLS的失效：</p>
<ul>
<li><p>当 <span class="math inline">\(p &gt; n\)</span> 时，<span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> 不可逆</p></li>
<li><p>当 <span class="math inline">\(p \approx n\)</span> 时，估计方差极大</p></li>
<li><p>预测性能下降，模型过拟合</p></li>
</ul>
<p><strong>稀疏性假设</strong></p>
<p>精确稀疏性： <span class="math display">\[ ||\pmb{\beta}||_0 = \#\{j: \beta_j \neq 0\} \ll p \]</span></p>
<p>近似稀疏性： 大多数系数的绝对值很小，只有少数系数较大。</p>
</section>
<section id="多重共线性原因与表现" class="level3">
<h3 class="anchored" data-anchor-id="多重共线性原因与表现">5.1.2 多重共线性原因与表现</h3>
<p>考虑线性回归模型： <span class="math display">\[ \mathbf{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon \]</span></p>
<p><strong>定义1（完全多重共线性）</strong>： 如果存在不全为零的常数 <span class="math inline">\(c_1, c_2, \dots, c_p\)</span>，使得： <span class="math display">\[ c_1 x_1 + c_2 x_2 + \cdots + c_p x_p = 0 \quad \text{（几乎处处成立）} \]</span> 则称自变量间存在完全多重共线性（perfect multicollinearity）。</p>
<p>在矩阵形式下，设设计矩阵 <span class="math inline">\(\mathbf{X} = [\mathbf{1}, \mathbf{x}_1, \dots, \mathbf{x}_p] \in \mathbb{R}^{n \times (p+1)}\)</span>，完全多重共线性意味着： <span class="math display">\[ \text{rank}(\mathbf{X}) &lt; p+1 \]</span> 即 <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> 是奇异的，不存在唯一的最小二乘解。</p>
<p><strong>定义2（近似多重共线性）</strong>： 如果存在不全为零的常数 <span class="math inline">\(c_1, c_2, \dots, c_p\)</span>，使得： <span class="math display">\[ c_1 x_1 + c_2 x_2 + \cdots + c_p x_p \approx 0 \]</span> 即自变量间存在高度但不完全的线性关系，则称为近似多重共线性。</p>
<p>多重共线系的原因：</p>
<ul>
<li><p>变量之间的共同变化趋势</p></li>
<li><p>时间序列数据趋势，滞后变量</p></li>
<li><p>经济指标关系约束或者虚拟变量陷阱</p></li>
<li><p>样本量不足</p></li>
<li><p>截面数据的规模效应</p></li>
</ul>
<p><strong>多重共线性的表现</strong></p>
<p>在回归结果中：</p>
<p>系数估计异常</p>
<ul>
<li><p>符号异常：系数符号与理论预期相反</p></li>
<li><p>数值异常大：系数绝对值异常大</p></li>
<li><p>统计不显著但联合显著：</p></li>
<li><p>单个 <span class="math inline">\(t\)</span> 检验：<span class="math inline">\(H_0: \beta_j = 0\)</span> 不拒绝</p></li>
<li><p>整体 <span class="math inline">\(F\)</span> 检验：<span class="math inline">\(H_0: \beta_1 = \cdots = \beta_p = 0\)</span> 拒绝</p></li>
</ul>
<p>稳定性检验</p>
<ul>
<li>删除观测影响：删除个别观测导致系数估计剧烈变化</li>
<li>添加/删除变量：添加或删除其他变量导致系数符号或大小显著变化</li>
</ul>
</section>
<section id="多重共线性的影响" class="level3">
<h3 class="anchored" data-anchor-id="多重共线性的影响">5.1.3 多重共线性的影响：</h3>
<ul>
<li><p>参数估计方面：参数估计仍然是无偏的，但是参数估计方差会很大</p></li>
<li><p>假设检验方面：F检验会显著，但是t检验会失效</p></li>
<li><p>预测：预测变得不稳定</p></li>
<li><p>参数估计的值不稳定，正负符号可能不符合预期</p></li>
</ul>
<p>多重共线性的主要影响在于参数估计量的方程膨胀，下面给出简单的推导过程：</p>
<p>这里只考虑考虑复共线性，即设计矩阵<span class="math inline">\(X\)</span>的列向量之间存在近似线性相关关系。</p>
<p>数学表述为：</p>
<p>存在非零向量<span class="math inline">\(c = (c_1, c_2, \ldots, c_p)' \neq 0\)</span>，使得：</p>
<p><span class="math display">\[
c_1X_1 + c_2X_2 + \cdots + c_pX_p \approx 0
\]</span></p>
<p>其中<span class="math inline">\(X_j \in \mathbb{R}^n\)</span>表示第<span class="math inline">\(j\)</span>个自变量的观测向量。</p>
<p>考虑Gram矩阵<span class="math inline">\(S = X'X\)</span>，其特征值问题为<span class="math inline">\(Sv = \lambda v\)</span>。利用Rayleigh商的极值性质：</p>
<p><span class="math display">\[\lambda_{min} = \min_{v \neq 0} \frac{v'Sv}{v'v} = \min_{v \neq 0} \frac{\lVert Xv \rVert^2}{\lVert v \rVert^2}\]</span></p>
<p>当存在多重共线性时，取<span class="math inline">\(v = c\)</span>（近似线性相关的系数向量），则有：</p>
<p><span class="math display">\[
\lambda_{min} \leq \frac{\lVert Xc \rVert^2}{\lVert c \rVert^2} \approx 0
\]</span></p>
<p>Gram矩阵的谱分解为： <span class="math display">\[
S = X'X = \sum_{j=1}^p \lambda_j v_j v_j'
\]</span></p>
<p>其逆矩阵相应为： <span class="math display">\[
(X'X)^{-1} = \sum_{j=1}^p \frac{1}{\lambda_j} v_j v_j'
\]</span></p>
<p>这时，逆矩阵中每个特征方向的贡献被<span class="math inline">\(\frac{1}{\lambda_j}\)</span>放大。当某些<span class="math inline">\(\lambda_j \to 0\)</span>时，对应项<span class="math inline">\(\frac{1}{\lambda_j} \to \infty\)</span>。</p>
<p>普通最小二乘估计的方差-协方差矩阵为： <span class="math display">\[
\text{Var}(\hat{\beta}) = \sigma^2(X'X)^{-1} = \sigma^2 \sum_{j=1}^p \frac{1}{\lambda_j} v_j v_j'
\]</span></p>
<p>第<span class="math inline">\(j\)</span>个参数估计的方差为： <span class="math display">\[
\text{Var}(\hat{\beta}_j) = \sigma^2[(X'X)^{-1}]_{jj} = \sigma^2 \sum_{k=1}^p \frac{v_{kj}^2}{\lambda_k}
\]</span></p>
<p>每个参数估计的方差可分解为在各特征向量方向上的贡献之和，权重为<span class="math inline">\(\frac{v_{kj}^2}{\lambda_k}\)</span>。</p>
<p>设<span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p &gt; 0\)</span>，当存在严重多重共线性时，假设前<span class="math inline">\(m\)</span>个特征值显著大于零，后<span class="math inline">\(p-m\)</span>个特征值接近零：</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}_j) = \sigma^2\left(\sum_{k=1}^m \frac{v_{kj}^2}{\lambda_k} + \sum_{k=m+1}^p \frac{v_{kj}^2}{\lambda_k}\right)
\]</span></p>
<p>其中第二项由于<span class="math inline">\(\lambda_k \approx 0\)</span>而急剧增大：</p>
<p><span class="math display">\[\sum_{k=m+1}^p \frac{v_{kj}^2}{\lambda_k} \to \infty\]</span></p>
<p>小特征值对应的特征方向在方差表达式中产生巨大贡献。</p>
<p>在多元线性回归模型 <span class="math inline">\(Y = X\beta + \varepsilon\)</span> 中，第 <span class="math inline">\(j\)</span> 个参数估计 <span class="math inline">\(\hat{\beta}_j\)</span> 的方差为： <span class="math display">\[ \text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{(1 - R_j^2) \sum_{i=1}^n (x_{ij} - \bar{x}_j)^2} \]</span>将方差重写为： <span class="math display">\[ \text{Var}(\hat{\beta}_j) = \underbrace{\frac{\sigma^2}{\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2}}_{\text{简单回归方差}} \times \underbrace{\frac{1}{1 - R_j^2}}_{VIF_j} \]</span></p>
<p><strong>方差膨胀因子(Variance Inflation Factor, VIF)</strong> 是衡量多重共线性严重程度的核心指标，定义为： <span class="math display">\[ VIF_j = \frac{1}{1 - R_j^2} \]</span> 其中：</p>
<ul>
<li><p><span class="math inline">\(R_j^2\)</span> 是第 <span class="math inline">\(j\)</span> 个自变量 <span class="math inline">\(X_j\)</span> 对其他所有自变量 <span class="math inline">\(X_1, X_2, \ldots, X_{j-1}, X_{j+1}, \ldots, X_p\)</span> 进行回归所得的决定系数</p></li>
<li><p><span class="math inline">\(VIF_j\)</span> 度量了由于与其他自变量相关而导致的第 <span class="math inline">\(j\)</span> 个参数估计方差的膨胀倍数</p></li>
<li><p>诊断标准： - <span class="math inline">\(\text{VIF}_j &gt; 10\)</span>：存在严重多重共线性 - <span class="math inline">\(\text{VIF}_j &gt; 5\)</span>：存在中度多重共线性</p></li>
</ul>
<p>这清晰地展示了方差膨胀因子的含义：参数估计方差相对于无多重共线性情况下的膨胀倍数。</p>
<p>将方差代入： <span class="math display">\[VIF_j = \frac{1}{1-R_j^2} = \frac{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2}{s^2} \cdot [(X'X)^{-1}]_{jj}\]</span></p>
<p>可知： <span class="math display">\[VIF_j \propto \sum_{k=1}^p \frac{v_{kj}^2}{\lambda_k}\]</span></p>
<p>方差膨胀因子本质上度量了参数方差在各特征方向上的累积放大效应。</p>
<p>矩阵<span class="math inline">\(X'X\)</span>的<strong>条件数</strong>定义为： <span class="math display">\[\kappa(X'X) = \frac{\lambda_{max}}{\lambda_{min}}\]</span></p>
<p>多重共线性下<span class="math inline">\(\lambda_{min} \to 0\)</span>，导致：</p>
<ul>
<li><p><span class="math inline">\(\kappa(X'X) \to \infty\)</span></p></li>
<li><p>矩阵求逆数值不稳定</p></li>
<li><p>估计量对数据扰动敏感</p></li>
</ul>
<p>设计矩阵的病态性： 条件数 <span class="math inline">\(\kappa(\mathbf{X}'\mathbf{X}) = \frac{\lambda_{\max}}{\lambda_{\min}}\)</span> 过大，其中 <span class="math inline">\(\lambda\)</span> 是特征值。</p>
</section>
<section id="多重共线性的解决方法" class="level3">
<h3 class="anchored" data-anchor-id="多重共线性的解决方法">5.1.4 多重共线性的解决方法</h3>
<ul>
<li>删除变量</li>
<li>合并变量</li>
<li>变量变换</li>
<li>增加样本量</li>
<li>正则化方法</li>
<li>降维方法</li>
</ul>
</section>
</section>
<section id="自变量选择" class="level2">
<h2 class="anchored" data-anchor-id="自变量选择">5.2 自变量选择</h2>
<section id="子集回归模型" class="level3">
<h3 class="anchored" data-anchor-id="子集回归模型">5.2.1 子集回归模型</h3>
<p>考虑线性回归模型： <span class="math display">\[ \mathbf{y} = \mathbf{X}\pmb{\beta} + \pmb{\varepsilon} \]</span></p>
<p>其中：</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>：响应变量</p></li>
<li><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>：设计矩阵（包含截距项）</p></li>
<li><p><span class="math inline">\(\pmb{\beta} \in \mathbb{R}^p\)</span>：回归系数</p></li>
<li><p><span class="math inline">\(\pmb{\varepsilon} \in \mathbb{R}^n\)</span>：误差项，<span class="math inline">\(\pmb{\varepsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_n)\)</span></p></li>
</ul>
<p>子集模型</p>
<p>设 <span class="math inline">\(\mathcal{M}\)</span> 表示一个候选模型，包含 <span class="math inline">\(k\)</span> 个自变量（不包括截距），则： <span class="math display">\[ \mathbf{y} = \mathbf{X}_{\mathcal{M}}\pmb{\beta}_{\mathcal{M}} + \pmb{\varepsilon}_{\mathcal{M}} \]</span> 其中 <span class="math inline">\(k = |\mathcal{M}|\)</span>。</p>
<ul>
<li><p><strong>残差平方和</strong>： <span class="math display">\[ \text{RSS}_{\mathcal{M}} = \|\mathbf{y} - \hat{\mathbf{y}}_{\mathcal{M}}\|^2 = \mathbf{y}^\top(\mathbf{I} - \mathbf{P}_{\mathcal{M}})\mathbf{y} \]</span> 其中 <span class="math inline">\(\mathbf{P}_{\mathcal{M}} = \mathbf{X}_{\mathcal{M}}(\mathbf{X}_{\mathcal{M}}^\top \mathbf{X}_{\mathcal{M}})^{-1}\mathbf{X}_{\mathcal{M}}^\top\)</span> 是投影矩阵。</p></li>
<li><p><strong>最大似然估计</strong>： 在正态假设下，似然函数为： <span class="math display">\[ L(\pmb{\beta}_{\mathcal{M}}, \sigma^2) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{\text{RSS}_{\mathcal{M}} + \|\mathbf{X}_{\mathcal{M}}\pmb{\beta}_{\mathcal{M}} - \mathbf{X}_{\mathcal{M}}\hat{\pmb{\beta}}_{\mathcal{M}}\|^2}{2\sigma^2}\right) \]</span> 最大似然估计为： <span class="math display">\[ \hat{\pmb{\beta}}_{\mathcal{M}} = (\mathbf{X}_{\mathcal{M}}^\top \mathbf{X}_{\mathcal{M}})^{-1}\mathbf{X}_{\mathcal{M}}^\top \mathbf{y}, \quad \hat{\sigma}^2_{\mathcal{M}} = \frac{\text{RSS}_{\mathcal{M}}}{n} \]</span></p></li>
<li><p><strong>对数似然</strong>： <span class="math display">\[ \ell(\mathcal{M}) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\hat{\sigma}^2_{\mathcal{M}}) - \frac{n}{2} \]</span></p></li>
</ul>
</section>
<section id="自变量选择标准" class="level3">
<h3 class="anchored" data-anchor-id="自变量选择标准">5.2.2 自变量选择标准</h3>
<p>在线性回归模型中，当有大量潜在自变量时，我们需要选择最优的子集。自变量选择的目标是：</p>
<ul>
<li><strong>提高预测精度</strong>：减少过拟合，降低预测方差</li>
<li><strong>增强模型解释性</strong>：识别真正重要的变量</li>
<li><strong>简化模型</strong>：减少数据收集和计算成本</li>
</ul>
<p>1 决定系数 <span class="math inline">\(R^2\)</span></p>
<p><span class="math display">\[ R^2_{\mathcal{M}} = 1 - \frac{\text{RSS}_{\mathcal{M}}}{\text{TSS}} \]</span> 其中 <span class="math inline">\(\text{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2\)</span> 是总平方和。</p>
<p><strong>问题</strong>：<span class="math inline">\(R^2\)</span> 随自变量增加而单调增加，倾向于选择大模型。</p>
<p>为了惩罚模型复杂度： <span class="math display">\[ R^2_{\text{adj},\mathcal{M}} = 1 - \frac{\text{RSS}_{\mathcal{M}}/(n-k-1)}{\text{TSS}/(n-1)} = 1 - \frac{n-1}{n-k-1}(1 - R^2_{\mathcal{M}}) \]</span></p>
<p><strong>性质</strong>： - 仅当新变量的 <span class="math inline">\(t\)</span> 统计量绝对值大于 1 时，<span class="math inline">\(R^2_{\text{adj}}\)</span> 才会增加 - 但仍不是一致模型选择准则</p>
<p>2 Akaike 信息准则 (AIC)</p>
<p>AIC 基于 Kullback-Leibler 散度最小化。设真实分布为 <span class="math inline">\(g(y)\)</span>，模型分布为 <span class="math inline">\(f(y|\theta)\)</span>，K-L 散度为： <span class="math display">\[ I(g; f) = \int g(y) \log g(y) dy - \int g(y) \log f(y|\theta) dy \]</span></p>
<p>最小化 K-L 散度等价于最大化 <span class="math inline">\(\mathbb{E}_g[\log f(y|\theta)]\)</span>。AIC 估计： <span class="math display">\[ \text{AIC} = -2 \log L(\hat{\theta}) + 2k \]</span></p>
<p>对于线性回归： <span class="math display">\[ \text{AIC} = n \log\left(\frac{\text{RSS}_{\mathcal{M}}}{n}\right) + 2(k+1) + n[\log(2\pi) + 1] \]</span></p>
<p>去掉常数项，常用简化形式： <span class="math display">\[ \text{AIC} = n \log(\hat{\sigma}^2_{\mathcal{M}}) + 2k \]</span></p>
<p>或等价的： <span class="math display">\[ \text{AIC} = \frac{\text{RSS}_{\mathcal{M}}}{n} e^{2k/n} \]</span></p>
<p><strong>版本差异</strong>：</p>
<ul>
<li><p>AIC：<span class="math inline">\(n \log(\text{RSS}/n) + 2k\)</span></p></li>
<li><p>AICc（小样本校正）：<span class="math inline">\(n \log(\text{RSS}/n) + 2k + \frac{2k(k+1)}{n-k-1}\)</span></p></li>
<li><p>AICu：<span class="math inline">\(n \log[\text{RSS}/(n-k)] + 2k\)</span></p></li>
</ul>
<p>3 贝叶斯信息准则 (BIC)</p>
<p>BIC 源于贝叶斯因子的 Laplace 近似。模型 <span class="math inline">\(\mathcal{M}\)</span> 的后验概率： <span class="math display">\[ P(\mathcal{M}|\mathbf{y}) \propto P(\mathbf{y}|\mathcal{M}) P(\mathcal{M}) \]</span></p>
<p>边缘似然： <span class="math display">\[ P(\mathbf{y}|\mathcal{M}) = \int L(\pmb{\beta}_{\mathcal{M}}, \sigma^2) \pi(\pmb{\beta}_{\mathcal{M}}, \sigma^2) d\pmb{\beta}_{\mathcal{M}} d\sigma^2 \]</span></p>
<p>使用 Jeffrey 无信息先验 <span class="math inline">\(\pi(\pmb{\beta}, \sigma^2) \propto 1/\sigma^2\)</span>，可得： <span class="math display">\[ -2 \log P(\mathbf{y}|\mathcal{M}) \approx n \log(\hat{\sigma}^2_{\mathcal{M}}) + k \log n \]</span></p>
<p>BIC 公式</p>
<p><span class="math display">\[ \text{BIC} = n \log\left(\frac{\text{RSS}_{\mathcal{M}}}{n}\right) + k \log n \]</span></p>
<p>或等价地： <span class="math display">\[ \text{BIC} = \frac{\text{RSS}_{\mathcal{M}}}{n} n^{k/n} \]</span></p>
<p>AIC 与 BIC 比较</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>准则</th>
<th>惩罚项</th>
<th>目标</th>
<th>渐近性质</th>
<th>模型倾向</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AIC</td>
<td><span class="math inline">\(2k\)</span></td>
<td>预测最优</td>
<td>非一致</td>
<td>偏大模型</td>
</tr>
<tr class="even">
<td>BIC</td>
<td><span class="math inline">\(k \log n\)</span></td>
<td>真实模型</td>
<td>一致</td>
<td>偏小模型</td>
</tr>
</tbody>
</table>
<p><strong>一致性</strong>：当 <span class="math inline">\(n \to \infty\)</span> 时，BIC 以概率 1 选择真实模型（如果真实模型在候选集中），而 AIC 可能选择过参数化模型。</p>
<p><strong>效率</strong>：在错误设定下，AIC 选择的模型可能有更好的预测性能。</p>
<ul>
<li><p>AIC：渐近有效（当真实模型不在候选集中时，选择的模型预测误差接近最优）</p></li>
<li><p>BIC：渐近一致但非有效</p></li>
</ul>
<p>模拟研究显示： 1. 当信噪比低、样本量小时，AIC 倾向于选择过大的模型 2. BIC 在样本量足够大时表现更好 3. <span class="math inline">\(C_p\)</span> 与 AIC 渐近等价（<span class="math inline">\(C_p = \text{AIC}/n + \text{常数}\)</span>） 高维情况 (<span class="math inline">\(p &gt; n\)</span>)</p>
<p>传统准则失效，需要改进： - 扩展 BIC(EBIC)：<span class="math inline">\(\text{BIC} + \gamma \log \binom{p}{k}\)</span>，<span class="math inline">\(\gamma \in [0,1]\)</span> - 高维 AIC：使用有效自由度代替 <span class="math inline">\(k\)</span></p>
<p>4 Mallow’s <span class="math inline">\(C_p\)</span> 统计量</p>
<p>考虑标准化预测误差： <span class="math display">\[ \Gamma_p(\mathcal{M}) = \frac{1}{\sigma^2} \mathbb{E}[\|\mathbf{X}\hat{\pmb{\beta}}_{\mathcal{M}} - \mathbf{X}\pmb{\beta}\|^2] \]</span></p>
<p>可以证明： <span class="math display">\[ \mathbb{E}[\text{RSS}_{\mathcal{M}}] = (n-k)\sigma^2 + \|\mathbf{X}\pmb{\beta} - \mathbf{X}_{\mathcal{M}}\pmb{\beta}_{\mathcal{M}}^*\|^2 \]</span></p>
<p>其中 <span class="math inline">\(\pmb{\beta}_{\mathcal{M}}^*\)</span> 是 <span class="math inline">\(\pmb{\beta}\)</span> 在子空间上的投影。因此： <span class="math display">\[ \mathbb{E}[C_p(\mathcal{M})] \approx \frac{1}{\sigma^2} \|\mathbf{X}\pmb{\beta} - \mathbf{X}_{\mathcal{M}}\pmb{\beta}_{\mathcal{M}}^*\|^2 + k \]</span></p>
<p>假设全模型（包含所有 <span class="math inline">\(p\)</span> 个变量）是正确的，则： <span class="math display">\[ C_p(\mathcal{M}) = \frac{\text{RSS}_{\mathcal{M}}}{\hat{\sigma}^2_{\text{full}}} + 2k - n \]</span> 其中 <span class="math inline">\(\hat{\sigma}^2_{\text{full}} = \text{RSS}_{\text{full}}/(n-p)\)</span>。</p>
<p>可以证明 <span class="math inline">\(C_p\)</span> 是 <span class="math inline">\(\Gamma_p(k)\)</span> 的无偏估计</p>
<p>解释 - <span class="math inline">\(C_p \approx k\)</span>：模型无偏 - <span class="math inline">\(C_p &gt; k\)</span>：模型有偏 - <span class="math inline">\(C_p &lt; k\)</span>：可能过拟合或随机波动</p>
<p>选择 <span class="math inline">\(C_p\)</span> 小且接近 <span class="math inline">\(k\)</span> 的模型。</p>
<p>准则选择指南</p>
<ul>
<li>预测导向：用 AIC、<span class="math inline">\(C_p\)</span> 或交叉验证</li>
<li>解释导向：用 BIC 或调整 <span class="math inline">\(R^2\)</span></li>
<li>大数据：BIC 更可靠（<span class="math inline">\(n \geq 1000\)</span>）</li>
<li>小数据：AICc 或交叉验证</li>
</ul>
<p>自变量选择需要权衡偏差-方差、解释性与预测精度。没有单一最优准则，实际应用中应： 1. 考虑研究目标（预测 vs 解释） 2. 结合多种准则 3. 进行模型诊断和验证 4. 考虑计算成本和可解释性</p>
<p><strong>核心公式总结</strong>：</p>
<ul>
<li><p><span class="math inline">\(R^2_{\text{adj}} = 1 - \frac{n-1}{n-k-1}(1-R^2)\)</span></p></li>
<li><p><span class="math inline">\(\text{AIC} = n \log(\text{RSS}/n) + 2k\)</span></p></li>
<li><p><span class="math inline">\(\text{BIC} = n \log(\text{RSS}/n) + k \log n\)</span></p></li>
<li><p><span class="math inline">\(C_p = \text{RSS}/\hat{\sigma}^2_{\text{full}} + 2k - n\)</span></p></li>
</ul>
<p>选择准则本身也有不确定性，模型平均和集成方法可以进一步改善预测性能。</p>
<p>最优子集回归</p>
<p>最优子集选择： 对于每个 <span class="math inline">\(k = 1, 2, \cdots, p\)</span>，选择使RSS最小的包含 <span class="math inline">\(k\)</span> 个自变量的模型。</p>
<p>计算挑战： 需要评估 <span class="math inline">\(2^p\)</span> 个模型，计算不可行。</p>
<p>逐步回归： - 前向选择 - 后向剔除 - 逐步选择</p>
<p>局限性： - 路径依赖 - 多重检验问题 - 标准误低估</p>
</section>
<section id="最优子集回归" class="level3">
<h3 class="anchored" data-anchor-id="最优子集回归">5.2.3 最优子集回归：</h3>
<p>对于给定的模型大小 <span class="math inline">\(k\)</span>，最优子集问题是组合优化问题： <span class="math display">\[ \mathcal{M}_k^* = \arg\min_{\mathcal{M}:|\mathcal{M}|=k} \text{RSS}(\mathcal{M}) \]</span></p>
<p>总搜索空间大小为 <span class="math inline">\(\sum_{k=0}^p \binom{p}{k} = 2^p\)</span>，是NP-hard问题。</p>
</section>
<section id="逐步回归" class="level3">
<h3 class="anchored" data-anchor-id="逐步回归">5.2.4 逐步回归</h3>
<p><strong>前向选择</strong></p>
<p>前向选择等价于求解以下序列优化问题： <span class="math display">\[ j_t = \arg\max_{j \notin \mathcal{M}_{t-1}} \frac{|\mathbf{x}_j^\top \mathbf{r}_{t-1}|}{\|\mathbf{x}_j\|} \]</span></p>
<p>其中 <span class="math inline">\(\mathbf{r}_{t-1}\)</span> 是第 <span class="math inline">\(t-1\)</span> 步的残差向量。</p>
<p>与正交匹配追踪（OMP）的关系</p>
<p>前向选择是OMP的特例。在正交化版本中： 1. 计算当前残差与所有预测变量的相关系数 2. 选择最大相关系数对应的变量 3. 正交化更新残差</p>
<p><strong>数学表达</strong>： <span class="math display">\[ \mathbf{q}_t = \frac{(\mathbf{I} - \mathbf{Q}_{t-1}\mathbf{Q}_{t-1}^\top)\mathbf{x}_{j_t}}{\|(\mathbf{I} - \mathbf{Q}_{t-1}\mathbf{Q}_{t-1}^\top)\mathbf{x}_{j_t}\|} \]</span> 其中 <span class="math inline">\(\mathbf{Q}_{t-1} = [\mathbf{q}_1, \dots, \mathbf{q}_{t-1}]\)</span> 为正交基。</p>
<p><strong>后向删除的</strong></p>
<p>后向删除基于假设检验： <span class="math display">\[ H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0 \]</span></p>
<p>使用 <span class="math inline">\(F\)</span> 统计量： <span class="math display">\[ F_j = \frac{(\text{RSS}_{-j} - \text{RSS})/1}{\text{RSS}/(n-k)} \]</span> 其中 <span class="math inline">\(\text{RSS}_{-j}\)</span> 是删除变量 <span class="math inline">\(j\)</span> 后的RSS。</p>
<p><strong>双向逐步回归</strong></p>
<p>算法描述</p>
<p>结合前向和后向步骤： 1. <strong>初始化</strong>：<span class="math inline">\(\mathcal{M} = \emptyset\)</span> 2. <strong>迭代直到收敛</strong>： - <strong>前向步</strong>：尝试添加一个变量 - <strong>后向步</strong>：尝试删除一个变量 - 每次选择使 RSS 减少最多（或增加最少）的操作 - 通常基于统计显著性（如 <span class="math inline">\(p\)</span> 值）决定</p>
<p>基于 <span class="math inline">\(F\)</span> 检验的版本</p>
<p>常用的停止准则是 <span class="math inline">\(F\)</span> 统计量： - <strong>添加变量</strong>：计算偏 <span class="math inline">\(F\)</span> 统计量 <span class="math display">\[   F_{\text{add}} = \frac{(\text{RSS}_{\text{old}} - \text{RSS}_{\text{new}})/1}{\text{RSS}_{\text{new}}/(n-|\mathcal{M}|-1)}   \]</span> 如果 <span class="math inline">\(F_{\text{add}} &gt; F_{\text{in}}\)</span>（进入临界值），则添加该变量 - <strong>删除变量</strong>：计算 <span class="math inline">\(F\)</span> 统计量 <span class="math display">\[   F_{\text{drop}} = \frac{(\text{RSS}_{\text{without}} - \text{RSS}_{\text{with}})/1}{\text{RSS}_{\text{with}}/(n-|\mathcal{M}|)}   \]</span> 如果 <span class="math inline">\(F_{\text{drop}} &lt; F_{\text{out}}\)</span>（删除临界值），则删除该变量</p>
<p>** 与LAR 算法联系**</p>
<p>最小角回归（Least Angle Regression, LAR）可以看作逐步回归的连续版本： 1. 开始时，所有系数为0 2. 找到与当前残差最相关的变量 3. 沿该方向移动系数，直到另一个变量与残差的相关性相等 4. 沿这两个变量的角平分线方向移动 5. 重复直到所有变量都进入模型</p>
<p>总结与比较</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>优点</th>
<th>缺点</th>
<th>计算复杂度</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>最优子集</td>
<td>全局最优</td>
<td>计算不可行（<span class="math inline">\(p&gt;40\)</span>）</td>
<td><span class="math inline">\(O(2^p)\)</span></td>
<td><span class="math inline">\(p \leq 20\)</span>，精确解重要</td>
</tr>
<tr class="even">
<td>前向选择</td>
<td>计算快，可处理 <span class="math inline">\(p&gt;n\)</span></td>
<td>贪心，可能不是最优</td>
<td><span class="math inline">\(O(p^2)\)</span></td>
<td>高维数据，计算资源有限</td>
</tr>
<tr class="odd">
<td>后向删除</td>
<td>考虑变量间交互</td>
<td>需要 <span class="math inline">\(n&gt;p\)</span></td>
<td><span class="math inline">\(O(p^3)\)</span></td>
<td><span class="math inline">\(p\)</span> 中等，全模型可拟合</td>
</tr>
<tr class="even">
<td>双向逐步</td>
<td>更灵活</td>
<td>可能过拟合</td>
<td><span class="math inline">\(O(p^3)\)</span></td>
<td>中等维度，平衡探索利用</td>
</tr>
</tbody>
</table>
<p>实际建议</p>
<ul>
<li>当 <span class="math inline">\(p \leq 20\)</span> 时，考虑最优子集（或所有子集）</li>
<li>当 <span class="math inline">\(p &gt; n\)</span> 时，使用前向选择或 LASSO</li>
<li>当关心解释性时，逐步回归可能优于黑箱方法</li>
<li>总是使用交叉验证选择最终模型大小</li>
</ul>
<p>数学洞察</p>
<p>逐步回归本质上是在逐步优化的基础上进行的坐标下降。设目标函数： <span class="math display">\[
J(\pmb{\beta}) = \frac{1}{2}\|\mathbf{y} - \mathbf{X}\pmb{\beta}\|^2 + \lambda\|\pmb{\beta}\|_0
\]</span> 其中 <span class="math inline">\(\|\pmb{\beta}\|_0 = \sum_{j=1}^p I(\beta_j \neq 0)\)</span>。这是 NP 难问题，逐步回归提供了近似解。</p>
<p>现代发展表明，凸松弛（如 LASSO）在理论和计算上都有优势，但逐步回归因其简单直观，仍在实践中广泛使用。</p>
</section>
<section id="调参策略" class="level3">
<h3 class="anchored" data-anchor-id="调参策略">5.2.6 调参策略</h3>
<p>网格搜索： 在 <span class="math inline">\(\lambda\)</span>（和 <span class="math inline">\(\alpha\)</span>）的网格上计算交叉验证误差。</p>
<p>信息准则：</p>
<ul>
<li><p>AIC：<span class="math inline">\(AIC = n\ln(\hat{\sigma}^2) + 2k\)</span></p></li>
<li><p>BIC：<span class="math inline">\(BIC = n\ln(\hat{\sigma}^2) + k\ln(n)\)</span></p></li>
</ul>
<p>一倍标准误准则： 选择调优参数，使得交叉验证误差在最小误差的一倍标准误范围内。</p>
</section>
</section>
<section id="主成分回归" class="level2">
<h2 class="anchored" data-anchor-id="主成分回归">5.3 主成分回归</h2>
<section id="主成分回归原理" class="level3">
<h3 class="anchored" data-anchor-id="主成分回归原理">5.3.1 主成分回归原理</h3>
<p>主成分回归（Principal Component Regression, PCR）是多元线性回归与主成分分析（PCA）的结合。当自变量存在多重共线性时，普通最小二乘法（OLS）估计的方差会变得很大，PCR通过先对自变量进行主成分分析，然后选取部分主成分作为新的自变量进行回归，从而改善估计的稳定性。</p>
</section>
<section id="理论推导过程" class="level3">
<h3 class="anchored" data-anchor-id="理论推导过程">5.3.2 理论推导过程</h3>
<p>设线性回归模型： <span class="math display">\[ \mathbf{y} = \mathbf{X}\pmb{\beta} + \pmb{\varepsilon} \]</span></p>
<p>其中：</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y} \in \mathbb{R}^{n}\)</span> 是因变量观测向量</p></li>
<li><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> 是自变量设计矩阵（已中心化）</p></li>
<li><p><span class="math inline">\(\pmb{\beta} \in \mathbb{R}^{p}\)</span> 是回归系数向量</p></li>
<li><p><span class="math inline">\(\pmb{\varepsilon} \in \mathbb{R}^{n}\)</span> 是误差向量，满足 <span class="math inline">\(\mathbb{E}(\pmb{\varepsilon}) = \mathbf{0}\)</span>, <span class="math inline">\(\text{Var}(\pmb{\varepsilon}) = \sigma^2 \mathbf{I}_n\)</span></p></li>
</ul>
<p>假设 <span class="math inline">\(\mathbf{X}\)</span> 已中心化（即每列均值为0），则无需考虑截距项。</p>
<p>则<span class="math inline">\(\mathbf{X}\)</span> 的样本协方差矩阵为： <span class="math display">\[ \mathbf{S} = \frac{1}{n-1} \mathbf{X}^\top \mathbf{X} \]</span></p>
<p>对 <span class="math inline">\(\mathbf{S}\)</span> 进行特征分解： <span class="math display">\[ \mathbf{S} = \mathbf{V}\pmb{\Lambda}\mathbf{V}^\top \]</span></p>
<p>其中：</p>
<ul>
<li><p><span class="math inline">\(\pmb{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_p)\)</span>，<span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0\)</span> 是特征值</p></li>
<li><p><span class="math inline">\(\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p]\)</span> 是正交特征向量矩阵，满足 <span class="math inline">\(\mathbf{V}^\top \mathbf{V} = \mathbf{I}_p\)</span></p></li>
</ul>
<p>主成分得分矩阵为： <span class="math display">\[ \mathbf{Z} = \mathbf{X}\mathbf{V} \]</span></p>
<p>其中 <span class="math inline">\(\mathbf{Z} = [\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_p]\)</span>，第 <span class="math inline">\(j\)</span> 个主成分 <span class="math inline">\(\mathbf{z}_j = \mathbf{X}\mathbf{v}_j\)</span>。</p>
<p>原模型 <span class="math inline">\(\mathbf{y} = \mathbf{X}\pmb{\beta} + \pmb{\varepsilon}\)</span> 可写为： <span class="math display">\[ \mathbf{y} = \mathbf{X}\mathbf{V}\mathbf{V}^\top\pmb{\beta} + \pmb{\varepsilon} = \mathbf{Z}\pmb{\alpha} + \pmb{\varepsilon} \]</span></p>
<p>其中 <span class="math inline">\(\pmb{\alpha} = \mathbf{V}^\top\pmb{\beta}\)</span> 是主成分回归中的系数向量。</p>
<p>选择前 <span class="math inline">\(k\)</span> (<span class="math inline">\(k \leq p\)</span>) 个主成分，得到约简模型： <span class="math display">\[ \mathbf{y} = \mathbf{Z}_k\pmb{\alpha}_k + \pmb{\varepsilon}^* \]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(\mathbf{Z}_k = \mathbf{X}\mathbf{V}_k\)</span>，<span class="math inline">\(\mathbf{V}_k = [\mathbf{v}_1, \dots, \mathbf{v}_k]\)</span></li>
<li><span class="math inline">\(\pmb{\alpha}_k = (\alpha_1, \dots, \alpha_k)^\top\)</span></li>
</ul>
<p>对约简模型应用最小二乘法： <span class="math display">\[ \hat{\pmb{\alpha}}_k = (\mathbf{Z}_k^\top \mathbf{Z}_k)^{-1} \mathbf{Z}_k^\top \mathbf{y} \]</span></p>
<p>由于 <span class="math inline">\(\mathbf{Z}_k^\top \mathbf{Z}_k = (n-1)\pmb{\Lambda}_k\)</span>，其中 <span class="math inline">\(\pmb{\Lambda}_k = \text{diag}(\lambda_1, \dots, \lambda_k)\)</span>，可得： <span class="math display">\[ \hat{\pmb{\alpha}}_k = \frac{1}{n-1} \pmb{\Lambda}_k^{-1} \mathbf{Z}_k^\top \mathbf{y} \]</span></p>
<p>分量形式为： <span class="math display">\[ \hat{\alpha}_j = \frac{\mathbf{z}_j^\top \mathbf{y}}{(n-1)\lambda_j}, \quad j=1,\dots,k \]</span></p>
<p>原始参数 <span class="math inline">\(\pmb{\beta}\)</span> 的PCR估计为： <span class="math display">\[ \hat{\pmb{\beta}}_{\text{PCR}} = \mathbf{V}_k \hat{\pmb{\alpha}}_k = \sum_{j=1}^k \mathbf{v}_j \hat{\alpha}_j \]</span></p>
<p>代入 <span class="math inline">\(\hat{\alpha}_j\)</span> 表达式： <span class="math display">\[ \hat{\pmb{\beta}}_{\text{PCR}} = \sum_{j=1}^k \mathbf{v}_j \frac{\mathbf{z}_j^\top \mathbf{y}}{(n-1)\lambda_j} = \sum_{j=1}^k \mathbf{v}_j \frac{\mathbf{v}_j^\top \mathbf{X}^\top \mathbf{y}}{(n-1)\lambda_j} \]</span></p>
<p><strong>算法步骤总结</strong></p>
<ul>
<li><p>中心化 <span class="math inline">\(\mathbf{X}\)</span> 和 <span class="math inline">\(\mathbf{y}\)</span></p></li>
<li><p>计算 <span class="math inline">\(\mathbf{X}\)</span> 的协方差矩阵 <span class="math inline">\(\mathbf{S}\)</span></p></li>
<li><p>对 <span class="math inline">\(\mathbf{S}\)</span> 进行特征分解，得到 <span class="math inline">\(\mathbf{V}\)</span> 和 <span class="math inline">\(\pmb{\Lambda}\)</span></p></li>
<li><p>选择主成分个数 <span class="math inline">\(k\)</span></p></li>
<li><p>计算主成分得分 <span class="math inline">\(\mathbf{Z}_k = \mathbf{X}\mathbf{V}_k\)</span></p></li>
<li><p>用OLS估计 <span class="math inline">\(\mathbf{y}\)</span> 对 <span class="math inline">\(\mathbf{Z}_k\)</span> 的回归系数 <span class="math inline">\(\hat{\pmb{\alpha}}_k\)</span></p></li>
<li><p>转换回原始空间：<span class="math inline">\(\hat{\pmb{\beta}}_{\text{PCR}} = \mathbf{V}_k \hat{\pmb{\alpha}}_k\)</span></p></li>
</ul>
<p><strong>选择主成分个数</strong> <span class="math inline">\(k\)</span> 的常用方法：</p>
<ol type="1">
<li><p><strong>累积方差贡献率</strong>：选择最小的 <span class="math inline">\(k\)</span> 使得 <span class="math display">\[    \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j} \geq \tau    \]</span> 通常取 <span class="math inline">\(\tau = 0.7 \sim 0.9\)</span></p></li>
<li><p><strong>交叉验证</strong>：通过最小化预测误差的交叉验证估计来选择 <span class="math inline">\(k\)</span></p></li>
<li><p><strong>特征值大于1准则</strong>（适用于标准化数据）</p></li>
</ol>
</section>
</section>
<section id="估计量的性质" class="level2">
<h2 class="anchored" data-anchor-id="估计量的性质">5.3.3 估计量的性质</h2>
<ol type="1">
<li>偏差</li>
</ol>
<p>PCR估计是有偏估计（除非 <span class="math inline">\(k=p\)</span>）： <span class="math display">\[
  \mathbb{E}[\hat{\pmb{\beta}}_{\text{PCR}}] = \mathbf{V}_k\mathbf{V}_k^\top\pmb{\beta}
\]</span></p>
<p>偏差为： <span class="math display">\[
\text{Bias}(\hat{\pmb{\beta}}_{\text{PCR}}) = (\mathbf{V}_k\mathbf{V}_k^\top - \mathbf{I})\pmb{\beta} = -\sum_{j=k+1}^p \mathbf{v}_j\mathbf{v}_j^\top\pmb{\beta}
\]</span></p>
<ol start="2" type="1">
<li>方差</li>
</ol>
<p><span class="math display">\[
\text{Var}(\hat{\pmb{\beta}}_{\text{PCR}}) = \sigma^2 \mathbf{V}_k \pmb{\Lambda}_k^{-1} \mathbf{V}_k^\top = \sigma^2 \sum_{j=1}^k \frac{\mathbf{v}_j\mathbf{v}_j^\top}{(n-1)\lambda_j}
\]</span></p>
<ol start="3" type="1">
<li>均方误差</li>
</ol>
<p><span class="math display">\[
\text{MSE}(\hat{\pmb{\beta}}_{\text{PCR}}) = \text{Var}(\hat{\pmb{\beta}}_{\text{PCR}}) + \|\text{Bias}(\hat{\pmb{\beta}}_{\text{PCR}})\|^2
\]</span></p>
<p>当存在小特征值时，PCR通过牺牲偏差来显著降低方差，从而可能获得比OLS更小的均方误差。</p>
</section>
<section id="偏最小二乘回归" class="level2">
<h2 class="anchored" data-anchor-id="偏最小二乘回归">5.4 偏最小二乘回归</h2>
<section id="pls的基本思想" class="level3">
<h3 class="anchored" data-anchor-id="pls的基本思想">5.4.1 PLS的基本思想</h3>
<p>潜变量构建： 寻找潜变量 <span class="math inline">\(\mathbf{t} = \mathbf{X}\mathbf{w}\)</span>，使得： 1. <span class="math inline">\(\mathbf{t}\)</span> 的方差尽可能大 2. <span class="math inline">\(\mathbf{t}\)</span> 与 <span class="math inline">\(\mathbf{y}\)</span> 的相关性尽可能大</p>
<p>优化问题：</p>
<p><span class="math display">\[
\begin{aligned}
\max_{\mathbf{w}} &amp; \quad \text{Cov}(\mathbf{X}\mathbf{w}, \mathbf{y}) \\
\text{s.t.} &amp; \quad \|\mathbf{w}\|_2 = 1
\end{aligned}
\]</span></p>
</section>
<section id="pls算法nipals" class="level3">
<h3 class="anchored" data-anchor-id="pls算法nipals">5.4.2 PLS算法（NIPALS）</h3>
<p>初始化</p>
<p>令： <span class="math display">\[E_0 = X - \bar{X}, \quad f_0 = y - \bar{y}\]</span> 其中<span class="math inline">\(E_0\)</span>和<span class="math inline">\(f_0\)</span>是中心化后的数据。</p>
<p>第h个成分的提取</p>
<p>步骤1：计算权重向量<span class="math inline">\(w_h\)</span></p>
<p><span class="math display">\[w_h = \frac{E_{h-1}^T f_{h-1}}{\|E_{h-1}^T f_{h-1}\|}\]</span></p>
<p><strong>解释</strong>：找到<span class="math inline">\(X\)</span>中与当前<span class="math inline">\(y\)</span>残差最相关的方向。</p>
<p>步骤2：计算得分向量<span class="math inline">\(t_h\)</span></p>
<p><span class="math display">\[t_h = E_{h-1} w_h\]</span></p>
<p><strong>解释</strong>：<span class="math inline">\(X\)</span>在权重方向上的投影，即潜变量。</p>
<p>步骤3：计算X载荷向量<span class="math inline">\(p_h\)</span></p>
<p><span class="math display">\[p_h = \frac{E_{h-1}^T t_h}{t_h^T t_h}\]</span></p>
<p><strong>解释</strong>：<span class="math inline">\(t_h\)</span>对原始<span class="math inline">\(X\)</span>变量的解释程度。</p>
<p>步骤4：计算y载荷<span class="math inline">\(c_h\)</span></p>
<p><span class="math display">\[c_h = \frac{f_{h-1}^T t_h}{t_h^T t_h}\]</span></p>
<p><strong>解释</strong>：<span class="math inline">\(t_h\)</span>对<span class="math inline">\(y\)</span>的解释程度（标量）。</p>
<p>步骤5：更新残差</p>
<p><span class="math display">\[E_h = E_{h-1} - t_h p_h^T\]</span> <span class="math display">\[f_h = f_{h-1} - c_h t_h\]</span></p>
</section>
</section>
<section id="案例分析" class="level2">
<h2 class="anchored" data-anchor-id="案例分析">5.5 案例分析</h2>
</section>
<section id="本章总结" class="level2">
<h2 class="anchored" data-anchor-id="本章总结">本章总结</h2>
<p>核心公式回顾 1. 主成分：<span class="math inline">\(\mathbf{Z} = \mathbf{X}\mathbf{V}\)</span>，其中 <span class="math inline">\(\mathbf{S} = \mathbf{V}\pmb{\Lambda}\mathbf{V}'\)</span> 2. 坐标下降：<span class="math inline">\(\beta_j \leftarrow S\left(\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - \tilde{y}_i^{(j)}), \lambda\right)\)</span></p>
<p>方法选择指南</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 18%">
<col style="width: 24%">
<col style="width: 19%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>核心思想</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>最优子集</td>
<td>穷举搜索</td>
<td>全局最优解</td>
<td>计算量大，不可行</td>
<td>变量数较少(&lt;20)</td>
</tr>
<tr class="even">
<td>逐步回归</td>
<td>逐步筛选</td>
<td>计算高效，自动选择</td>
<td>局部最优，结果不稳定</td>
<td>初步变量筛选</td>
</tr>
<tr class="odd">
<td>主成分回归</td>
<td>正交变换</td>
<td>消除共线性，稳定</td>
<td>可解释性差</td>
<td>强共线性，预测为主</td>
</tr>
<tr class="even">
<td>偏最小二乘</td>
<td>协同降维</td>
<td>利用Y信息，预测好</td>
<td>计算复杂，理论深</td>
<td>高维数据，预测精度要求高</td>
</tr>
</tbody>
</table>
<p>实践建议</p>
<ol type="1">
<li>数据预处理：标准化自变量，确保惩罚的公平性</li>
<li>模型评估：使用交叉验证，避免过拟合</li>
<li>结果验证：在测试集上评估预测性能</li>
<li>稳健性检查：尝试不同方法，比较结果一致性</li>
<li>领域知识：结合实际问题背景选择合适方法</li>
</ol>
<p>“传统变量选择方法如逐步回归，虽然应用广泛，但存在选择结果不稳定、忽略变量间理论关系等局限性。而降维方法如主成分回归，虽然解决了共线性问题，却损失了原始变量的可解释性。20世纪70年代以来，一种基于惩罚似然的新思路——正则化方法逐渐发展起来，它通过在损失函数中加入惩罚项，同时实现参数估计和变量选择。下一章将系统介绍岭回归、Lasso及其变体，这些方法不仅能够处理多重共线性，还能产生稀疏解，在高维数据分析中表现出显著优势。”</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "已复制");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "已复制");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/part-2-introduction.html" class="pagination-link" aria-label="part-2-introduction.html">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">part-2-introduction.html</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/6 正则化.html" class="pagination-link" aria-label="6 正则化">
        <span class="nav-page-text"><span class="chapter-title">6 正则化</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>