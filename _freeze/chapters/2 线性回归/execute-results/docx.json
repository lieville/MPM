{
  "hash": "52a2da3a626e7b4ef77915357a2852e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2 线性回归\"\nformat: html\neditor: visual\n---\n\n\n\n## 本章导读\n\n线性回归和线性分析是一个强大而又经久不衰的工具，其思想的简洁性、数学的优雅性、解释的直观性以及广泛的可扩展性让人赞叹。理解线性回归，不仅是掌握一个工具，更是理解现代定量科学如何从数据中提取信息、建立模型的基本范式，它是通往更复杂数据世界的一扇不可或缺的大门。下面我们从历史的角度回溯线性回归思想的历程：\n\n（1）起源：从“平均主义”到“关系量化”（18-19世纪）\n\n线性回归的萌芽并非源于复杂的数学理论，而是来自天文学和测量学中对“误差”处理的迫切需求。\n\na)核心问题： 在测量同一天文现象（如行星轨道）时，不同观测者或同一观测者的多次测量会得到略有差异的结果。哪一个是最可信的“真值”？\n\nb)先驱与关键思想：\n\n-   高斯与勒让德：德国数学家高斯和法国数学家勒让德在18世纪末至19世纪初独立提出了最小二乘法。\n\n-   核心思想：最有可能的“真值”或最佳拟合线，是能使所有观测值与预测值之差的平方和达到最小的那条线。误差平方（而非绝对差）在数学上更易处理，且能惩罚大的误差。\n\n-   首次应用：勒让德在1805年明确发表了最小二乘法，用于分析彗星轨道；高斯声称更早使用它预测了谷神星的轨迹。\n\n此时，线性回归更多地被视为一种精妙的“数值计算技术”，而非一个统计模型。\n\n（2）发展：奠定统计学基石（19世纪末 - 20世纪中叶）\n\n随着生物学、经济学等社会科学对数据分析的需求增长，学者们开始探究这种“关系”背后的不确定性和统计意义。\n\na)弗朗西斯·高尔顿： 被誉为回归的“概念之父”。他在1886年研究遗传学（身高遗传）时发现，虽然高个子父母的孩子也倾向于高，但其身高会“回归”到一个平均趋势。 他引入了 “回归”（Regression） 一词，原意指“倒退、退回平均值”。这一现象揭示了相关但非完美的关系，是现代相关与回归分析的起点。\n\nb)卡尔·皮尔逊： 高尔顿的学生，将回归与相关分析系统化、数学化。 他建立了皮尔逊相关系数，并发展了完整的双变量正态分布理论，为回归分析提供了坚实的概率分布基础。\n\nc)罗纳德·费希尔： 20世纪统计学巨擘，完成了线性回归的现代统计框架。 他的关键贡献在于：方差分析、显著性检验、最大似然估计。 他将回归从描述性工具升级为推断性工具——我们不仅能拟合一条线，还能检验斜率是否显著不为零（即变量间是否存在统计显著的关系），并进行预测和置信区间估计。\n\n至此，线性回归从一个“曲线拟合”方法，演变为一套完整的、基于概率论的统计推断体系。\n\n（3）成熟与扩展：应对复杂现实（20世纪下半叶至今）\n\n计算机的出现和科学研究的复杂化，推动了线性回归模型的极大丰富。\n\na)计算革命： 最小二乘法的求解涉及矩阵运算。计算机使快速处理海量数据和多个变量成为可能，催生了多元线性回归的广泛应用。\n\nb)理论扩展与变体：\n\n-   广义线性模型： 由内尔德和韦德伯恩提出，允许因变量不限于连续正态分布（如二项分布的Logistic回归、泊松分布的计数数据回归）。\n\n-   正则化方法： 为应对高维数据、多重共线性和过拟合问题，产生了岭回归、Lasso回归等，它们在损失函数中加入惩罚项，提升模型稳健性和可解释性。\n\n-   稳健回归： 针对异常值敏感问题，发展出如Huber回归等方法。\n\nc)成为机器学习的基础：\n\n-   在机器学习领域，线性回归被视为最简单的监督学习算法，是理解参数学习、梯度下降优化等概念的理想起点。\n\n-   它也是许多复杂模型（如神经网络中的单个神经元）的构成模块。\n\n（4）核心思想与局限\n\na)核心思想： 建模确定性关系： 假设因变量（Y）可以由自变量（X）的线性组合加上一个随机误差来完美解释。 目标： 通过数据估计线性方程的系数，以量化X变化时Y的平均变化量，并用于预测和解释。\n\nb)主要局限：\n\n-   线性假设强： 无法直接捕捉变量间的非线性关系。\n\n-   对假设敏感： 要求误差独立、同方差、正态分布等，现实数据常不严格满足。\n\n-   易受异常值影响。\n\n-   相关非因果： 这是最重要的一点。回归揭示的是关联性，而非因果性。忽视混淆变量会导致错误的因果解读。\n\n线性回归的发展脉络清晰可见：起源于天文学的误差处理（最小二乘法），概念化于生物学的遗传研究（高尔顿的“回归”），体系化于统计学的推断革命（皮尔逊、费希尔），扩展化于计算机时代的复杂数据分析（正则化、广义模型）， 基础化于人工智能/机器学习的模型基石。\n\n线性回归是预测建模最基础、最重要的方法。本章将系统介绍线性回归模型的三种表述形式，详细推导参数估计的两种主要方法——最小二乘估计和极大似然估计，深入探讨模型拟合优度的评价指标，并建立完整的统计推断框架。最后，我们将讨论如何利用建立好的模型进行预测，为后续的模型诊断和扩展奠定坚实基础。\n\n## 2.1 线性回归模型\n\n变量之间的关系：函数关系，统计关系，没有关系\n\n回忆数理统计的内容，相关系数：\n\n$$\n\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n$$ $$\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n$$\n\n取值范围： $-1 ≤ ρ ≤ 1$。\n\n-   $ρ = 1$：完全正相关（所有点落在一条斜向上的直线上）。\n\n-   $ρ = -1$：完全负相关（所有点落在一条斜向下的直线上）。\n\n-   $ρ = 0$：总体中**不存在线性相关**。但这不意味着没有其他关系（如曲线关系）。\n\n我们几乎永远无法收集到整个总体的数据，因此 $ρ$ 对我们来说通常是未知的。\n\n样本相关系数由英国统计学家卡尔·皮尔逊提出，故常称“皮尔逊积矩相关系数”。 公式本质是样本协方差除以各自样本标准差的乘积取值范围： $-1 ≤ r ≤ 1$。 它是一个统计量，如果你从同一个总体中抽取不同的样本，计算出的 $r$ 值会不同。这种波动称为抽样变异。\n\n### 2.1.1 总体回归模型\n\n总体回归模型： $$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n$$\n\n其中：\n\n-   $\\beta_0$ 是截距项\n\n-   $\\beta_1, \\beta_2, \\cdots, \\beta_p$ 是回归系数\n\n-   $\\varepsilon$ 是随机误差项，满足 $E(\\varepsilon|X) = 0$\n\n    总体回归函数描述了因变量 $Y$ 与自变量 $X_1, X_2, \\cdots, X_p$ 之间的真实关系：\n\n    $$\n    E(Y|X_1, \\cdots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\n    $$\n\n### 2.1.2 样本回归模型\n\n基于 $n$ 个观测样本 $(x_{i1}, x_{i2}, \\cdots, x_{ip}, y_i)$, $i=1,2,\\cdots,n$，样本回归模型为：\n\n$$\ny_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\cdots + \\hat{\\beta}_p x_{ip} + e_i\n$$\n\n其中：\n\n-   $\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_p$ 是总体参数的估计值\n\n-   $e_i = y_i - \\hat{y}_i$ 是第 $i$ 个观测的残差\n\n-   $\\hat{y}_i$ 是第 $i$ 个观测的拟合值\n\n样本回归函数：\n\n$$\n\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\cdots + \\hat{\\beta}_p x_{ip} \n$$\n\n### 2.1.3 矩阵形式的回归模型\n\n令： $$\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix},\n\\quad \n\\mathbf{X} = \\begin{pmatrix} \n1 & x_{11} & \\cdots & x_{1p} \\\\\n1 & x_{21} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & \\cdots & x_{np} \n\\end{pmatrix},\n\\quad \n\\pmb{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix},\n\\quad \n\\pmb{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\n\n则线性回归模型的矩阵形式为： $$\n\\mathbf{y} = \\mathbf{X}\\pmb{\\beta} + \\pmb{\\varepsilon}\n$$\n\n其中：\n\n-   $\\mathbf{y}$ 是 $n \\times 1$ 的因变量向量\n\n-   $\\mathbf{X}$ 是 $n \\times (p+1)$ 的设计矩阵\n\n-   $\\pmb{\\beta}$ 是 $(p+1) \\times 1$ 的参数向量\n\n-   $\\pmb{\\varepsilon}$ 是 $n \\times 1$ 的误差项向量\n\n### 2.1.4 基本假设\n\n假设 1：线性关系\n\n-   内容：因变量 $Y$ 与自变量 $X$ 之间存在线性关系，且参数 $β$ 是线性的。误差项 $ε$ 以相加的形式进入模型。\n\n-   含义：$X$ 变化一个单位，$Y$ 平均变化 $β$ 个单位，与 $X$ 本身的取值无关。\n\n-   违反后果：模型无法捕捉真实关系，导致预测偏差。例如，真实关系是二次的，用线性模型会系统性地高估或低估。\n\n-   检验与处理：绘制 $Y$ 与每个 $X$ 的散点图；检查残差与拟合值的图。若违反，可对 $X$ 或 $Y$ 进行变量变换（如取对数、平方），或使用多项式回归。\n\n假设 2：随机抽样\n\n-   内容：样本数据是从总体中随机抽样得到的。\n\n-   含义：样本能代表总体，避免选择偏差。\n\n-   违反后果：样本估计无法推广到总体（外部效度低）。例如，只用大学生数据研究全民收入。\n\n-   检验与处理：无法从数据本身直接诊断，需了解抽样过程。处理方法是改进抽样设计。\n\n假设 3：严格外生性\n\n-   内容：给定所有自变量 $X$ 的值，误差项 $ε$ 的条件期望为零。即 $E(ε|X) = 0$。\n\n-   含义：模型中没有遗漏重要的解释变量，且所有包含的 $X$ 都与误差项 $ε$ 不相关。换句话说，$X$ 是“外生”的。\n\n-   违反后果（最严重之一）：导致 “内生性” 问题，使参数估计 $β$ 有偏且不一致。无论样本多大，偏差都不会消失。\n\n-   常见原因：遗漏变量、测量误差、互为因果（反向因果）。\n\n-   检验与处理：难以直接检验。通常基于理论推理。处理需用工具变量法、面板数据模型、断点回归等高级方法。\n\n假设 4：无完全多重共线性\n\n-   内容：自变量之间不存在完全的线性关系。\n\n-   含义：没有一个自变量可以精确地由其他自变量的线性组合表示。样本数据中，各 $X$ 提供独立信息。 \\* 违反后果：OLS 无法唯一求解参数估计值（矩阵不可逆）。\n\n-   检验：查看相关系数矩阵；计算方差膨胀因子和条件数。\n\n-   处理：删除高度相关的变量之一；使用主成分回归或岭回归等降维/正则化方法。\n\n假设 5：球形误差（同方差与无自相关） 这是两个子假设的合称：\n\n5a) 同方差性：给定所有 $X$，误差项 $ε$ 的条件方差为常数。即 $Var(ε|X) = σ²$。\n\n-   违反后果：OLS估计量仍无偏，但不再是最有效的（方差不是最小）；标准误估计有偏，导致假设检验失效。\n\n-   检验：绘制残差与拟合值的散点图，看是否呈漏斗形、扇形等。正式检验有Breusch-Pagan检验、White检验等。\n\n-   处理：使用稳健标准误（最常见且简单）、加权最小二乘法、对变量进行变换。\n\n5b) 无自相关：对于不同观测 $i$ 和 $j$，其误差项互不相关。即 $Cov(εᵢ, εⱼ|X) = 0$ (i ≠ j)。\n\n-   违反后果：与异方差类似，效率降低，标准误估计有偏（通常偏低），导致 $t$ 统计量虚高，易得出显著结论。\n\n-   常见于：时间序列数据（本期误差受上期影响）、空间数据。\n\n-   检验：绘制残差与时间/顺序的图；使用Durbin-Watson检验（针对一阶自相关）。\n\n-   处理：时间序列模型（如ARIMA）、使用异方差自相关稳健标准误、广义最小二乘法。\n\n假设 6：误差项的正态性（可选但重要） \\* 内容：误差项 $ε$ 服从正态分布。即 $ε|X ~ N(0, σ²I)$。\n\n-   含义：在样本量不大时，此假设是进行精确统计推断（如 $t$ 检验、$F$ 检验）的基础。\n\n-   违反后果：在小样本下，$β$ 的分布不是精确的 $t$ 分布，检验和置信区间可能不准确。但在大样本下（中心极限定理），OLS估计量近似正态，此假设可放松。\n\n-   检验：绘制残差的正态Q-Q图；使用Shapiro-Wilk、Kolmogorov-Smirnov等检验。\n\n-   处理：增大样本量；使用自助法进行推断；考虑对 $Y$ 进行变换。\n\n这六个假设可以按目标归类：\n\n| 目标 | 核心假设 | 关键点 |\n|:-----------------------|:-----------------------|:-----------------------|\n| 估计无偏 | 1\\. 线性关系<br>3. 严格外生性 | 模型设定正确，无遗漏变量和反向因果。 |\n| 估计可算 | 4\\. 无完全多重共线性 | 自变量提供独立信息，矩阵可逆。 |\n| 估计有效（BLUE） | 5\\. 球形误差（同方差+无自相关） | OLS估计量的方差最小。 |\n| 推断可靠 | 6\\. 误差正态性（小样本）<br>+ 以上所有 | 可用于假设检验和置信区间。 |\n\n## 2.2 参数估计方法\n\n### 2.2.1 普通最小二乘估计\n\n基本原理：寻找参数估计值 $\\hat{\\pmb{\\beta}}$，使得残差平方和最小。\n\n残差平方和： $$\nSSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (\\mathbf{y} - \\mathbf{X}\\pmb{\\beta})'(\\mathbf{y} - \\mathbf{X}\\pmb{\\beta})\n$$\n\n最小二乘估计量： 通过对 $SSE$ 关于 $\\pmb{\\beta}$ 求导并令导数为零，可得正规方程组： $$\n\\mathbf{X}'\\mathbf{X}\\hat{\\pmb{\\beta}} = \\mathbf{X}'\\mathbf{y}\n$$\n\n当 $\\mathbf{X}'\\mathbf{X}$ 可逆时，OLS估计量为： $$\n\\hat{\\pmb{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\n$$\n\nOLS估计量的性质（在经典假设下）：\n\n1\\. 无偏性：$E(\\hat{\\pmb{\\beta}}) = \\pmb{\\beta}$\n\n2\\. 方差-协方差矩阵：$\\text{Var}(\\hat{\\pmb{\\beta}}) = \\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}$\n\n3\\. 高斯-马尔可夫定理：OLS估计量是最佳线性无偏估计\n\n### 2.2.2 极大似然估计\n\n基本思想：在正态误差的假设下，选择使样本观测值出现概率最大的参数值。\n\n似然函数：假设 $\\varepsilon_i \\sim N(0, \\sigma^2)$，则 $y_i \\sim N(\\mathbf{x}_i'\\pmb{\\beta}, \\sigma^2)$，似然函数为： $$\nL(\\pmb{\\beta}, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i'\\pmb{\\beta})^2}{2\\sigma^2}\\right)\n$$\n\n对数似然函数： $$\n\\ell(\\pmb{\\beta}, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i'\\pmb{\\beta})^2\n$$\n\n极大似然估计量： - 回归系数：$\\hat{\\pmb{\\beta}}_{MLE} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$（与OLS相同） - 误差方差：$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{x}_i'\\hat{\\pmb{\\beta}})^2$\n\n注意：$\\hat{\\sigma}^2_{MLE}$ 是有偏估计，通常使用无偏估计： $$\ns^2 = \\frac{1}{n-p-1} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n$$\n\n### 2.2.3 点估计与区间估计\n\n系数估计： - 点估计：$\\hat{\\beta}_j$（OLS或MLE估计值） - 标准误：$SE(\\hat{\\beta}_j) = s \\sqrt{[(\\mathbf{X}'\\mathbf{X})^{-1}]_{jj}}$\n\n系数的置信区间（置信水平 $1-\\alpha$）： $$\n\\hat{\\beta}_j \\pm t_{1-\\alpha/2}(n-p-1) \\cdot SE(\\hat{\\beta}_j)\n$$\n\n误差方差的置信区间： $$\n\\left[ \\frac{(n-p-1)s^2}{\\chi^2_{1-\\alpha/2}(n-p-1)}, \\frac{(n-p-1)s^2}{\\chi^2_{\\alpha/2}(n-p-1)} \\right]\n$$\n\n**最小二乘法实现**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 100\nx <- seq(1, 10, length.out = n)\ny <- 2 + 1.5 * x + rnorm(n, 0, 1.5)\n\n# 手动实现OLS\nmanual_ols <- function(x, y) {\n  x_mean <- mean(x)\n  y_mean <- mean(y)\n  slope <- sum((x - x_mean) * (y - y_mean)) / sum((x - x_mean)^2)\n  intercept <- y_mean - slope * x_mean\n  return(c(intercept = intercept, slope = slope))\n}\n\nmanual_coef <- manual_ols(x, y)\nlm_coef <- coef(lm(y ~ x))\n\ncat(\"参数估计比较:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n参数估计比较:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.frame(\n  方法 = c(\"手动OLS\", \"lm函数\"),\n  截距 = c(manual_coef[1], lm_coef[1]),\n  斜率 = c(manual_coef[2], lm_coef[2])\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               方法     截距     斜率\nintercept   手动OLS 1.907728 1.541433\n(Intercept)  lm函数 1.907728 1.541433\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import fetch_openml\n\n# 设置样式\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# 生成数据\nnp.random.seed(123)\nn = 100\nx = np.linspace(1, 10, n)\ny = 2 + 1.5 * x + np.random.normal(0, 1.5, n)\n\ndef manual_ols(x, y):\n    \"\"\"手动实现OLS\"\"\"\n    x_mean, y_mean = np.mean(x), np.mean(y)\n    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n    intercept = y_mean - slope * x_mean\n    return intercept, slope\n\n# 比较结果\nmanual_intercept, manual_slope = manual_ols(x, y)\nlr = LinearRegression()\nlr.fit(x.reshape(-1, 1), y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinearRegression()\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"参数估计比较:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n参数估计比较:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"{'方法':<10} {'截距':<8} {'斜率':<8}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n方法         截距       斜率      \n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"{'手动OLS':<10} {manual_intercept:.4f}  {manual_slope:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n手动OLS      2.0149  1.5047\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"{'Sklearn':<10} {lr.intercept_:.4f}  {lr.coef_[0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSklearn    2.0149  1.5047\n```\n\n\n:::\n:::\n\n\n## 2.3 拟合优度\n\n### 2.3.1 总平方和分解\n\n总平方和：$SST = \\sum_{i=1}^n (y_i - \\bar{y})^2$ 回归平方和：$SSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$ 残差平方和：$SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n\n平方和分解公式： $$\nSST = SSR + SSE\n$$\n\n### 2.3.2 决定系数\n\n定义： $$\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n$$\n\n性质： - $0 \\leq R^2 \\leq 1$ - $R^2$ 衡量模型对因变量变异的解释比例 - $R^2$ 随自变量增加而增加，可能过拟合\n\n### 2.3.3 调整的决定系数\n\n定义： $$\nR^2_{adj} = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - (1-R^2)\\frac{n-1}{n-p-1}\n$$\n\n优点：对模型复杂度施加惩罚，更适合模型比较\n\n## 2.4 假设检验\n\n### 2.4.1 模型整体显著性检验（F检验）\n\n原假设：$H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$\n\nF统计量： $$\nF = \\frac{SSR/p}{SSE/(n-p-1)} = \\frac{MSR}{MSE} \\sim F(p, n-p-1)\n$$\n\n决策规则：如果 $F > F_{1-\\alpha}(p, n-p-1)$，拒绝原假设，认为模型整体显著\n\n### 2.4.2 单个系数显著性检验（t检验）\n\n原假设：$H_0: \\beta_j = 0$\n\nt统计量： $$\nt = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} \\sim t(n-p-1)\n$$\n\n决策规则：如果 $|t| > t_{1-\\alpha/2}(n-p-1)$，拒绝原假设，认为该自变量对因变量有显著影响\n\n**交互演示：t检验与p值**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2.3.2 交互式假设检验演示\ninteractive_hypothesis_test <- function() {\n  if (!require(manipulate)) {\n    stop(\"请安装manipulate包: install.packages('manipulate')\")\n  }\n  \n  set.seed(123)\n  \n  manipulate({\n    # 生成数据\n    n <- sample_size\n    x <- runif(n, 0, 10)\n    true_slope <- 0  # 零假设为真\n    y <- 1 + true_slope * x + rnorm(n, 0, noise_sd)\n    \n    data <- data.frame(x = x, y = y)\n    model <- lm(y ~ x, data = data)\n    summary_model <- summary(model)\n    \n    # 提取t统计量和p值\n    t_stat <- summary_model$coefficients[2, \"t value\"]\n    p_value <- summary_model$coefficients[2, \"Pr(>|t|)\"]\n    \n    # 绘制数据点和回归线\n    par(mfrow = c(1, 2))\n    \n    # 左图：散点图和回归线\n    plot(x, y, pch = 16, col = \"blue\", \n         main = paste(\"数据与回归线\\nn =\", n, \"σ =\", noise_sd),\n         xlab = \"X\", ylab = \"Y\",\n         xlim = c(0, 10), ylim = c(1 - 3*noise_sd, 1 + 3*noise_sd))\n    abline(model, col = \"red\", lwd = 2)\n    abline(h = mean(y), col = \"green\", lwd = 2, lty = 2)\n    legend(\"topright\", \n           legend = c(\"数据\", \"回归线\", \"Y均值\"),\n           col = c(\"blue\", \"red\", \"green\"),\n           lwd = c(NA, 2, 2), pch = c(16, NA, NA),\n           lty = c(NA, 1, 2))\n    \n    # 右图：t分布和p值\n    x_t <- seq(-4, 4, length.out = 100)\n    y_t <- dt(x_t, df = n - 2)\n    \n    plot(x_t, y_t, type = \"l\", lwd = 2, col = \"black\",\n         main = paste(\"t检验: t =\", round(t_stat, 3), \n                     \"p =\", round(p_value, 4)),\n         xlab = \"t统计量\", ylab = \"密度\")\n    \n    # 着色拒绝域\n    alpha <- 0.05\n    critical_value <- qt(1 - alpha/2, df = n - 2)\n    \n    # 左侧拒绝域\n    x_left <- seq(-4, -critical_value, length.out = 50)\n    y_left <- dt(x_left, df = n - 2)\n    polygon(c(-4, x_left, -critical_value), c(0, y_left, 0), \n            col = \"red\", density = 20, angle = 45)\n    \n    # 右侧拒绝域  \n    x_right <- seq(critical_value, 4, length.out = 50)\n    y_right <- dt(x_right, df = n - 2)\n    polygon(c(critical_value, x_right, 4), c(0, y_right, 0),\n            col = \"red\", density = 20, angle = 45)\n    \n    # 标记t统计量\n    abline(v = t_stat, col = \"blue\", lwd = 2, lty = 2)\n    points(t_stat, dt(t_stat, df = n - 2), pch = 16, col = \"blue\", cex = 1.5)\n    text(t_stat, dt(t_stat, df = n - 2) + 0.02, \n         paste(\"t =\", round(t_stat, 3)), pos = 3, col = \"blue\")\n    \n    # 标记临界值\n    abline(v = c(-critical_value, critical_value), \n           col = \"red\", lwd = 1, lty = 2)\n    text(critical_value, 0.3, paste(\"临界值 =\", round(critical_value, 3)), \n         pos = 4, col = \"red\")\n    \n    # 添加决策\n    decision <- ifelse(p_value < alpha, \"拒绝H0\", \"不拒绝H0\")\n    legend(\"topright\", \n           legend = c(paste(\"p值:\", round(p_value, 4)),\n                     paste(\"α:\", alpha),\n                     paste(\"决策:\", decision)),\n           bty = \"n\", cex = 0.8)\n    \n  },\n  sample_size = slider(10, 200, initial = 30, step = 10, label = \"样本量 n\"),\n  noise_sd = slider(0.5, 5, initial = 2, step = 0.5, label = \"噪声标准差 σ\")\n  )\n}\n\n# 运行交互演示（需要RStudio）\n# interactive_hypothesis_test()\n\n# 静态版本\nstatic_hypothesis_test <- function(n = 30, noise_sd = 2) {\n  set.seed(123)\n  \n  # 生成数据\n  x <- runif(n, 0, 10)\n  true_slope <- 0  # 零假设为真\n  y <- 1 + true_slope * x + rnorm(n, 0, noise_sd)\n  \n  data <- data.frame(x = x, y = y)\n  model <- lm(y ~ x, data = data)\n  summary_model <- summary(model)\n  \n  # 提取统计量\n  t_stat <- summary_model$coefficients[2, \"t value\"]\n  p_value <- summary_model$coefficients[2, \"Pr(>|t|)\"]\n  conf_int <- confint(model)[2, ]\n  \n  # 绘制图形\n  par(mfrow = c(1, 2))\n  \n  # 左图：数据与回归线\n  plot(x, y, pch = 16, col = \"blue\", \n       main = paste(\"数据与回归线\\nn =\", n, \"σ =\", noise_sd),\n       xlab = \"X\", ylab = \"Y\")\n  abline(model, col = \"red\", lwd = 2)\n  abline(h = mean(y), col = \"green\", lwd = 2, lty = 2)\n  legend(\"topright\", \n         legend = c(\"数据\", \"回归线\", \"Y均值\"),\n         col = c(\"blue\", \"red\", \"green\"),\n         lwd = c(NA, 2, 2), pch = c(16, NA, NA),\n         lty = c(NA, 1, 2))\n  \n  # 右图：t分布\n  x_t <- seq(-4, 4, length.out = 100)\n  y_t <- dt(x_t, df = n - 2)\n  \n  plot(x_t, y_t, type = \"l\", lwd = 2, col = \"black\",\n       main = paste(\"t检验: t =\", round(t_stat, 3), \n                   \"p =\", round(p_value, 4)),\n       xlab = \"t统计量\", ylab = \"密度\")\n  \n  alpha <- 0.05\n  critical_value <- qt(1 - alpha/2, df = n - 2)\n  \n  # 着色拒绝域\n  x_left <- seq(-4, -critical_value, length.out = 50)\n  y_left <- dt(x_left, df = n - 2)\n  polygon(c(-4, x_left, -critical_value), c(0, y_left, 0), \n          col = \"red\", alpha = 0.3)\n  \n  x_right <- seq(critical_value, 4, length.out = 50)\n  y_right <- dt(x_right, df = n - 2)\n  polygon(c(critical_value, x_right, 4), c(0, y_right, 0),\n          col = \"red\", alpha = 0.3)\n  \n  # 标记统计量\n  abline(v = t_stat, col = \"blue\", lwd = 2, lty = 2)\n  points(t_stat, dt(t_stat, df = n - 2), pch = 16, col = \"blue\", cex = 1.5)\n  \n  # 标记临界值\n  abline(v = c(-critical_value, critical_value), \n         col = \"red\", lwd = 1, lty = 2)\n  \n  # 决策\n  decision <- ifelse(p_value < alpha, \"拒绝H0\", \"不拒绝H0\")\n  legend(\"topright\", \n         legend = c(paste(\"p值:\", round(p_value, 4)),\n                   paste(\"α:\", alpha),\n                   paste(\"决策:\", decision)),\n         bty = \"n\")\n  \n  par(mfrow = c(1, 1))\n  \n  cat(\"假设检验结果:\\n\")\n  cat(\"t统计量:\", round(t_stat, 4), \"\\n\")\n  cat(\"p值:\", round(p_value, 4), \"\\n\")\n  cat(\"95%置信区间: [\", round(conf_int[1], 4), \",\", round(conf_int[2], 4), \"]\\n\")\n  cat(\"决策:\", decision, \"\\n\")\n  \n  return(list(\n    model = model,\n    t_stat = t_stat,\n    p_value = p_value,\n    decision = decision\n  ))\n}\n\n# 运行静态检验演示\nstatic_test <- static_hypothesis_test(n = 50, noise_sd = 1.5)\n```\n\n::: {.cell-output-display}\n![](2-线性回归_files/figure-docx/unnamed-chunk-3-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n假设检验结果:\nt统计量: 0.8363 \np值: 0.4071 \n95%置信区间: [ -0.0804 , 0.1949 ]\n决策: 不拒绝H0 \n```\n\n\n:::\n:::\n\n\n### 2.4.3 系数线性组合的检验\n\n原假设：\n\n$H_0: \\mathbf{C}\\pmb{\\beta} = \\mathbf{d}$\n\nF统计量： $$\nF = \\frac{(\\mathbf{C}\\hat{\\pmb{\\beta}} - \\mathbf{d})'[\\mathbf{C}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{C}']^{-1}(\\mathbf{C}\\hat{\\pmb{\\beta}} - \\mathbf{d})}{q \\cdot MSE} \\sim F(q, n-p-1)\n$$\n\n其中 $\\mathbf{C}$ 是 $q \\times (p+1)$ 的约束矩阵，$\\mathbf{d}$ 是 $q \\times 1$ 的常数向量\n\n**模型显著性检验**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(y ~ x)\nsummary_model <- summary(model)\n\ncat(\"模型显著性检验:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n模型显著性检验:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"F统计量:\", round(summary_model$fstatistic[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF统计量: 869.473 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p值:\", format.pval(pf(summary_model$fstatistic[1], \n                          summary_model$fstatistic[2], \n                          summary_model$fstatistic[3], \n                          lower.tail = FALSE)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np值: < 2.22e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n系数检验:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n系数检验:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(summary_model$coefficients, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   1.9077     0.3186  5.9885        0\nx             1.5414     0.0523 29.4868        0\n```\n\n\n:::\n:::\n\n\n## 2.5 预测\n\n### 2.5.1 点预测\n\n对于新的自变量观测 $\\mathbf{x}_0 = (1, x_{01}, x_{02}, \\cdots, x_{0p})'$，因变量的点预测为： $$\n\\hat{y}_0 = \\mathbf{x}_0'\\hat{\\pmb{\\beta}}\n$$\n\n### 2.5.2 区间预测\n\n均值的置信区间（估计条件均值的置信区间）： $$\n\\hat{y}_0 \\pm t_{1-\\alpha/2}(n-p-1) \\cdot s \\sqrt{\\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0}\n$$\n\n个别值的预测区间（预测单个新观测值的区间）： $$\n\\hat{y}_0 \\pm t_{1-\\alpha/2}(n-p-1) \\cdot s \\sqrt{1 + \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0}\n$$\n\n重要区别： - 均值置信区间：估计的是 $E(y|\\mathbf{x}_0)$ 的置信区间 - 个别值预测区间：预测的是单个 $y_0$ 的区间，包含额外的误差方差项\n\n### 2.5.3 预测精度的影响因素\n\n1.  样本量 $n$：样本量越大，预测越精确\n2.  自变量变异：$\\mathbf{X}'\\mathbf{X}$ 的行列式值越大，预测越精确\n3.  杠杆值：$\\mathbf{x}_0$ 与样本中心的距离影响预测方差\n4.  误差方差：$\\sigma^2$ 越小，预测越精确\n\n## 2.6 案例分析\n\n## 本章总结\n\n核心公式回顾\n\n1.  OLS估计：$\\hat{\\pmb{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$\n2.  系数方差：$\\text{Var}(\\hat{\\pmb{\\beta}}) = \\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}$\n3.  拟合优度：$R^2 = 1 - \\frac{SSE}{SST}$\n4.  F检验：$F = \\frac{MSR}{MSE} \\sim F(p, n-p-1)$\n5.  t检验：$t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} \\sim t(n-p-1)$\n6.  预测区间：$\\hat{y}_0 \\pm t_{\\alpha/2} \\cdot s \\sqrt{1 + \\mathbf{x}_0'(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{x}_0}$\n\n关键概念\n\n| 概念      | 定义                | 应用                  |\n|-----------|---------------------|-----------------------|\n| OLS估计   | 最小化残差平方和    | 参数估计的主要方法    |\n| MLE估计   | 最大化似然函数      | 在正态假设下与OLS等价 |\n| $R^2$     | 模型解释的变异比例  | 拟合优度初步评价      |\n| 调整$R^2$ | 对复杂度惩罚的$R^2$ | 模型比较              |\n| F检验     | 模型整体显著性检验  | 判断模型是否有意义    |\n| t检验     | 单个系数显著性检验  | 变量选择依据          |\n| 预测区间  | 个别值的预测范围    | 实际预测应用          |\n\n线性回归为后续所有预测建模方法提供了理论基础和分析框架。理解本章内容对于掌握现代预测建模技术至关重要。",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}