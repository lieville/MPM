% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{report}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={应用回归分析--基于R和Python},
  pdfauthor={李世纪},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{应用回归分析--基于R和Python}
\author{李世纪}
\date{2025-11-20}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

这是一部介绍回归分析的著作，并由R、Python来实现输出。

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

This is a book created from markdown and executable code.

See Knuth (1984) for additional discussion of literate programming.
首先要安装包

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("tidyverse")}
\CommentTok{\#install.packages("palmerpenguins")}
\CommentTok{\#install.packages("tinytex")}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\chapter{Summary}\label{summary}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-knuth84}
Knuth, Donald E. 1984. {``Literate Programming.''} \emph{Comput. J.} 27
(2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\end{CSLReferences}

\part{经典回归分析}

\chapter{第一章
回归分析与软件}\label{ux7b2cux4e00ux7ae0-ux56deux5f52ux5206ux6790ux4e0eux8f6fux4ef6}

\section{1.1
下载和安装R、Rstudio、Rtools}\label{ux4e0bux8f7dux548cux5b89ux88c5rrstudiortools}

R下载地\url{https://www.r-project.org/}\\
Rstudio下载地\url{https://posit.co/downloads/}\\
Rtools：\url{https://cran.r-project.org/bin/windows/Rtools/}\\
先安装R，再安装Rtools和Rstudio，Rtools可以暂时不安装.

\section{1.2 设置}\label{ux8bbeux7f6e}

Rstudio Rstudio的菜单栏：tools--global options 可以设置工作环境。\\
在general里面可以设置工作目录，我一般会在D盘下设置一个英文目录，比如
D:/OneDrive/R，这里我开通了OneDrive，把R工作目录放在OneDrive里面，可以在各个平台比如台式机、平板、手机、笔记本等使用OneDrive更新。\\
在appearence里面可以调整字体大小和背景，我一般喜欢调大一点字体，并且使用黑色背景，这样看起来舒服一点。\\
在pane
layout里面可以设置Rstudio的工作模块，一般我分为四块，如果是台式机，或者显示器较大，可以增加模块，比如我在使用台式机的时候，点击add
column，增加了一个source模块，这样可以两个Rmd文件对照着看。也可以在这里更换各个模块的位置。\\
在packages里面点change，可以更换安装包的镜像来源，比如大家可以选中科大ustc的CRAN
mirrors，这样下载包和更新包都会快很多，但是有时候会出现问题，安装各种包失败的时候可以改为默认地址试试。

\section{1.3 RMD格式}\label{rmdux683cux5f0f}

点击''FIle''-``new file''，常用的一个是R script 格式为.r，一个是R
markdown，格式为.rmd,建议使用后者，还有最新的格式.qmd，近似于R
markdown的升级版，第一次使用R markdown需要安装支持包。\\
R
markdown文件就像写笔记，既有代码，又有说明文字、标题格式等，还可以即时更新，保存为网页文件html、PDF和word文件以及PPT，可以分块运行，可读性很强，如果想记学习笔记，写实验报告，教学培训，Rmd文件格式是比较好的选择。

\section{1.4 包安装}\label{ux5305ux5b89ux88c5}

需要安装包的话，直接在Rstudio界面右下部分packages选项里点击install，填写包的名字安装，注意包的名字不要错，特别是大小写。packages选项里还可以查看各种包及包里面函数格式的说明，或者直接点击help选项查找相关说明。\\
也可以直接使用命令 install.packages(``XX'')~~remove.packages(``XX'')
来安装和卸载包，但是推荐上一种操作，因为代码运行一遍就会重装一遍包

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("readxl") \#包readxl可以读取xls、xlsx文件}
\CommentTok{\#install.packages("heaven") \#包heaven可以读取.sav文件}
\end{Highlighting}
\end{Shaded}

R语言自带的mtcars数据集

\section{1.5 寻求帮助}\label{ux5bfbux6c42ux5e2eux52a9}

在help里面输入函数或者包名称，查找帮助手册
在函数前面加？运行，获得帮助手册

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#?lm }
\CommentTok{\#help(lm)\#查看函数的使用方法}
\end{Highlighting}
\end{Shaded}

\chapter{第二章
一元回归}\label{ux7b2cux4e8cux7ae0-ux4e00ux5143ux56deux5f52}

\chapter{2 一元回归}\label{ux4e00ux5143ux56deux5f52}

\section{2.1读取数据、数据可视化}\label{ux8bfbux53d6ux6570ux636eux6570ux636eux53efux89c6ux5316}

cars为R自带数据集，是1920年汽车的速度和刹车距离的数据集 画图

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\OtherTok{=}\NormalTok{cars}
\CommentTok{\#View(car) \#查看数据，数据单独窗口显示}
\FunctionTok{str}\NormalTok{(car) }\CommentTok{\#查看数据}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   50 obs. of  2 variables:
 $ speed: num  4 4 7 7 8 9 10 10 10 11 ...
 $ dist : num  2 10 4 22 16 10 18 26 34 17 ...
\end{verbatim}

描述统计、数据可视化

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(car}\SpecialCharTok{$}\NormalTok{speed,car}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{main=}\StringTok{"刹车距离与车速"}\NormalTok{,}\AttributeTok{xlab =}\StringTok{"车速"}\NormalTok{,}\AttributeTok{ylab =} \StringTok{"刹车距离"}\NormalTok{) }\CommentTok{\# 画x、y的散点图}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第二章-一元线性回归_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#hist(car$dist,probability=T,main="直方图",xlab ="刹车距离",ylab = "密度") \#参数probability=T表示使用频率作图,否则使用频数作图}
\CommentTok{\#stem(car$dist) \#对变量y作茎叶图}

\CommentTok{\#箱线图。参数horizontal=F默认为垂直型,而horizontal=T则为水平型。}
\CommentTok{\#boxplot(cars$dist,horizontal=T,main="刹车距离箱线图") }
\end{Highlighting}
\end{Shaded}

\section{2.2建立回归模型}\label{ux5efaux7acbux56deux5f52ux6a21ux578b}

符号\textasciitilde 的前是因变量，后是自变量，如果只选择一部分变量，则把变量列举出来，用+号连接，例如X1+X2+X4,非线性的例如X1\textsuperscript{2+X2}4+X3*X4

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model2}\OtherTok{=}\FunctionTok{lm}\NormalTok{(dist}\SpecialCharTok{\textasciitilde{}}\NormalTok{speed,}\AttributeTok{data =}\NormalTok{ car) }
\FunctionTok{names}\NormalTok{(model2) }\CommentTok{\# 建模的结果是一个列表，使用该命令查看列表项}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        
\end{verbatim}

\section{2.3
参数估计，t、F检验，拟合优度R方}\label{ux53c2ux6570ux4f30ux8ba1tfux68c0ux9a8cux62dfux5408ux4f18ux5ea6rux65b9}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = dist ~ speed, data = car)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
\end{verbatim}

\section{2.4 参数区间估计}\label{ux53c2ux6570ux533aux95f4ux4f30ux8ba1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(model2,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}\CommentTok{\#回归系数的置信区间，默认置信度0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                47.5 %     52.5 %
(Intercept) -18.005117 -17.153073
speed         3.906217   3.958601
\end{verbatim}

\section{2.5 y预测}\label{yux9884ux6d4b}

点估计

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model2)}\SpecialCharTok{$}\NormalTok{fitted.values }\CommentTok{\#y的拟合值，yhat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre2}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model2);pre2 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        1         2         3         4         5         6         7         8 
-1.849460 -1.849460  9.947766  9.947766 13.880175 17.812584 21.744993 21.744993 
        9        10        11        12        13        14        15        16 
21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 
       17        18        19        20        21        22        23        24 
33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 
       25        26        27        28        29        30        31        32 
41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 
       33        34        35        36        37        38        39        40 
53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 
       41        42        43        44        45        46        47        48 
61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 
       49        50 
76.798715 80.731124 
\end{verbatim}

区间估计

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre2.con}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model2,}\AttributeTok{interval =} \StringTok{\textquotesingle{}confidence\textquotesingle{}}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }\CommentTok{\#y的置信区间估计,y均值估计}
\CommentTok{\#pre2.con}
\NormalTok{pre2.new}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model2,}\AttributeTok{interval =} \StringTok{\textquotesingle{}prediction\textquotesingle{}}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }\CommentTok{\#y的预测区间估计,y新值估计}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in predict.lm(model2, interval = "prediction", level = 0.05): predictions on current data refer to _future_ responses
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#pre2.new}
\end{Highlighting}
\end{Shaded}

\chapter{Hello, Quarto}\label{hello-quarto}

\chapter{3 多元回归}\label{ux591aux5143ux56deux5f52}

\section{3.1
读取数据、数据可视化}\label{ux8bfbux53d6ux6570ux636eux6570ux636eux53efux89c6ux5316-1}

R自带数据集

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data3}\OtherTok{=}\NormalTok{ChickWeight}
\CommentTok{\#View(data3) \#查看数据，数据单独窗口显示 }
\FunctionTok{str}\NormalTok{(data3) }\CommentTok{\#查看数据}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:
 $ weight: num  42 51 59 64 76 93 106 125 149 171 ...
 $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...
 $ Chick : Ord.factor w/ 50 levels "18"<"16"<"15"<..: 15 15 15 15 15 15 15 15 15 15 ...
 $ Diet  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "formula")=Class 'formula'  language weight ~ Time | Chick
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "outer")=Class 'formula'  language ~Diet
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "labels")=List of 2
  ..$ x: chr "Time"
  ..$ y: chr "Body weight"
 - attr(*, "units")=List of 2
  ..$ x: chr "(days)"
  ..$ y: chr "(gm)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(data3[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]) }\CommentTok{\#对自变量绘制散点图矩阵}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第三章-多元线性回归_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\section{3.2
建立多元回归模型}\label{ux5efaux7acbux591aux5143ux56deux5f52ux6a21ux578b}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3}\OtherTok{=}\FunctionTok{lm}\NormalTok{(weight}\SpecialCharTok{\textasciitilde{}}\NormalTok{Time}\SpecialCharTok{+}\NormalTok{Diet,}\AttributeTok{data=}\NormalTok{data3);model3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = weight ~ Time + Diet, data = data3)

Coefficients:
(Intercept)         Time        Diet2        Diet3        Diet4  
      10.92         8.75        16.17        36.50        30.23  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(model3) }\CommentTok{\# 查看建模结果列表里面的项}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "contrasts"     "xlevels"       "call"          "terms"        
[13] "model"        
\end{verbatim}

三个命令计算残差值

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#summary(model3)$residuals \#残差值e}
\CommentTok{\#model3$residuals \#残差值e}
\CommentTok{\#resid(model3) \#残差值e}
\NormalTok{sse}\OtherTok{=}\FunctionTok{sum}\NormalTok{(}\FunctionTok{resid}\NormalTok{(model3)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{);sse  }\CommentTok{\#残差平方和SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 742336.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigmahat}\OtherTok{=}\NormalTok{sse}\SpecialCharTok{/}\NormalTok{model3}\SpecialCharTok{$}\NormalTok{df.residual;sigmahat }\CommentTok{\#标准误，σ估计值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1295.526
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plot(model3) \#残差图}
\end{Highlighting}
\end{Shaded}

\section{3.3
参数估计，t、F检验，拟合优度R方}\label{ux53c2ux6570ux4f30ux8ba1tfux68c0ux9a8cux62dfux5408ux4f18ux5ea6rux65b9-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model3) }\CommentTok{\#输出建模结果，主要看参数估计、t检验和F检验和R}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = weight ~ Time + Diet, data = data3)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  < 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  < 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#summary(model3)$coefficients}
\CommentTok{\#model3$coefficients}
\end{Highlighting}
\end{Shaded}

\section{3.4 参数区间估计}\label{ux53c2ux6570ux533aux95f4ux4f30ux8ba1-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(model3,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}\CommentTok{\#回归系数的置信区间，默认置信度0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               47.5 %    52.5 %
(Intercept) 10.713563 11.135219
Time         8.736577  8.764407
Diet2       15.909752 16.422396
Diet3       36.243085 36.755730
Diet4       29.975776 30.491136
\end{verbatim}

\section{3.5 y预测}\label{yux9884ux6d4b-1}

点估计

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model3)}\SpecialCharTok{$}\NormalTok{fitted.values }\CommentTok{\#y的拟合值，yhat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre3}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model3) }
\CommentTok{\#pre3}
\end{Highlighting}
\end{Shaded}

区间估计

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre3.con}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model3,}\AttributeTok{interval =} \StringTok{\textquotesingle{}confidence\textquotesingle{}}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }\CommentTok{\#y的置信区间估计,y均值估计}
\CommentTok{\#pre3.con}
\NormalTok{pre3.new}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model3,}\AttributeTok{interval =} \StringTok{\textquotesingle{}prediction\textquotesingle{}}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }\CommentTok{\#y的预测区间估计,y新值估计}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in predict.lm(model3, interval = "prediction", level = 0.05): predictions on current data refer to _future_ responses
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#pre3.new}
\end{Highlighting}
\end{Shaded}

\chapter{Hello, Quarto}\label{hello-quarto-1}

\chapter{4 残差分析
正态性、异常值点}\label{ux6b8bux5deeux5206ux6790-ux6b63ux6001ux6027ux5f02ux5e38ux503cux70b9}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data3}\OtherTok{=}\NormalTok{ChickWeight}
\CommentTok{\#View(data3) \#查看数据，数据单独窗口显示 }
\FunctionTok{str}\NormalTok{(data3) }\CommentTok{\#查看数据}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:
 $ weight: num  42 51 59 64 76 93 106 125 149 171 ...
 $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...
 $ Chick : Ord.factor w/ 50 levels "18"<"16"<"15"<..: 15 15 15 15 15 15 15 15 15 15 ...
 $ Diet  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "formula")=Class 'formula'  language weight ~ Time | Chick
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "outer")=Class 'formula'  language ~Diet
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "labels")=List of 2
  ..$ x: chr "Time"
  ..$ y: chr "Body weight"
 - attr(*, "units")=List of 2
  ..$ x: chr "(days)"
  ..$ y: chr "(gm)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model3}\OtherTok{=}\FunctionTok{lm}\NormalTok{(weight}\SpecialCharTok{\textasciitilde{}}\NormalTok{Time}\SpecialCharTok{+}\NormalTok{Diet,}\AttributeTok{data=}\NormalTok{data3);model3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = weight ~ Time + Diet, data = data3)

Coefficients:
(Intercept)         Time        Diet2        Diet3        Diet4  
      10.92         8.75        16.17        36.50        30.23  
\end{verbatim}

\section{4.1 残差分析}\label{ux6b8bux5deeux5206ux6790}

正态性（相关性）：Residuals vs Fitted
残差与拟合图：残差值与预测值没有任何关系\\
正态性：Normal Q-Q图：点应该落在直线上\\
异方差：Scale-Location 位置尺度图：水平线周围的点应该随机分布\\
异常值点=离群点、高杠杆点、强影响点：Residuals vs Levrerage 残差与杠杆图

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#建立y的拟合值和模型的残差值的散点图}
\CommentTok{\#plot(fitted(model3),resid(model3))  }
\CommentTok{\#plot(model3$fitted.values,model3$residuals)}
\CommentTok{\#上面两行命令应该是等价的}
\FunctionTok{plot}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/unnamed-chunk-2-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/unnamed-chunk-2-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/unnamed-chunk-2-4.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#综合检测 }
\FunctionTok{library}\NormalTok{(gvlma)}
\NormalTok{gvlma.test}\OtherTok{=}\FunctionTok{gvlma}\NormalTok{(model3)}
\FunctionTok{summary}\NormalTok{(gvlma.test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = weight ~ Time + Diet, data = data3)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  < 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  < 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: < 2.2e-16


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = model3) 

                     Value   p-value                   Decision
Global Stat        234.830 0.000e+00 Assumptions NOT satisfied!
Skewness            21.679 3.223e-06 Assumptions NOT satisfied!
Kurtosis           155.887 0.000e+00 Assumptions NOT satisfied!
Link Function       55.335 1.016e-13 Assumptions NOT satisfied!
Heteroscedasticity   1.929 1.649e-01    Assumptions acceptable.
\end{verbatim}

\section{4.2
包CAR的线性、正态性以及异常值点检验}\label{ux5305carux7684ux7ebfux6027ux6b63ux6001ux6027ux4ee5ux53caux5f02ux5e38ux503cux70b9ux68c0ux9a8c}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#线性检验}
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{crPlots}\NormalTok{(model3) }\CommentTok{\#可以通过红线和绿线趋势是否一致来判断线性关系  }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/正态性、相关性、异方差、共线性检验-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 正态性检验 }
\FunctionTok{qqPlot}\NormalTok{(model3,}\AttributeTok{main=}\StringTok{\textquotesingle{}QQ图\textquotesingle{}}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/正态性、相关性、异方差、共线性检验-2.pdf}}

\begin{verbatim}
[1] 399 400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#异方差检验}
\FunctionTok{ncvTest}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 225.9712, Df = 1, p = < 2.22e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#自相关性检验}
\FunctionTok{durbinWatsonTest}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 lag Autocorrelation D-W Statistic p-value
   1       0.7886502     0.4193412       0
 Alternative hypothesis: rho != 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#多重共线性}
\FunctionTok{vif}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         GVIF Df GVIF^(1/(2*Df))
Time 1.000832  1        1.000416
Diet 1.000832  3        1.000139
\end{verbatim}

\section{4.3
异常点:离群点、强影响点、高杠杆点}\label{ux5f02ux5e38ux70b9ux79bbux7fa4ux70b9ux5f3aux5f71ux54cdux70b9ux9ad8ux6760ux6746ux70b9}

离群点具有异常的y值\\
强影响点具有异常的x值\\
高杠杆点具有异常的x和y值

包car中的\\
hatvalues（）和hat（）------帽子矩阵\\
dffits（）------DFFITS准则\\
cooks.distance()------Cook统计量，值越大越有可能是异常值点\\
covratio（）------COVRATIO准则

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\CommentTok{\#离群点：预测效果不佳的点，具有大残差}
\FunctionTok{outlierTest}\NormalTok{(model3)}\CommentTok{\#若不显著 Bonferonni P\textgreater{}0.05 ，表明没有离群点}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    rstudent unadjusted p-value Bonferroni p
400 4.016296         6.7008e-05     0.038731
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#高杠杆值点:距离样本总体较远的点，对回归参数影响加大。}
\CommentTok{\#杠杆值大于均值的2\textasciitilde{}3倍的样本点即可认为是高杠杆点。}
\FunctionTok{hatvalues}\NormalTok{(model3) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          1           2           3           4           5           6 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
          7           8           9          10          11          12 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         13          14          15          16          17          18 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         19          20          21          22          23          24 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         25          26          27          28          29          30 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         31          32          33          34          35          36 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         37          38          39          40          41          42 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         43          44          45          46          47          48 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         49          50          51          52          53          54 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         55          56          57          58          59          60 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         61          62          63          64          65          66 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         67          68          69          70          71          72 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         73          74          75          76          77          78 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         79          80          81          82          83          84 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 
         85          86          87          88          89          90 
0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 
         91          92          93          94          95          96 
0.004632982 0.005015494 0.005701805 0.006691916 0.007985827 0.008717708 
         97          98          99         100         101         102 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        103         104         105         106         107         108 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        109         110         111         112         113         114 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        115         116         117         118         119         120 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        121         122         123         124         125         126 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        127         128         129         130         131         132 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        133         134         135         136         137         138 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        139         140         141         142         143         144 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        145         146         147         148         149         150 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        151         152         153         154         155         156 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        157         158         159         160         161         162 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        163         164         165         166         167         168 
0.005015494 0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 
        169         170         171         172         173         174 
0.007277421 0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 
        175         176         177         178         179         180 
0.005015494 0.008717708 0.007277421 0.006140934 0.005308246 0.004779358 
        181         182         183         184         185         186 
0.004554270 0.004632982 0.008717708 0.007277421 0.006140934 0.005308246 
        187         188         189         190         191         192 
0.004779358 0.004554270 0.004632982 0.005015494 0.005701805 0.006691916 
        193         194         195         196         197         198 
0.007985827 0.008746707 0.008717708 0.007277421 0.008717708 0.007277421 
        199         200         201         202         203         204 
0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 0.005015494 
        205         206         207         208         209         210 
0.005701805 0.006691916 0.007985827 0.008746707 0.008717708 0.007277421 
        211         212         213         214         215         216 
0.006140934 0.005308246 0.004779358 0.004554270 0.004632982 0.005015494 
        217         218         219         220         221         222 
0.005701805 0.006691916 0.007985827 0.008746707 0.012858948 0.011352607 
        223         224         225         226         227         228 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        229         230         231         232         233         234 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        235         236         237         238         239         240 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        241         242         243         244         245         246 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        247         248         249         250         251         252 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        253         254         255         256         257         258 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        259         260         261         262         263         264 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        265         266         267         268         269         270 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        271         272         273         274         275         276 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        277         278         279         280         281         282 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        283         284         285         286         287         288 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        289         290         291         292         293         294 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        295         296         297         298         299         300 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        301         302         303         304         305         306 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        307         308         309         310         311         312 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        313         314         315         316         317         318 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        319         320         321         322         323         324 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        325         326         327         328         329         330 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        331         332         333         334         335         336 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        337         338         339         340         341         342 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        343         344         345         346         347         348 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        349         350         351         352         353         354 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        355         356         357         358         359         360 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        361         362         363         364         365         366 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        367         368         369         370         371         372 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        373         374         375         376         377         378 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        379         380         381         382         383         384 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        385         386         387         388         389         390 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        391         392         393         394         395         396 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        397         398         399         400         401         402 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        403         404         405         406         407         408 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        409         410         411         412         413         414 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        415         416         417         418         419         420 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        421         422         423         424         425         426 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        427         428         429         430         431         432 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        433         434         435         436         437         438 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        439         440         441         442         443         444 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        445         446         447         448         449         450 
0.009314617 0.010238675 0.011466532 0.012194386 0.012858948 0.011352607 
        451         452         453         454         455         456 
0.010150067 0.009251326 0.008656384 0.008365243 0.008377901 0.008694359 
        457         458         459         460         461         462 
0.009314617 0.010238675 0.011466532 0.012194386 0.012866519 0.011384852 
        463         464         465         466         467         468 
0.010206984 0.009332916 0.008762648 0.008496179 0.008533511 0.008874642 
        469         470         471         472         473         474 
0.009519573 0.010468303 0.011720834 0.012461024 0.012866519 0.011384852 
        475         476         477         478         479         480 
0.010206984 0.009332916 0.008762648 0.008496179 0.008533511 0.008874642 
        481         482         483         484         485         486 
0.009519573 0.010468303 0.011720834 0.012461024 0.012866519 0.011384852 
        487         488         489         490         491         492 
0.010206984 0.009332916 0.008762648 0.008496179 0.008533511 0.008874642 
        493         494         495         496         497         498 
0.009519573 0.010468303 0.011720834 0.012461024 0.012866519 0.011384852 
        499         500         501         502         503         504 
0.010206984 0.009332916 0.008762648 0.008496179 0.008533511 0.008874642 
        505         506         507         508         509         510 
0.009519573 0.010468303 0.012866519 0.011384852 0.010206984 0.009332916 
        511         512         513         514         515         516 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        517         518         519         520         521         522 
0.011720834 0.012461024 0.012866519 0.011384852 0.010206984 0.009332916 
        523         524         525         526         527         528 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        529         530         531         532         533         534 
0.011720834 0.012461024 0.012866519 0.011384852 0.010206984 0.009332916 
        535         536         537         538         539         540 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        541         542         543         544         545         546 
0.011720834 0.012461024 0.012866519 0.011384852 0.010206984 0.009332916 
        547         548         549         550         551         552 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        553         554         555         556         557         558 
0.011720834 0.012461024 0.012866519 0.011384852 0.010206984 0.009332916 
        559         560         561         562         563         564 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        565         566         567         568         569         570 
0.011720834 0.012461024 0.012866519 0.011384852 0.010206984 0.009332916 
        571         572         573         574         575         576 
0.008762648 0.008496179 0.008533511 0.008874642 0.009519573 0.010468303 
        577         578 
0.011720834 0.012461024 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#强影响点：对模型的参数估计有较大的影响的点（综合考虑了残差和杠杆值），若将其删除则会导致模型发生本质的改变}
\FunctionTok{cooks.distance}\NormalTok{(model3) }\CommentTok{\#通过Cook距离来判断}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           1            2            3            4            5            6 
1.322607e-03 5.809608e-04 1.640446e-04 2.716123e-07 1.809312e-05 2.091494e-05 
           7            8            9           10           11           12 
7.118725e-05 5.559704e-05 3.324251e-06 6.898208e-06 2.138644e-04 1.462250e-04 
          13           14           15           16           17           18 
1.157842e-03 4.825803e-04 1.399089e-04 6.086801e-05 7.028544e-06 1.482284e-05 
          19           20           21           22           23           24 
2.659584e-05 1.632507e-05 1.090639e-04 3.609431e-04 6.665063e-04 5.671593e-04 
          25           26           27           28           29           30 
1.409099e-03 1.274781e-04 7.901910e-05 1.057161e-05 7.028544e-06 2.310843e-07 
          31           32           33           34           35           36 
6.247651e-07 1.632507e-05 1.296627e-04 3.609431e-04 1.823806e-04 7.353964e-05 
          37           38           39           40           41           42 
1.322607e-03 4.825803e-04 9.739616e-05 1.057161e-05 3.575785e-05 9.268460e-05 
          43           44           45           46           47           48 
1.400872e-04 5.058262e-04 1.985245e-04 2.181194e-04 8.425870e-04 1.951596e-03 
          49           50           51           52           53           54 
1.238855e-03 2.100687e-04 4.127018e-06 9.729103e-06 2.769972e-06 4.066668e-05 
          55           56           57           58           59           60 
4.537083e-04 7.308356e-04 1.889544e-03 9.782848e-04 1.453800e-03 1.101796e-03 
          61           62           63           64           65           66 
1.238855e-03 4.825803e-04 1.640446e-04 9.258198e-05 1.924142e-04 4.639302e-04 
          67           68           69           70           71           72 
4.537083e-04 1.660000e-04 1.473227e-05 7.446561e-05 8.425870e-04 1.951596e-03 
          73           74           75           76           77           78 
1.238855e-03 4.825803e-04 1.176928e-04 4.749575e-05 4.853357e-05 1.306687e-04 
          79           80           81           82           83           84 
6.527345e-04 1.287205e-03 4.004889e-03 6.966152e-03 1.305054e-02 1.672364e-02 
          85           86           87           88           89           90 
1.322607e-03 5.306306e-04 2.180747e-04 4.749575e-05 7.028544e-06 2.091494e-05 
          91           92           93           94           95           96 
2.538811e-05 2.376421e-04 5.534601e-04 1.241431e-03 4.651476e-03 1.322607e-03 
          97           98           99          100          101          102 
5.809608e-04 1.640446e-04 1.731791e-05 1.234983e-05 4.187290e-06 4.853924e-04 
         103          104          105          106          107          108 
1.342523e-03 2.988161e-03 4.903439e-03 9.251252e-03 1.284621e-02 1.238855e-03 
         109          110          111          112          113          114 
2.765292e-04 3.540521e-05 1.512545e-07 3.575785e-05 2.155400e-04 5.235526e-04 
         115          116          117          118          119          120 
1.095808e-03 2.219860e-03 3.334545e-03 5.446155e-03 6.866104e-03 1.409099e-03 
         121          122          123          124          125          126 
5.809608e-04 2.797829e-04 3.505410e-04 7.191909e-04 1.167865e-03 1.957260e-03 
         127          128          129          130          131          132 
1.484618e-03 8.593744e-04 2.537248e-04 3.050045e-05 5.324977e-04 1.238855e-03 
         133          134          135          136          137          138 
4.825803e-04 9.739616e-05 1.687384e-06 5.938201e-05 7.717536e-05 6.802559e-06 
         139          140          141          142          143          144 
1.924680e-06 1.090639e-04 2.873701e-04 1.029622e-04 1.462250e-04 1.238855e-03 
         145          146          147          148          149          150 
4.368101e-04 4.802363e-05 9.729103e-06 1.889971e-04 7.008697e-04 1.457326e-03 
         151          152          153          154          155          156 
3.146820e-03 5.688624e-03 8.004233e-03 1.129052e-02 1.338317e-02 1.238855e-03 
         157          158          159          160          161          162 
4.825803e-04 2.479690e-04 2.008554e-04 3.001111e-04 6.204269e-04 1.668096e-03 
         163          164          165          166          167          168 
2.682851e-03 5.151861e-03 6.628723e-03 6.687998e-03 6.989153e-03 1.238855e-03 
         169          170          171          172          173          174 
4.825803e-04 9.739616e-05 2.716123e-07 1.245086e-04 6.569795e-04 1.728358e-03 
         175          176          177          178          179          180 
3.348389e-03 1.238855e-03 3.131794e-04 9.067245e-06 1.279130e-04 4.265201e-04 
         181          182          183          184          185          186 
1.596104e-03 2.768754e-03 1.322607e-03 5.809608e-04 2.180747e-04 6.086801e-05 
         187          188          189          190          191          192 
3.197114e-06 6.308518e-05 2.320881e-04 7.242782e-04 1.281093e-03 2.161295e-03 
         193          194          195          196          197          198 
3.510276e-03 3.814421e-03 1.079568e-03 4.927740e-05 1.409099e-03 4.368101e-04 
         199          200          201          202          203          204 
7.901910e-05 1.687384e-06 1.889971e-04 5.338231e-04 8.311001e-04 1.614265e-03 
         205          206          207          208          209          210 
1.797544e-03 2.456143e-03 2.202954e-03 1.951596e-03 1.238855e-03 3.933198e-04 
         211          212          213          214          215          216 
6.256159e-05 2.439677e-05 1.889971e-04 4.588141e-04 1.094091e-03 1.543983e-03 
         217          218          219          220          221          222 
2.494617e-03 3.951604e-03 6.303472e-03 8.293360e-03 3.395097e-04 5.245160e-05 
         223          224          225          226          227          228 
1.366344e-08 5.971922e-05 1.058900e-03 3.077037e-03 9.481552e-03 1.116269e-02 
         229          230          231          232          233          234 
1.705822e-02 2.417322e-02 2.433313e-02 2.785113e-02 3.941452e-04 1.942573e-04 
         235          236          237          238          239          240 
5.819346e-06 9.786000e-06 6.843895e-05 5.042740e-04 7.637126e-04 2.034799e-03 
         241          242          243          244          245          246 
1.909202e-03 2.161289e-03 2.629603e-03 3.709846e-03 5.156393e-04 9.841547e-05 
         247          248          249          250          251          252 
1.908548e-06 6.325311e-05 6.843895e-05 1.765750e-04 3.416238e-05 2.910412e-04 
         253          254          255          256          257          258 
7.154784e-04 7.527432e-04 1.866601e-03 2.479696e-03 4.528551e-04 9.841547e-05 
         259          260          261          262          263          264 
2.678411e-05 4.552135e-05 1.314730e-03 2.851313e-03 5.071766e-03 8.437665e-03 
         265          266          267          268          269          270 
1.325023e-02 2.045683e-02 2.880485e-02 3.613230e-02 3.395097e-04 3.484886e-05 
         271          272          273          274          275          276 
1.366344e-08 3.694189e-06 3.272326e-05 1.161559e-04 2.542632e-04 2.833292e-04 
         277          278          279          280          281          282 
1.309991e-03 3.473886e-03 5.864802e-03 5.656987e-03 4.528551e-04 2.083226e-05 
         283          284          285          286          287          288 
4.147293e-05 4.552135e-05 2.279568e-05 4.655339e-07 2.004311e-05 9.214404e-06 
         289          290          291          292          293          294 
5.298421e-06 6.715158e-04 2.081733e-03 3.109961e-03 2.889486e-04 3.557478e-06 
         295          296          297          298          299          300 
2.678411e-05 6.325311e-05 1.385585e-04 2.797629e-04 3.844452e-04 9.662359e-04 
         301          302          303          304          305          306 
7.816976e-04 7.527432e-04 5.297132e-04 6.855832e-04 2.889486e-04 3.557478e-06 
         307          308          309          310          311          312 
2.678411e-05 6.325311e-05 3.529052e-05 4.655339e-07 2.190034e-04 5.599201e-05 
         313          314          315          316          317          318 
4.185396e-04 8.096351e-04 1.775323e-04 9.464930e-04 2.889486e-04 2.083226e-05 
         319          320          321          322          323          324 
1.529377e-05 4.552135e-05 1.385585e-04 9.702631e-05 4.766436e-06 2.214435e-07 
         325          326          327          328          329          330 
5.803052e-04 3.325765e-03 1.071230e-02 1.858551e-02 4.528551e-04 2.083226e-05 
         331          332          333          334          335          336 
1.529377e-05 8.389486e-05 1.989027e-04 3.616876e-04 3.844452e-04 1.040258e-03 
         337          338          339          340          341          342 
8.508471e-04 1.821494e-03 3.684616e-03 7.143868e-03 5.992915e-05 2.549750e-04 
         343          344          345          346          347          348 
6.672228e-04 1.054943e-03 1.429899e-03 1.423996e-03 1.139196e-03 1.392589e-03 
         349          350          351          352          353          354 
4.452002e-04 1.403473e-06 2.860572e-04 1.188120e-03 8.406493e-05 4.547197e-04 
         355          356          357          358          359          360 
4.856215e-04 4.675888e-04 1.478604e-04 4.616155e-05 5.678026e-05 1.123455e-04 
         361          362          363          364          365          366 
1.650962e-03 5.440400e-03 8.516392e-03 1.051237e-02 1.445596e-04 3.994043e-04 
         367          368          369          370          371          372 
6.034906e-04 7.647966e-04 6.243449e-04 7.519654e-04 3.131423e-04 9.184053e-04 
         373          374          375          376          377          378 
1.944624e-03 5.603750e-03 7.994833e-03 1.367292e-02 8.406493e-05 4.547197e-04 
         379          380          381          382          383          384 
6.034906e-04 3.241839e-04 1.478604e-04 1.132726e-06 1.760825e-04 3.526966e-04 
         385          386          387          388          389          390 
3.315228e-03 1.279982e-02 1.980694e-02 2.326651e-02 8.406493e-05 2.549750e-04 
         391          392          393          394          395          396 
5.429568e-04 2.431306e-04 4.222173e-05 6.990410e-04 3.102905e-03 6.328623e-03 
         397          398          399          400          401          402 
1.452513e-02 2.605162e-02 3.478156e-02 3.880169e-02 1.445596e-04 5.136212e-04 
         403          404          405          406          407          408 
7.341535e-04 8.329681e-04 5.132351e-04 4.705453e-04 7.260547e-05 2.110292e-05 
         409          410          411          412          413          414 
1.636404e-04 7.857189e-04 1.193083e-05 2.413265e-04 8.406493e-05 5.136212e-04 
         415          416          417          418          419          420 
1.116784e-03 1.483100e-03 1.904843e-03 3.541394e-03 3.213681e-03 4.583780e-03 
         421          422          423          424          425          426 
4.027772e-03 3.707061e-03 5.172051e-03 5.457139e-03 8.406493e-05 4.547197e-04 
         427          428          429          430          431          432 
7.341535e-04 9.780411e-04 5.132351e-04 8.829195e-04 7.849882e-04 3.466373e-04 
         433          434          435          436          437          438 
3.057688e-05 1.182111e-03 6.003037e-03 6.674060e-03 5.992915e-05 3.994043e-04 
         439          440          441          442          443          444 
7.341535e-04 6.995350e-04 1.098896e-03 8.829195e-04 6.617193e-04 7.821979e-04 
         445          446          447          448          449          450 
4.452002e-04 1.326562e-04 1.376551e-03 3.214099e-03 8.406493e-05 1.766195e-04 
         451          452          453          454          455          456 
4.314847e-04 6.371835e-04 3.669673e-04 2.926874e-04 3.243340e-06 1.989628e-04 
         457          458          459          460          461          462 
1.113521e-03 5.254631e-03 9.539028e-03 1.556350e-02 1.445690e-06 1.054828e-04 
         463          464          465          466          467          468 
1.660215e-04 1.101189e-04 9.171365e-05 2.900721e-05 1.046386e-04 1.586272e-04 
         469          470          471          472          473          474 
5.694775e-05 3.550305e-04 5.460175e-04 8.631312e-04 1.445690e-06 1.677668e-04 
         475          476          477          478          479          480 
2.785430e-04 1.370162e-04 9.171365e-05 9.459855e-06 2.565622e-04 1.489777e-04 
         481          482          483          484          485          486 
7.810602e-04 2.060484e-03 5.171096e-03 6.204025e-03 1.445690e-06 2.407363e-05 
         487          488          489          490          491          492 
8.245105e-05 8.033068e-06 5.418379e-04 1.071356e-03 1.918545e-03 8.259428e-04 
         493          494          495          496          497          498 
3.755836e-04 7.336012e-07 5.460175e-04 1.224790e-03 1.445690e-06 1.054828e-04 
         499          500          501          502          503          504 
2.003118e-04 8.615766e-05 9.171365e-05 1.516906e-04 4.921716e-04 9.186522e-04 
         505          506          507          508          509          510 
1.959313e-03 4.577972e-03 5.078875e-08 1.348265e-04 3.696413e-04 3.600590e-04 
         511          512          513          514          515          516 
2.385028e-04 1.814771e-04 1.670228e-04 7.164384e-04 1.748601e-03 1.004206e-03 
         517          518          519          520          521          522 
6.806478e-04 1.649571e-03 2.732725e-06 7.973570e-05 3.224837e-04 1.996193e-04 
         523          524          525          526          527          528 
1.421686e-04 1.001226e-04 6.274355e-06 8.193540e-05 9.988451e-05 2.119892e-04 
         529          530          531          532          533          534 
4.075691e-04 3.375717e-04 5.078875e-08 5.758512e-05 1.660215e-04 3.155449e-04 
         535          536          537          538          539          540 
1.715264e-04 4.278346e-05 4.518769e-06 6.195024e-05 2.596561e-04 3.082678e-04 
         541          542          543          544          545          546 
7.047372e-05 7.825792e-04 9.491498e-06 1.348265e-04 3.224837e-04 2.739669e-04 
         547          548          549          550          551          552 
7.061649e-05 1.789934e-05 8.229481e-05 5.597675e-05 2.497811e-03 6.412703e-03 
         553          554          555          556          557          558 
1.396838e-02 1.859110e-02 2.732725e-06 5.758512e-05 2.378190e-04 1.101189e-04 
         559          560          561          562          563          564 
1.376346e-05 5.860540e-07 4.564821e-05 7.605911e-06 1.203363e-05 3.099122e-05 
         565          566          567          568          569          570 
5.248934e-04 2.879350e-04 5.078875e-08 3.903110e-05 1.349479e-04 1.370162e-04 
         571          572          573          574          575          576 
5.227290e-05 5.922811e-05 1.046386e-04 1.792014e-04 8.509694e-04 2.060484e-03 
         577          578 
4.238636e-03 3.012860e-03 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{influence.measures}\NormalTok{(model3) }\CommentTok{\#样本点有强影响则在右侧用 * 标记}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Influence measures of
     lm(formula = weight ~ Time + Diet, data = data3) :

       dfb.1_  dfb.Time  dfb.Dit2  dfb.Dit3  dfb.Dit4     dffit cov.r   cook.d
1    8.13e-02 -5.62e-02 -3.35e-02 -3.35e-02 -3.39e-02  0.081303 1.011 1.32e-03
2    5.36e-02 -3.30e-02 -2.45e-02 -2.45e-02 -2.47e-02  0.053868 1.013 5.81e-04
3    2.79e-02 -1.46e-02 -1.43e-02 -1.43e-02 -1.43e-02  0.028618 1.014 1.64e-04
4    1.08e-03 -4.41e-04 -6.29e-04 -6.29e-04 -6.30e-04  0.001164 1.014 2.72e-07
5   -8.15e-03  2.10e-03  5.45e-03  5.45e-03  5.44e-03 -0.009503 1.013 1.81e-05
6   -7.68e-03  4.50e-04  6.05e-03  6.05e-03  6.02e-03 -0.010217 1.013 2.09e-05
7   -1.17e-02 -2.59e-03  1.12e-02  1.12e-02  1.11e-02 -0.018851 1.013 7.12e-05
8   -7.92e-03 -5.10e-03  9.54e-03  9.54e-03  9.44e-03 -0.016659 1.013 5.56e-05
9   -1.36e-03 -1.83e-03  2.20e-03  2.20e-03  2.18e-03 -0.004073 1.015 3.32e-06
10   1.19e-03  3.32e-03 -2.95e-03 -2.95e-03 -2.91e-03  0.005868 1.016 6.90e-06
11   2.96e-03  2.14e-02 -1.51e-02 -1.51e-02 -1.49e-02  0.032676 1.016 2.14e-04
12   1.11e-03  1.87e-02 -1.20e-02 -1.20e-02 -1.18e-02  0.027018 1.017 1.46e-04
13   7.61e-02 -5.26e-02 -3.14e-02 -3.14e-02 -3.17e-02  0.076064 1.012 1.16e-03
14   4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
15   2.57e-02 -1.35e-02 -1.32e-02 -1.32e-02 -1.32e-02  0.026428 1.014 1.40e-04
16   1.62e-02 -6.61e-03 -9.42e-03 -9.42e-03 -9.43e-03  0.017431 1.014 6.09e-05
17   5.08e-03 -1.31e-03 -3.40e-03 -3.40e-03 -3.39e-03  0.005923 1.014 7.03e-06
18   6.47e-03 -3.78e-04 -5.09e-03 -5.09e-03 -5.07e-03  0.008602 1.013 1.48e-05
19   7.15e-03  1.58e-03 -6.82e-03 -6.82e-03 -6.77e-03  0.011522 1.013 2.66e-05
20   4.29e-03  2.76e-03 -5.17e-03 -5.17e-03 -5.12e-03  0.009027 1.014 1.63e-05
21   7.77e-03  1.05e-02 -1.26e-02 -1.26e-02 -1.25e-02  0.023334 1.014 1.09e-04
22   8.63e-03  2.40e-02 -2.13e-02 -2.13e-02 -2.10e-02  0.042455 1.013 3.61e-04
23   5.23e-03  3.79e-02 -2.67e-02 -2.67e-02 -2.63e-02  0.057699 1.013 6.67e-04
24   2.19e-03  3.69e-02 -2.37e-02 -2.37e-02 -2.32e-02  0.053221 1.015 5.67e-04
25   8.39e-02 -5.81e-02 -3.46e-02 -3.46e-02 -3.49e-02  0.083923 1.011 1.41e-03
26   2.51e-02 -1.55e-02 -1.15e-02 -1.15e-02 -1.16e-02  0.025226 1.015 1.27e-04
27   1.93e-02 -1.01e-02 -9.91e-03 -9.91e-03 -9.95e-03  0.019861 1.014 7.90e-05
28   6.76e-03 -2.75e-03 -3.93e-03 -3.93e-03 -3.93e-03  0.007264 1.014 1.06e-05
29   5.08e-03 -1.31e-03 -3.40e-03 -3.40e-03 -3.39e-03  0.005923 1.014 7.03e-06
30   8.07e-04 -4.73e-05 -6.36e-04 -6.36e-04 -6.33e-04  0.001074 1.013 2.31e-07
31  -1.10e-03 -2.43e-04  1.04e-03  1.04e-03  1.04e-03 -0.001766 1.013 6.25e-07
32   4.29e-03  2.76e-03 -5.17e-03 -5.17e-03 -5.12e-03  0.009027 1.014 1.63e-05
33   8.48e-03  1.15e-02 -1.38e-02 -1.38e-02 -1.36e-02  0.025442 1.014 1.30e-04
34   8.63e-03  2.40e-02 -2.13e-02 -2.13e-02 -2.10e-02  0.042455 1.013 3.61e-04
35   2.74e-03  1.98e-02 -1.40e-02 -1.40e-02 -1.37e-02  0.030174 1.016 1.82e-04
36   7.87e-04  1.33e-02 -8.52e-03 -8.52e-03 -8.36e-03  0.019159 1.017 7.35e-05
37   8.13e-02 -5.62e-02 -3.35e-02 -3.35e-02 -3.39e-02  0.081303 1.011 1.32e-03
38   4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
39   2.15e-02 -1.12e-02 -1.10e-02 -1.10e-02 -1.10e-02  0.022050 1.014 9.74e-05
40   6.76e-03 -2.75e-03 -3.93e-03 -3.93e-03 -3.93e-03  0.007264 1.014 1.06e-05
41  -1.15e-02  2.96e-03  7.67e-03  7.67e-03  7.65e-03 -0.013360 1.013 3.58e-05
42  -1.62e-02  9.46e-04  1.27e-02  1.27e-02  1.27e-02 -0.021510 1.012 9.27e-05
43  -1.64e-02 -3.63e-03  1.56e-02  1.56e-02  1.55e-02 -0.026446 1.012 1.40e-04
44  -2.39e-02 -1.54e-02  2.88e-02  2.88e-02  2.85e-02 -0.050269 1.009 5.06e-04
45  -1.05e-02 -1.42e-02  1.70e-02  1.70e-02  1.68e-02 -0.031483 1.013 1.99e-04
46  -6.71e-03 -1.87e-02  1.66e-02  1.66e-02  1.63e-02 -0.033000 1.014 2.18e-04
47  -5.88e-03 -4.26e-02  3.01e-02  3.01e-02  2.95e-02 -0.064880 1.012 8.43e-04
48  -4.06e-03 -6.85e-02  4.39e-02  4.39e-02  4.31e-02 -0.098792 1.008 1.95e-03
49   7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
50   3.22e-02 -1.98e-02 -1.47e-02 -1.47e-02 -1.48e-02  0.032385 1.015 2.10e-04
51   4.42e-03 -2.31e-03 -2.26e-03 -2.26e-03 -2.27e-03  0.004539 1.015 4.13e-06
52  -6.48e-03  2.64e-03  3.77e-03  3.77e-03  3.77e-03 -0.006969 1.014 9.73e-06
53  -3.19e-03  8.23e-04  2.13e-03  2.13e-03  2.13e-03 -0.003718 1.014 2.77e-06
54   1.07e-02 -6.27e-04 -8.44e-03 -8.44e-03 -8.40e-03  0.014248 1.013 4.07e-05
55   2.95e-02  6.54e-03 -2.82e-02 -2.82e-02 -2.80e-02  0.047608 1.009 4.54e-04
56   2.87e-02  1.85e-02 -3.46e-02 -3.46e-02 -3.43e-02  0.060435 1.007 7.31e-04
57   3.24e-02  4.38e-02 -5.26e-02 -5.26e-02 -5.19e-02  0.097254 1.000 1.89e-03
58   1.42e-02  3.96e-02 -3.52e-02 -3.52e-02 -3.46e-02  0.069922 1.009 9.78e-04
59   7.73e-03  5.60e-02 -3.95e-02 -3.95e-02 -3.88e-02  0.085251 1.009 1.45e-03
60   3.05e-03  5.14e-02 -3.30e-02 -3.30e-02 -3.24e-02  0.074198 1.012 1.10e-03
61   7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
62   4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
63   2.79e-02 -1.46e-02 -1.43e-02 -1.43e-02 -1.43e-02  0.028618 1.014 1.64e-04
64   2.00e-02 -8.15e-03 -1.16e-02 -1.16e-02 -1.16e-02  0.021498 1.013 9.26e-05
65   2.66e-02 -6.86e-03 -1.78e-02 -1.78e-02 -1.78e-02  0.030996 1.012 1.92e-04
66   3.62e-02 -2.12e-03 -2.85e-02 -2.85e-02 -2.84e-02  0.048142 1.009 4.64e-04
67   2.95e-02  6.54e-03 -2.82e-02 -2.82e-02 -2.80e-02  0.047608 1.009 4.54e-04
68   1.37e-02  8.81e-03 -1.65e-02 -1.65e-02 -1.63e-02  0.028789 1.012 1.66e-04
69   2.86e-03  3.86e-03 -4.64e-03 -4.64e-03 -4.58e-03  0.008575 1.014 1.47e-05
70  -3.92e-03 -1.09e-02  9.70e-03  9.70e-03  9.55e-03 -0.019280 1.015 7.45e-05
71  -5.88e-03 -4.26e-02  3.01e-02  3.01e-02  2.95e-02 -0.064880 1.012 8.43e-04
72  -4.06e-03 -6.85e-02  4.39e-02  4.39e-02  4.31e-02 -0.098792 1.008 1.95e-03
73   7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
74   4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
75   2.36e-02 -1.24e-02 -1.21e-02 -1.21e-02 -1.21e-02  0.024239 1.014 1.18e-04
76   1.43e-02 -5.84e-03 -8.32e-03 -8.32e-03 -8.33e-03  0.015397 1.014 4.75e-05
77   1.33e-02 -3.44e-03 -8.93e-03 -8.93e-03 -8.92e-03  0.015565 1.013 4.85e-05
78   1.92e-02 -1.12e-03 -1.51e-02 -1.51e-02 -1.51e-02  0.025541 1.012 1.31e-04
79   3.54e-02  7.85e-03 -3.38e-02 -3.38e-02 -3.35e-02  0.057114 1.007 6.53e-04
80   3.82e-02  2.46e-02 -4.60e-02 -4.60e-02 -4.55e-02  0.080244 1.003 1.29e-03
81   4.72e-02  6.39e-02 -7.67e-02 -7.67e-02 -7.57e-02  0.141817 0.984 4.00e-03
82   3.81e-02  1.06e-01 -9.42e-02 -9.42e-02 -9.28e-02  0.187314 0.971 6.97e-03
83   2.33e-02  1.69e-01 -1.19e-01 -1.19e-01 -1.17e-01  0.257048 0.947 1.31e-02
84   1.20e-02  2.02e-01 -1.30e-01 -1.30e-01 -1.27e-01  0.291335 0.936 1.67e-02
85   8.13e-02 -5.62e-02 -3.35e-02 -3.35e-02 -3.39e-02  0.081303 1.011 1.32e-03
86   5.12e-02 -3.15e-02 -2.34e-02 -2.34e-02 -2.36e-02  0.051480 1.013 5.31e-04
87   3.21e-02 -1.68e-02 -1.65e-02 -1.65e-02 -1.65e-02  0.032997 1.013 2.18e-04
88   1.43e-02 -5.84e-03 -8.32e-03 -8.32e-03 -8.33e-03  0.015397 1.014 4.75e-05
89   5.08e-03 -1.31e-03 -3.40e-03 -3.40e-03 -3.39e-03  0.005923 1.014 7.03e-06
90  -7.68e-03  4.50e-04  6.05e-03  6.05e-03  6.02e-03 -0.010217 1.013 2.09e-05
91  -6.98e-03 -1.55e-03  6.66e-03  6.66e-03  6.61e-03 -0.011257 1.013 2.54e-05
92  -1.64e-02 -1.05e-02  1.97e-02  1.97e-02  1.95e-02 -0.034447 1.012 2.38e-04
93  -1.75e-02 -2.37e-02  2.84e-02  2.84e-02  2.81e-02 -0.052581 1.010 5.53e-04
94  -1.60e-02 -4.46e-02  3.96e-02  3.96e-02  3.90e-02 -0.078780 1.007 1.24e-03
95  -1.39e-02 -1.00e-01  7.08e-02  7.08e-02  6.96e-02 -0.152756 0.992 4.65e-03
96   8.13e-02 -5.62e-02 -3.35e-02 -3.35e-02 -3.39e-02  0.081303 1.011 1.32e-03
97   5.36e-02 -3.30e-02 -2.45e-02 -2.45e-02 -2.47e-02  0.053868 1.013 5.81e-04
98   2.79e-02 -1.46e-02 -1.43e-02 -1.43e-02 -1.43e-02  0.028618 1.014 1.64e-04
99   8.65e-03 -3.52e-03 -5.03e-03 -5.03e-03 -5.03e-03  0.009297 1.014 1.73e-05
100  6.73e-03 -1.74e-03 -4.51e-03 -4.51e-03 -4.50e-03  0.007851 1.014 1.23e-05
101 -3.44e-03  2.01e-04  2.71e-03  2.71e-03  2.70e-03 -0.004572 1.013 4.19e-06
102 -3.05e-02 -6.77e-03  2.91e-02  2.91e-02  2.89e-02 -0.049244 1.009 4.85e-04
103 -3.90e-02 -2.51e-02  4.69e-02  4.69e-02  4.65e-02 -0.081954 1.002 1.34e-03
104 -4.08e-02 -5.51e-02  6.62e-02  6.62e-02  6.54e-02 -0.122404 0.992 2.99e-03
105 -3.19e-02 -8.89e-02  7.89e-02  7.89e-02  7.77e-02 -0.156942 0.984 4.90e-03
106 -1.96e-02 -1.42e-01  1.00e-01  1.00e-01  9.83e-02 -0.215971 0.967 9.25e-03
107 -1.05e-02 -1.77e-01  1.13e-01  1.13e-01  1.11e-01 -0.254841 0.955 1.28e-02
108  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
109  3.70e-02 -2.28e-02 -1.69e-02 -1.69e-02 -1.70e-02  0.037158 1.014 2.77e-04
110  1.29e-02 -6.78e-03 -6.63e-03 -6.63e-03 -6.66e-03  0.013294 1.015 3.54e-05
111 -8.08e-04  3.29e-04  4.70e-04  4.70e-04  4.70e-04 -0.000869 1.014 1.51e-07
112 -1.15e-02  2.96e-03  7.67e-03  7.67e-03  7.65e-03 -0.013360 1.013 3.58e-05
113 -2.47e-02  1.44e-03  1.94e-02  1.94e-02  1.93e-02 -0.032806 1.011 2.16e-04
114 -3.17e-02 -7.03e-03  3.03e-02  3.03e-02  3.00e-02 -0.051145 1.009 5.24e-04
115 -3.52e-02 -2.27e-02  4.24e-02  4.24e-02  4.20e-02 -0.074026 1.004 1.10e-03
116 -3.51e-02 -4.75e-02  5.70e-02  5.70e-02  5.63e-02 -0.105439 0.998 2.22e-03
117 -2.63e-02 -7.32e-02  6.50e-02  6.50e-02  6.40e-02 -0.129290 0.994 3.33e-03
118 -1.50e-02 -1.09e-01  7.67e-02  7.67e-02  7.53e-02 -0.165362 0.987 5.45e-03
119 -7.63e-03 -1.29e-01  8.26e-02  8.26e-02  8.10e-02 -0.185755 0.984 6.87e-03
120  8.39e-02 -5.81e-02 -3.46e-02 -3.46e-02 -3.49e-02  0.083923 1.011 1.41e-03
121  5.36e-02 -3.30e-02 -2.45e-02 -2.45e-02 -2.47e-02  0.053868 1.013 5.81e-04
122  3.64e-02 -1.91e-02 -1.86e-02 -1.86e-02 -1.87e-02  0.037377 1.013 2.80e-04
123  3.89e-02 -1.59e-02 -2.26e-02 -2.26e-02 -2.26e-02  0.041841 1.011 3.51e-04
124  5.14e-02 -1.33e-02 -3.44e-02 -3.44e-02 -3.43e-02  0.059953 1.007 7.19e-04
125  5.75e-02 -3.36e-03 -4.53e-02 -4.53e-02 -4.51e-02  0.076434 1.002 1.17e-03
126  6.14e-02  1.36e-02 -5.86e-02 -5.86e-02 -5.81e-02  0.099021 0.995 1.96e-03
127  4.10e-02  2.64e-02 -4.94e-02 -4.94e-02 -4.89e-02  0.086193 1.001 1.48e-03
128  2.18e-02  2.95e-02 -3.54e-02 -3.54e-02 -3.50e-02  0.065536 1.008 8.59e-04
129  7.24e-03  2.02e-02 -1.79e-02 -1.79e-02 -1.76e-02  0.035593 1.014 2.54e-04
130 -1.12e-03 -8.10e-03  5.72e-03  5.72e-03  5.62e-03 -0.012339 1.017 3.05e-05
131 -2.12e-03 -3.57e-02  2.29e-02  2.29e-02  2.25e-02 -0.051568 1.015 5.32e-04
132  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
133  4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
134  2.15e-02 -1.12e-02 -1.10e-02 -1.10e-02 -1.10e-02  0.022050 1.014 9.74e-05
135 -2.70e-03  1.10e-03  1.57e-03  1.57e-03  1.57e-03 -0.002902 1.014 1.69e-06
136 -1.48e-02  3.81e-03  9.88e-03  9.88e-03  9.86e-03 -0.017217 1.013 5.94e-05
137 -1.48e-02  8.64e-04  1.16e-02  1.16e-02  1.16e-02 -0.019628 1.013 7.72e-05
138  3.61e-03  8.01e-04 -3.45e-03 -3.45e-03 -3.42e-03  0.005827 1.013 6.80e-06
139  1.47e-03  9.49e-04 -1.77e-03 -1.77e-03 -1.76e-03  0.003099 1.014 1.92e-06
140  7.77e-03  1.05e-02 -1.26e-02 -1.26e-02 -1.25e-02  0.023334 1.014 1.09e-04
141  7.70e-03  2.15e-02 -1.90e-02 -1.90e-02 -1.88e-02  0.037880 1.014 2.87e-04
142  2.06e-03  1.49e-02 -1.05e-02 -1.05e-02 -1.03e-02  0.022671 1.016 1.03e-04
143  1.11e-03  1.87e-02 -1.20e-02 -1.20e-02 -1.18e-02  0.027018 1.017 1.46e-04
144  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
145  4.65e-02 -2.86e-02 -2.12e-02 -2.12e-02 -2.14e-02  0.046705 1.014 4.37e-04
146  1.51e-02 -7.89e-03 -7.73e-03 -7.73e-03 -7.75e-03  0.015483 1.015 4.80e-05
147 -6.48e-03  2.64e-03  3.77e-03  3.77e-03  3.77e-03 -0.006969 1.014 9.73e-06
148 -2.63e-02  6.80e-03  1.76e-02  1.76e-02  1.76e-02 -0.030719 1.012 1.89e-04
149 -4.45e-02  2.60e-03  3.51e-02  3.51e-02  3.49e-02 -0.059185 1.007 7.01e-04
150 -5.30e-02 -1.17e-02  5.05e-02  5.05e-02  5.01e-02 -0.085404 1.000 1.46e-03
151 -5.98e-02 -3.85e-02  7.20e-02  7.20e-02  7.12e-02 -0.125669 0.987 3.15e-03
152 -5.64e-02 -7.62e-02  9.15e-02  9.15e-02  9.04e-02 -0.169238 0.971 5.69e-03
153 -4.08e-02 -1.14e-01  1.01e-01  1.01e-01  9.95e-02 -0.200922 0.964 8.00e-03
154 -2.17e-02 -1.57e-01  1.11e-01  1.11e-01  1.09e-01 -0.238856 0.956 1.13e-02
155 -1.07e-02 -1.80e-01  1.16e-01  1.16e-01  1.13e-01 -0.260183 0.952 1.34e-02
156  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
157  4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
158  3.43e-02 -1.79e-02 -1.76e-02 -1.76e-02 -1.76e-02  0.035187 1.013 2.48e-04
159  2.95e-02 -1.20e-02 -1.71e-02 -1.71e-02 -1.71e-02  0.031668 1.012 2.01e-04
160  3.32e-02 -8.56e-03 -2.22e-02 -2.22e-02 -2.22e-02  0.038714 1.011 3.00e-04
161  4.19e-02 -2.45e-03 -3.30e-02 -3.30e-02 -3.28e-02  0.055681 1.007 6.20e-04
162  5.67e-02  1.26e-02 -5.41e-02 -5.41e-02 -5.37e-02  0.091390 0.998 1.67e-03
163  5.52e-02  3.55e-02 -6.64e-02 -6.64e-02 -6.58e-02  0.115988 0.991 2.68e-03
164  5.36e-02  7.25e-02 -8.71e-02 -8.71e-02 -8.60e-02  0.160989 0.975 5.15e-03
165  3.71e-02  1.03e-01 -9.19e-02 -9.19e-02 -9.05e-02  0.182681 0.973 6.63e-03
166  1.66e-02  1.20e-01 -8.50e-02 -8.50e-02 -8.35e-02  0.183372 0.981 6.69e-03
167  7.70e-03  1.30e-01 -8.33e-02 -8.33e-02 -8.17e-02  0.187423 0.983 6.99e-03
168  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
169  4.88e-02 -3.01e-02 -2.23e-02 -2.23e-02 -2.25e-02  0.049093 1.013 4.83e-04
170  2.15e-02 -1.12e-02 -1.10e-02 -1.10e-02 -1.10e-02  0.022050 1.014 9.74e-05
171  1.08e-03 -4.41e-04 -6.29e-04 -6.29e-04 -6.30e-04  0.001164 1.014 2.72e-07
172 -2.14e-02  5.52e-03  1.43e-02  1.43e-02  1.43e-02 -0.024932 1.012 1.25e-04
173 -4.31e-02  2.52e-03  3.39e-02  3.39e-02  3.38e-02 -0.057300 1.007 6.57e-04
174 -5.77e-02 -1.28e-02  5.50e-02  5.50e-02  5.46e-02 -0.093031 0.997 1.73e-03
175 -6.17e-02 -3.97e-02  7.42e-02  7.42e-02  7.35e-02 -0.129654 0.985 3.35e-03
176  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
177  3.93e-02 -2.42e-02 -1.80e-02 -1.80e-02 -1.81e-02  0.039544 1.014 3.13e-04
178  6.55e-03 -3.43e-03 -3.36e-03 -3.36e-03 -3.37e-03  0.006727 1.015 9.07e-06
179 -2.35e-02  9.58e-03  1.37e-02  1.37e-02  1.37e-02 -0.025270 1.013 1.28e-04
180 -3.96e-02  1.02e-02  2.65e-02  2.65e-02  2.64e-02 -0.046158 1.010 4.27e-04
181 -6.72e-02  3.93e-03  5.29e-02  5.29e-02  5.27e-02 -0.089392 0.998 1.60e-03
182 -7.31e-02 -1.62e-02  6.97e-02  6.97e-02  6.92e-02 -0.117863 0.987 2.77e-03
183  8.13e-02 -5.62e-02 -3.35e-02 -3.35e-02 -3.39e-02  0.081303 1.011 1.32e-03
184  5.36e-02 -3.30e-02 -2.45e-02 -2.45e-02 -2.47e-02  0.053868 1.013 5.81e-04
185  3.21e-02 -1.68e-02 -1.65e-02 -1.65e-02 -1.65e-02  0.032997 1.013 2.18e-04
186  1.62e-02 -6.61e-03 -9.42e-03 -9.42e-03 -9.43e-03  0.017431 1.014 6.09e-05
187  3.42e-03 -8.84e-04 -2.29e-03 -2.29e-03 -2.29e-03  0.003995 1.014 3.20e-06
188 -1.33e-02  7.81e-04  1.05e-02  1.05e-02  1.05e-02 -0.017746 1.013 6.31e-05
189 -2.11e-02 -4.68e-03  2.01e-02  2.01e-02  2.00e-02 -0.034043 1.011 2.32e-04
190 -2.86e-02 -1.84e-02  3.45e-02  3.45e-02  3.41e-02 -0.060163 1.008 7.24e-04
191 -2.67e-02 -3.60e-02  4.33e-02  4.33e-02  4.28e-02 -0.080042 1.005 1.28e-03
192 -2.11e-02 -5.89e-02  5.23e-02  5.23e-02  5.15e-02 -0.104009 1.001 2.16e-03
193 -1.20e-02 -8.70e-02  6.15e-02  6.15e-02  6.04e-02 -0.132619 0.998 3.51e-03
194 -5.68e-03 -9.58e-02  6.15e-02  6.15e-02  6.03e-02 -0.138242 0.999 3.81e-03
195  7.34e-02 -5.08e-02 -3.03e-02 -3.03e-02 -3.06e-02  0.073445 1.012 1.08e-03
196  1.56e-02 -9.61e-03 -7.13e-03 -7.13e-03 -7.18e-03  0.015683 1.016 4.93e-05
197  8.39e-02 -5.81e-02 -3.46e-02 -3.46e-02 -3.49e-02  0.083923 1.011 1.41e-03
198  4.65e-02 -2.86e-02 -2.12e-02 -2.12e-02 -2.14e-02  0.046705 1.014 4.37e-04
199  1.93e-02 -1.01e-02 -9.91e-03 -9.91e-03 -9.95e-03  0.019861 1.014 7.90e-05
200 -2.70e-03  1.10e-03  1.57e-03  1.57e-03  1.57e-03 -0.002902 1.014 1.69e-06
201 -2.63e-02  6.80e-03  1.76e-02  1.76e-02  1.76e-02 -0.030719 1.012 1.89e-04
202 -3.88e-02  2.27e-03  3.06e-02  3.06e-02  3.04e-02 -0.051645 1.008 5.34e-04
203 -4.00e-02 -8.86e-03  3.81e-02  3.81e-02  3.78e-02 -0.064457 1.006 8.31e-04
204 -4.28e-02 -2.75e-02  5.15e-02  5.15e-02  5.10e-02 -0.089888 1.000 1.61e-03
205 -3.16e-02 -4.27e-02  5.13e-02  5.13e-02  5.07e-02 -0.094851 1.001 1.80e-03
206 -2.25e-02 -6.28e-02  5.58e-02  5.58e-02  5.49e-02 -0.110898 1.000 2.46e-03
207 -9.52e-03 -6.89e-02  4.87e-02  4.87e-02  4.78e-02 -0.104985 1.005 2.20e-03
208 -4.06e-03 -6.85e-02  4.39e-02  4.39e-02  4.31e-02 -0.098792 1.008 1.95e-03
209  7.87e-02 -5.44e-02 -3.25e-02 -3.25e-02 -3.28e-02  0.078683 1.011 1.24e-03
210  4.41e-02 -2.72e-02 -2.02e-02 -2.02e-02 -2.03e-02  0.044318 1.014 3.93e-04
211  1.72e-02 -9.01e-03 -8.82e-03 -8.82e-03 -8.85e-03  0.017672 1.015 6.26e-05
212 -1.03e-02  4.18e-03  5.97e-03  5.97e-03  5.97e-03 -0.011035 1.014 2.44e-05
213 -2.63e-02  6.80e-03  1.76e-02  1.76e-02  1.76e-02 -0.030719 1.012 1.89e-04
214 -3.60e-02  2.11e-03  2.84e-02  2.84e-02  2.82e-02 -0.047876 1.009 4.59e-04
215 -4.59e-02 -1.02e-02  4.38e-02  4.38e-02  4.34e-02 -0.073974 1.003 1.09e-03
216 -4.18e-02 -2.69e-02  5.03e-02  5.03e-02  4.98e-02 -0.087904 1.000 1.54e-03
217 -3.72e-02 -5.03e-02  6.05e-02  6.05e-02  5.97e-02 -0.111798 0.995 2.49e-03
218 -2.86e-02 -7.97e-02  7.08e-02  7.08e-02  6.97e-02 -0.140801 0.990 3.95e-03
219 -1.61e-02 -1.17e-01  8.25e-02  8.25e-02  8.11e-02 -0.177985 0.983 6.30e-03
220 -8.39e-03 -1.42e-01  9.08e-02  9.08e-02  8.91e-02 -0.204296 0.977 8.29e-03
221  1.69e-02 -2.44e-02  2.72e-02  5.77e-04  3.59e-04  0.041170 1.021 3.40e-04
222  5.77e-03 -8.34e-03  1.13e-02  1.97e-04  1.23e-04  0.016181 1.020 5.25e-05
223 -7.64e-05  1.10e-04 -1.93e-04 -2.61e-06 -1.63e-06 -0.000261 1.019 1.37e-08
224  3.76e-03 -5.44e-03  1.33e-02  1.28e-04  8.00e-05  0.017265 1.018 5.97e-05
225  9.72e-03 -1.41e-02  5.77e-02  3.32e-04  2.07e-04  0.072738 1.012 1.06e-03
226  5.30e-03 -7.67e-03  9.98e-02  1.81e-04  1.13e-04  0.124126 1.001 3.08e-03
227 -1.10e-02  1.59e-02  1.75e-01 -3.76e-04 -2.35e-04  0.218616 0.968 9.48e-03
228 -3.35e-02  4.84e-02  1.86e-01 -1.14e-03 -7.12e-04  0.237364 0.962 1.12e-02
229 -6.60e-02  9.55e-02  2.21e-01 -2.25e-03 -1.40e-03  0.294129 0.940 1.71e-02
230 -1.05e-01  1.51e-01  2.51e-01 -3.57e-03 -2.23e-03  0.350951 0.919 2.42e-02
231 -1.27e-01  1.84e-01  2.37e-01 -4.34e-03 -2.70e-03  0.351736 0.930 2.43e-02
232 -1.47e-01  2.12e-01  2.45e-01 -5.00e-03 -3.12e-03  0.376569 0.925 2.79e-02
233  1.82e-02 -2.63e-02  2.93e-02  6.21e-04  3.87e-04  0.044360 1.021 3.94e-04
234  1.11e-02 -1.61e-02  2.18e-02  3.79e-04  2.36e-04  0.031141 1.020 1.94e-04
235  1.58e-03 -2.28e-03  3.98e-03  5.38e-05  3.35e-05  0.005389 1.019 5.82e-06
236 -1.52e-03  2.20e-03 -5.39e-03 -5.20e-05 -3.24e-05 -0.006989 1.018 9.79e-06
237 -2.47e-03  3.57e-03 -1.47e-02 -8.43e-05 -5.25e-05 -0.018483 1.017 6.84e-05
238 -2.14e-03  3.10e-03 -4.04e-02 -7.32e-05 -4.56e-05 -0.050183 1.015 5.04e-04
239  3.12e-03 -4.50e-03 -4.94e-02  1.06e-04  6.63e-05 -0.061765 1.013 7.64e-04
240  1.42e-02 -2.06e-02 -7.89e-02  4.85e-04  3.02e-04 -0.100880 1.007 2.03e-03
241  2.19e-02 -3.17e-02 -7.36e-02  7.49e-04  4.67e-04 -0.097705 1.009 1.91e-03
242  3.10e-02 -4.48e-02 -7.44e-02  1.06e-03  6.60e-04 -0.103958 1.010 2.16e-03
243  4.15e-02 -5.99e-02 -7.72e-02  1.42e-03  8.82e-04 -0.114678 1.010 2.63e-03
244  5.30e-02 -7.67e-02 -8.88e-02  1.81e-03  1.13e-03 -0.136255 1.008 3.71e-03
245  2.08e-02 -3.01e-02  3.36e-02  7.11e-04  4.43e-04  0.050740 1.020 5.16e-04
246  7.91e-03 -1.14e-02  1.55e-02  2.70e-04  1.68e-04  0.022164 1.020 9.84e-05
247 -9.03e-04  1.31e-03 -2.28e-03 -3.08e-05 -1.92e-05 -0.003086 1.019 1.91e-06
248 -3.87e-03  5.60e-03 -1.37e-02 -1.32e-04 -8.23e-05 -0.017769 1.018 6.33e-05
249 -2.47e-03  3.57e-03 -1.47e-02 -8.43e-05 -5.25e-05 -0.018483 1.017 6.84e-05
250 -1.27e-03  1.83e-03 -2.39e-02 -4.33e-05 -2.70e-05 -0.029690 1.016 1.77e-04
251  6.59e-04 -9.52e-04 -1.05e-02  2.25e-05  1.40e-05 -0.013058 1.017 3.42e-05
252  5.37e-03 -7.77e-03 -2.98e-02  1.83e-04  1.14e-04 -0.038119 1.016 2.91e-04
253  1.34e-02 -1.94e-02 -4.50e-02  4.58e-04  2.85e-04 -0.059779 1.015 7.15e-04
254  1.83e-02 -2.65e-02 -4.39e-02  6.24e-04  3.89e-04 -0.061315 1.016 7.53e-04
255  3.49e-02 -5.05e-02 -6.50e-02  1.19e-03  7.43e-04 -0.096591 1.013 1.87e-03
256  4.33e-02 -6.27e-02 -7.25e-02  1.48e-03  9.22e-04 -0.111349 1.012 2.48e-03
257  1.95e-02 -2.82e-02  3.14e-02  6.66e-04  4.15e-04  0.047550 1.020 4.53e-04
258  7.91e-03 -1.14e-02  1.55e-02  2.70e-04  1.68e-04  0.022164 1.020 9.84e-05
259 -3.38e-03  4.89e-03 -8.54e-03 -1.15e-04 -7.20e-05 -0.011562 1.019 2.68e-05
260 -3.28e-03  4.75e-03 -1.16e-02 -1.12e-04 -6.99e-05 -0.015074 1.018 4.55e-05
261 -1.08e-02  1.57e-02 -6.43e-02 -3.70e-04 -2.30e-04 -0.081061 1.011 1.31e-03
262 -5.10e-03  7.38e-03 -9.61e-02 -1.74e-04 -1.09e-04 -0.119473 1.002 2.85e-03
263  8.05e-03 -1.16e-02 -1.28e-01  2.75e-04  1.71e-04 -0.159524 0.991 5.07e-03
264  2.91e-02 -4.20e-02 -1.61e-01  9.91e-04  6.18e-04 -0.206085 0.976 8.44e-03
265  5.81e-02 -8.40e-02 -1.95e-01  1.98e-03  1.24e-03 -0.258764 0.957 1.33e-02
266  9.62e-02 -1.39e-01 -2.31e-01  3.28e-03  2.05e-03 -0.322333 0.934 2.05e-02
267  1.39e-01 -2.00e-01 -2.58e-01  4.73e-03  2.95e-03 -0.383350 0.915 2.88e-02
268  1.67e-01 -2.42e-01 -2.80e-01  5.71e-03  3.56e-03 -0.430201 0.897 3.61e-02
269  1.69e-02 -2.44e-02  2.72e-02  5.77e-04  3.59e-04  0.041170 1.021 3.40e-04
270  4.71e-03 -6.80e-03  9.25e-03  1.61e-04  1.00e-04  0.013189 1.020 3.48e-05
271 -7.64e-05  1.10e-04 -1.93e-04 -2.61e-06 -1.63e-06 -0.000261 1.019 1.37e-08
272 -9.36e-04  1.35e-03 -3.31e-03 -3.19e-05 -1.99e-05 -0.004294 1.018 3.69e-06
273  1.71e-03 -2.47e-03  1.01e-02  5.83e-05  3.63e-05  0.012780 1.017 3.27e-05
274  1.03e-03 -1.49e-03  1.94e-02  3.51e-05  2.19e-05  0.024080 1.017 1.16e-04
275 -1.80e-03  2.60e-03  2.85e-02 -6.13e-05 -3.82e-05  0.035629 1.016 2.54e-04
276 -5.30e-03  7.66e-03  2.94e-02 -1.81e-04 -1.13e-04  0.037611 1.016 2.83e-04
277 -1.82e-02  2.63e-02  6.09e-02 -6.20e-04 -3.86e-04  0.080910 1.012 1.31e-03
278 -3.94e-02  5.69e-02  9.43e-02 -1.34e-03 -8.37e-04  0.131871 1.004 3.47e-03
279 -6.20e-02  8.96e-02  1.15e-01 -2.12e-03 -1.32e-03  0.171472 0.998 5.86e-03
280 -6.55e-02  9.47e-02  1.10e-01 -2.24e-03 -1.39e-03  0.168371 1.001 5.66e-03
281  1.95e-02 -2.82e-02  3.14e-02  6.66e-04  4.15e-04  0.047550 1.020 4.53e-04
282  3.64e-03 -5.26e-03  7.15e-03  1.24e-04  7.74e-05  0.010197 1.020 2.08e-05
283 -4.21e-03  6.09e-03 -1.06e-02 -1.44e-04 -8.95e-05 -0.014388 1.019 4.15e-05
284 -3.28e-03  4.75e-03 -1.16e-02 -1.12e-04 -6.99e-05 -0.015074 1.018 4.55e-05
285 -1.43e-03  2.06e-03 -8.47e-03 -4.86e-05 -3.03e-05 -0.010667 1.017 2.28e-05
286 -6.51e-05  9.41e-05 -1.23e-03 -2.22e-06 -1.38e-06 -0.001524 1.017 4.66e-07
287 -5.05e-04  7.30e-04  8.00e-03 -1.72e-05 -1.07e-05  0.010002 1.017 2.00e-05
288  9.56e-04 -1.38e-03 -5.31e-03  3.26e-05  2.03e-05 -0.006782 1.018 9.21e-06
289 -1.15e-03  1.67e-03  3.87e-03 -3.94e-05 -2.46e-05  0.005143 1.018 5.30e-06
290 -1.73e-02  2.50e-02  4.14e-02 -5.90e-04 -3.67e-04  0.057910 1.016 6.72e-04
291 -3.69e-02  5.33e-02  6.87e-02 -1.26e-03 -7.84e-04  0.102014 1.013 2.08e-03
292 -4.86e-02  7.02e-02  8.13e-02 -1.66e-03 -1.03e-03  0.124727 1.010 3.11e-03
293  1.56e-02 -2.25e-02  2.51e-02  5.32e-04  3.31e-04  0.037980 1.021 2.89e-04
294  1.50e-03 -2.17e-03  2.95e-03  5.13e-05  3.20e-05  0.004214 1.020 3.56e-06
295 -3.38e-03  4.89e-03 -8.54e-03 -1.15e-04 -7.20e-05 -0.011562 1.019 2.68e-05
296 -3.87e-03  5.60e-03 -1.37e-02 -1.32e-04 -8.23e-05 -0.017769 1.018 6.33e-05
297 -3.51e-03  5.08e-03 -2.09e-02 -1.20e-04 -7.47e-05 -0.026300 1.017 1.39e-04
298 -1.60e-03  2.31e-03 -3.01e-02 -5.45e-05 -3.40e-05 -0.037373 1.016 2.80e-04
299  2.21e-03 -3.20e-03 -3.51e-02  7.54e-05  4.70e-05 -0.043814 1.015 3.84e-04
300  9.79e-03 -1.42e-02 -5.44e-02  3.34e-04  2.08e-04 -0.069479 1.013 9.66e-04
301  1.40e-02 -2.03e-02 -4.71e-02  4.79e-04  2.98e-04 -0.062486 1.015 7.82e-04
302  1.83e-02 -2.65e-02 -4.39e-02  6.24e-04  3.89e-04 -0.061315 1.016 7.53e-04
303  1.86e-02 -2.69e-02 -3.46e-02  6.35e-04  3.95e-04 -0.051430 1.018 5.30e-04
304  2.28e-02 -3.29e-02 -3.81e-02  7.77e-04  4.84e-04 -0.058511 1.019 6.86e-04
305  1.56e-02 -2.25e-02  2.51e-02  5.32e-04  3.31e-04  0.037980 1.021 2.89e-04
306  1.50e-03 -2.17e-03  2.95e-03  5.13e-05  3.20e-05  0.004214 1.020 3.56e-06
307 -3.38e-03  4.89e-03 -8.54e-03 -1.15e-04 -7.20e-05 -0.011562 1.019 2.68e-05
308 -3.87e-03  5.60e-03 -1.37e-02 -1.32e-04 -8.23e-05 -0.017769 1.018 6.33e-05
309 -1.77e-03  2.56e-03 -1.05e-02 -6.05e-05 -3.77e-05 -0.013272 1.017 3.53e-05
310 -6.51e-05  9.41e-05 -1.23e-03 -2.22e-06 -1.38e-06 -0.001524 1.017 4.66e-07
311 -1.67e-03  2.41e-03  2.65e-02 -5.69e-05 -3.55e-05  0.033066 1.016 2.19e-04
312 -2.36e-03  3.41e-03  1.31e-02 -8.04e-05 -5.01e-05  0.016718 1.017 5.60e-05
313 -1.03e-02  1.48e-02  3.44e-02 -3.50e-04 -2.18e-04  0.045715 1.016 4.19e-04
314 -1.90e-02  2.74e-02  4.55e-02 -6.48e-04 -4.04e-04  0.063591 1.016 8.10e-04
315 -1.08e-02  1.56e-02  2.00e-02 -3.67e-04 -2.29e-04  0.029770 1.020 1.78e-04
316 -2.68e-02  3.87e-02  4.48e-02 -9.13e-04 -5.69e-04  0.068756 1.018 9.46e-04
317  1.56e-02 -2.25e-02  2.51e-02  5.32e-04  3.31e-04  0.037980 1.021 2.89e-04
318  3.64e-03 -5.26e-03  7.15e-03  1.24e-04  7.74e-05  0.010197 1.020 2.08e-05
319 -2.56e-03  3.70e-03 -6.45e-03 -8.73e-05 -5.44e-05 -0.008737 1.019 1.53e-05
320 -3.28e-03  4.75e-03 -1.16e-02 -1.12e-04 -6.99e-05 -0.015074 1.018 4.55e-05
321 -3.51e-03  5.08e-03 -2.09e-02 -1.20e-04 -7.47e-05 -0.026300 1.017 1.39e-04
322 -9.40e-04  1.36e-03 -1.77e-02 -3.21e-05 -2.00e-05 -0.022008 1.017 9.70e-05
323 -2.46e-04  3.56e-04  3.90e-03 -8.40e-06 -5.23e-06  0.004878 1.017 4.77e-06
324 -1.48e-04  2.14e-04  8.23e-04 -5.06e-06 -3.15e-06  0.001051 1.018 2.21e-07
325 -1.21e-02  1.75e-02  4.05e-02 -4.12e-04 -2.57e-04  0.053833 1.016 5.80e-04
326 -3.85e-02  5.57e-02  9.23e-02 -1.31e-03 -8.19e-04  0.129021 1.005 3.33e-03
327 -8.40e-02  1.21e-01  1.56e-01 -2.86e-03 -1.79e-03  0.232169 0.980 1.07e-02
328 -1.19e-01  1.73e-01  2.00e-01 -4.07e-03 -2.54e-03  0.306595 0.956 1.86e-02
329  1.95e-02 -2.82e-02  3.14e-02  6.66e-04  4.15e-04  0.047550 1.020 4.53e-04
330  3.64e-03 -5.26e-03  7.15e-03  1.24e-04  7.74e-05  0.010197 1.020 2.08e-05
331 -2.56e-03  3.70e-03 -6.45e-03 -8.73e-05 -5.44e-05 -0.008737 1.019 1.53e-05
332 -4.46e-03  6.45e-03 -1.58e-02 -1.52e-04 -9.48e-05 -0.020464 1.018 8.39e-05
333 -4.21e-03  6.09e-03 -2.50e-02 -1.44e-04 -8.96e-05 -0.031512 1.017 1.99e-04
334 -1.82e-03  2.62e-03 -3.42e-02 -6.20e-05 -3.86e-05 -0.042497 1.015 3.62e-04
335  2.21e-03 -3.20e-03 -3.51e-02  7.54e-05  4.70e-05 -0.043814 1.015 3.84e-04
336  1.02e-02 -1.47e-02 -5.64e-02  3.47e-04  2.16e-04 -0.072094 1.012 1.04e-03
337  1.46e-02 -2.12e-02 -4.91e-02  5.00e-04  3.11e-04 -0.065193 1.014 8.51e-04
338  2.85e-02 -4.12e-02 -6.83e-02  9.72e-04  6.06e-04 -0.095423 1.011 1.82e-03
339  4.91e-02 -7.10e-02 -9.14e-02  1.68e-03  1.04e-03 -0.135801 1.006 3.68e-03
340  7.37e-02 -1.07e-01 -1.23e-01  2.51e-03  1.57e-03 -0.189309 0.996 7.14e-03
341 -7.10e-03  1.03e-02 -2.42e-04 -1.14e-02 -1.51e-04 -0.017296 1.022 5.99e-05
342 -1.27e-02  1.84e-02 -4.34e-04 -2.50e-02 -2.71e-04 -0.035678 1.019 2.55e-04
343 -1.69e-02  2.44e-02 -5.77e-04 -4.26e-02 -3.59e-04 -0.057725 1.016 6.67e-04
344 -1.58e-02  2.29e-02 -5.40e-04 -5.60e-02 -3.36e-04 -0.072600 1.013 1.05e-03
345 -1.13e-02  1.63e-02 -3.86e-04 -6.71e-02 -2.40e-04 -0.084541 1.010 1.43e-03
346 -3.60e-03  5.21e-03 -1.23e-04 -6.78e-02 -7.67e-05 -0.084368 1.010 1.42e-03
347  3.81e-03 -5.50e-03  1.30e-04 -6.04e-02  8.10e-05 -0.075450 1.011 1.14e-03
348  1.18e-02 -1.70e-02  4.01e-04 -6.53e-02  2.50e-04 -0.083429 1.011 1.39e-03
349  1.06e-02 -1.53e-02  3.61e-04 -3.55e-02  2.25e-04 -0.047149 1.016 4.45e-04
350  7.90e-04 -1.14e-03  2.70e-05 -1.89e-03  1.68e-05 -0.002647 1.019 1.40e-06
351 -1.37e-02  1.98e-02 -4.66e-04  2.54e-02 -2.91e-04  0.037790 1.019 2.86e-04
352 -3.00e-02  4.34e-02 -1.02e-03  5.02e-02 -6.38e-04  0.077040 1.017 1.19e-03
353 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
354 -1.70e-02  2.46e-02 -5.80e-04 -3.34e-02 -3.61e-04 -0.047649 1.019 4.55e-04
355 -1.44e-02  2.08e-02 -4.92e-04 -3.64e-02 -3.06e-04 -0.049243 1.017 4.86e-04
356 -1.05e-02  1.52e-02 -3.59e-04 -3.72e-02 -2.24e-04 -0.048321 1.016 4.68e-04
357 -3.63e-03  5.25e-03 -1.24e-04 -2.16e-02 -7.72e-05 -0.027168 1.017 1.48e-04
358 -6.49e-04  9.38e-04 -2.21e-05 -1.22e-02 -1.38e-05 -0.015179 1.017 4.62e-05
359 -8.49e-04  1.23e-03 -2.90e-05  1.35e-02 -1.81e-05  0.016835 1.017 5.68e-05
360 -3.34e-03  4.83e-03 -1.14e-04  1.85e-02 -7.10e-05  0.023681 1.017 1.12e-04
361 -2.04e-02  2.95e-02 -6.96e-04  6.84e-02 -4.34e-04  0.090846 1.010 1.65e-03
362 -4.93e-02  7.12e-02 -1.68e-03  1.18e-01 -1.05e-03  0.165166 0.996 5.44e-03
363 -7.48e-02  1.08e-01 -2.55e-03  1.39e-01 -1.59e-03  0.206837 0.988 8.52e-03
364 -8.95e-02  1.29e-01 -3.05e-03  1.50e-01 -1.90e-03  0.229919 0.984 1.05e-02
365 -1.10e-02  1.59e-02 -3.76e-04 -1.78e-02 -2.34e-04 -0.026863 1.021 1.45e-04
366 -1.59e-02  2.30e-02 -5.44e-04 -3.13e-02 -3.39e-04 -0.044656 1.019 3.99e-04
367 -1.61e-02  2.32e-02 -5.48e-04 -4.05e-02 -3.42e-04 -0.054897 1.017 6.03e-04
368 -1.35e-02  1.95e-02 -4.60e-04 -4.76e-02 -2.86e-04 -0.061806 1.015 7.65e-04
369 -7.46e-03  1.08e-02 -2.55e-04 -4.43e-02 -1.59e-04 -0.055841 1.014 6.24e-04
370 -2.62e-03  3.79e-03 -8.94e-05 -4.93e-02 -5.57e-05 -0.061288 1.013 7.52e-04
371  2.00e-03 -2.88e-03  6.81e-05 -3.16e-02  4.24e-05 -0.039541 1.016 3.13e-04
372  9.55e-03 -1.38e-02  3.26e-04 -5.30e-02  2.03e-04 -0.067736 1.013 9.18e-04
373  2.21e-02 -3.20e-02  7.56e-04 -7.43e-02  4.71e-04 -0.098609 1.009 1.94e-03
374  5.00e-02 -7.23e-02  1.71e-03 -1.20e-01  1.06e-03 -0.167639 0.995 5.60e-03
375  7.25e-02 -1.05e-01  2.47e-03 -1.35e-01  1.54e-03 -0.200364 0.990 7.99e-03
376  1.02e-01 -1.48e-01  3.49e-03 -1.71e-01  2.17e-03 -0.262510 0.973 1.37e-02
377 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
378 -1.70e-02  2.46e-02 -5.80e-04 -3.34e-02 -3.61e-04 -0.047649 1.019 4.55e-04
379 -1.61e-02  2.32e-02 -5.48e-04 -4.05e-02 -3.42e-04 -0.054897 1.017 6.03e-04
380 -8.77e-03  1.27e-02 -2.99e-04 -3.10e-02 -1.86e-04 -0.040232 1.017 3.24e-04
381 -3.63e-03  5.25e-03 -1.24e-04 -2.16e-02 -7.72e-05 -0.027168 1.017 1.48e-04
382 -1.02e-04  1.47e-04 -3.47e-06 -1.91e-03 -2.16e-06 -0.002378 1.017 1.13e-06
383 -1.50e-03  2.16e-03 -5.10e-05  2.37e-02 -3.18e-05  0.029649 1.016 1.76e-04
384 -5.92e-03  8.55e-03 -2.02e-04  3.28e-02 -1.26e-04  0.041965 1.016 3.53e-04
385 -2.89e-02  4.18e-02 -9.87e-04  9.70e-02 -6.15e-04  0.128834 1.003 3.32e-03
386 -7.58e-02  1.10e-01 -2.59e-03  1.82e-01 -1.61e-03  0.254135 0.965 1.28e-02
387 -1.15e-01  1.66e-01 -3.91e-03  2.13e-01 -2.44e-03  0.316792 0.947 1.98e-02
388 -1.34e-01  1.93e-01 -4.56e-03  2.24e-01 -2.84e-03  0.343615 0.940 2.33e-02
389 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
390 -1.27e-02  1.84e-02 -4.34e-04 -2.50e-02 -2.71e-04 -0.035678 1.019 2.55e-04
391 -1.52e-02  2.20e-02 -5.20e-04 -3.85e-02 -3.24e-04 -0.052070 1.017 5.43e-04
392 -7.59e-03  1.10e-02 -2.59e-04 -2.68e-02 -1.61e-04 -0.034840 1.017 2.43e-04
393  1.94e-03 -2.80e-03  6.62e-05  1.15e-02  4.13e-05  0.014517 1.017 4.22e-05
394  2.52e-03 -3.65e-03  8.62e-05  4.75e-02  5.37e-05  0.059090 1.014 6.99e-04
395 -6.29e-03  9.09e-03 -2.15e-04  9.98e-02 -1.34e-04  0.124648 1.001 3.10e-03
396 -2.51e-02  3.63e-02 -8.58e-04  1.40e-01 -5.34e-04  0.178292 0.986 6.33e-03
397 -6.09e-02  8.80e-02 -2.08e-03  2.04e-01 -1.29e-03  0.271090 0.951 1.45e-02
398 -1.09e-01  1.57e-01 -3.71e-03  2.61e-01 -2.31e-03  0.364626 0.912 2.61e-02
399 -1.53e-01  2.21e-01 -5.21e-03  2.84e-01 -3.25e-03  0.422219 0.894 3.48e-02
400 -1.74e-01  2.51e-01 -5.93e-03  2.91e-01 -3.69e-03  0.446241 0.889 3.88e-02
401 -1.10e-02  1.59e-02 -3.76e-04 -1.78e-02 -2.34e-04 -0.026863 1.021 1.45e-04
402 -1.81e-02  2.61e-02 -6.17e-04 -3.55e-02 -3.84e-04 -0.050642 1.018 5.14e-04
403 -1.77e-02  2.56e-02 -6.05e-04 -4.47e-02 -3.77e-04 -0.060553 1.016 7.34e-04
404 -1.41e-02  2.03e-02 -4.80e-04 -4.97e-02 -2.99e-04 -0.064504 1.014 8.33e-04
405 -6.77e-03  9.78e-03 -2.31e-04 -4.02e-02 -1.44e-04 -0.050626 1.015 5.13e-04
406 -2.07e-03  2.99e-03 -7.07e-05 -3.90e-02 -4.40e-05 -0.048474 1.015 4.71e-04
407  9.61e-04 -1.39e-03  3.28e-05 -1.52e-02  2.04e-05 -0.019037 1.017 7.26e-05
408  1.45e-03 -2.09e-03  4.94e-05 -8.03e-03  3.08e-05 -0.010263 1.018 2.11e-05
409 -6.42e-03  9.28e-03 -2.19e-04  2.15e-02 -1.36e-04  0.028581 1.017 1.64e-04
410 -1.87e-02  2.70e-02 -6.38e-04  4.48e-02 -3.98e-04  0.062645 1.016 7.86e-04
411 -2.79e-03  4.03e-03 -9.52e-05  5.20e-03 -5.93e-05  0.007717 1.020 1.19e-05
412  1.35e-02 -1.95e-02  4.61e-04 -2.26e-02  2.87e-04 -0.034709 1.020 2.41e-04
413 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
414 -1.81e-02  2.61e-02 -6.17e-04 -3.55e-02 -3.84e-04 -0.050642 1.018 5.14e-04
415 -2.19e-02  3.16e-02 -7.46e-04 -5.52e-02 -4.65e-04 -0.074696 1.014 1.12e-03
416 -1.88e-02  2.71e-02 -6.40e-04 -6.64e-02 -3.99e-04 -0.086098 1.011 1.48e-03
417 -1.30e-02  1.89e-02 -4.45e-04 -7.75e-02 -2.77e-04 -0.097600 1.008 1.90e-03
418 -5.69e-03  8.23e-03 -1.94e-04 -1.07e-01 -1.21e-04 -0.133196 0.999 3.54e-03
419  6.40e-03 -9.25e-03  2.18e-04 -1.02e-01  1.36e-04 -0.126861 1.001 3.21e-03
420  2.14e-02 -3.09e-02  7.29e-04 -1.19e-01  4.54e-04 -0.151604 0.995 4.58e-03
421  3.19e-02 -4.61e-02  1.09e-03 -1.07e-01  6.78e-04 -0.142053 0.999 4.03e-03
422  4.07e-02 -5.88e-02  1.39e-03 -9.75e-02  8.65e-04 -0.136239 1.003 3.71e-03
423  5.82e-02 -8.42e-02  1.99e-03 -1.08e-01  1.24e-03 -0.160984 1.001 5.17e-03
424  6.44e-02 -9.30e-02  2.20e-03 -1.08e-01  1.37e-03 -0.165359 1.002 5.46e-03
425 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
426 -1.70e-02  2.46e-02 -5.80e-04 -3.34e-02 -3.61e-04 -0.047649 1.019 4.55e-04
427 -1.77e-02  2.56e-02 -6.05e-04 -4.47e-02 -3.77e-04 -0.060553 1.016 7.34e-04
428 -1.52e-02  2.20e-02 -5.20e-04 -5.39e-02 -3.24e-04 -0.069901 1.014 9.78e-04
429 -6.77e-03  9.78e-03 -2.31e-04 -4.02e-02 -1.44e-04 -0.050626 1.015 5.13e-04
430 -2.84e-03  4.10e-03 -9.68e-05 -5.34e-02 -6.03e-05 -0.066415 1.013 8.83e-04
431  3.16e-03 -4.57e-03  1.08e-04 -5.01e-02  6.72e-05 -0.062620 1.013 7.85e-04
432  5.86e-03 -8.48e-03  2.00e-04 -3.26e-02  1.25e-04 -0.041602 1.016 3.47e-04
433 -2.77e-03  4.01e-03 -9.47e-05  9.30e-03 -5.90e-05  0.012354 1.018 3.06e-05
434 -2.29e-02  3.32e-02 -7.83e-04  5.50e-02 -4.88e-04  0.076851 1.014 1.18e-03
435 -6.27e-02  9.07e-02 -2.14e-03  1.17e-01 -1.33e-03  0.173490 0.998 6.00e-03
436 -7.12e-02  1.03e-01 -2.43e-03  1.19e-01 -1.51e-03  0.182948 0.997 6.67e-03
437 -7.10e-03  1.03e-02 -2.42e-04 -1.14e-02 -1.51e-04 -0.017296 1.022 5.99e-05
438 -1.59e-02  2.30e-02 -5.44e-04 -3.13e-02 -3.39e-04 -0.044656 1.019 3.99e-04
439 -1.77e-02  2.56e-02 -6.05e-04 -4.47e-02 -3.77e-04 -0.060553 1.016 7.34e-04
440 -1.29e-02  1.86e-02 -4.40e-04 -4.56e-02 -2.74e-04 -0.059109 1.015 7.00e-04
441 -9.90e-03  1.43e-02 -3.38e-04 -5.88e-02 -2.11e-04 -0.074101 1.012 1.10e-03
442 -2.84e-03  4.10e-03 -9.68e-05 -5.34e-02 -6.03e-05 -0.066415 1.013 8.83e-04
443  2.90e-03 -4.19e-03  9.90e-05 -4.60e-02  6.17e-05 -0.057490 1.014 6.62e-04
444  8.81e-03 -1.27e-02  3.01e-04 -4.89e-02  1.87e-04 -0.062508 1.014 7.82e-04
445  1.06e-02 -1.53e-02  3.61e-04 -3.55e-02  2.25e-04 -0.047149 1.016 4.45e-04
446 -7.68e-03  1.11e-02 -2.62e-04  1.84e-02 -1.63e-04  0.025733 1.019 1.33e-04
447 -3.00e-02  4.34e-02 -1.02e-03  5.58e-02 -6.38e-04  0.082933 1.015 1.38e-03
448 -4.94e-02  7.14e-02 -1.68e-03  8.26e-02 -1.05e-03  0.126803 1.010 3.21e-03
449 -8.41e-03  1.22e-02 -2.87e-04 -1.35e-02 -1.79e-04 -0.020484 1.022 8.41e-05
450 -1.06e-02  1.53e-02 -3.61e-04 -2.08e-02 -2.25e-04 -0.029693 1.020 1.77e-04
451 -1.36e-02  1.96e-02 -4.64e-04 -3.43e-02 -2.89e-04 -0.046416 1.017 4.31e-04
452 -1.23e-02  1.78e-02 -4.19e-04 -4.35e-02 -2.61e-04 -0.056411 1.015 6.37e-04
453 -5.72e-03  8.27e-03 -1.95e-04 -3.40e-02 -1.22e-04 -0.042805 1.016 3.67e-04
454 -1.63e-03  2.36e-03 -5.57e-05 -3.07e-02 -3.47e-05 -0.038227 1.016 2.93e-04
455 -2.03e-04  2.93e-04 -6.93e-06  3.22e-03 -4.32e-06  0.004023 1.017 3.24e-06
456 -4.44e-03  6.42e-03 -1.52e-04  2.47e-02 -9.45e-05  0.031516 1.017 1.99e-04
457 -1.67e-02  2.42e-02 -5.72e-04  5.62e-02 -3.56e-04  0.074590 1.013 1.11e-03
458 -4.84e-02  7.00e-02 -1.65e-03  1.16e-01 -1.03e-03  0.162309 0.997 5.25e-03
459 -7.92e-02  1.14e-01 -2.70e-03  1.47e-01 -1.68e-03  0.218989 0.984 9.54e-03
460 -1.09e-01  1.58e-01 -3.72e-03  1.83e-01 -2.32e-03  0.280260 0.966 1.56e-02
461  1.09e-03 -1.57e-03  3.70e-05  3.70e-05  1.78e-03  0.002686 1.022 1.45e-06
462 -8.03e-03  1.16e-02 -2.74e-04 -2.74e-04 -1.61e-02 -0.022946 1.020 1.05e-04
463 -8.21e-03  1.19e-02 -2.80e-04 -2.80e-04 -2.13e-02 -0.028788 1.018 1.66e-04
464 -4.92e-03  7.11e-03 -1.68e-04 -1.68e-04 -1.81e-02 -0.023445 1.018 1.10e-04
465 -2.68e-03  3.88e-03 -9.16e-05 -9.16e-05 -1.70e-02 -0.021396 1.017 9.17e-05
466 -4.20e-04  6.07e-04 -1.43e-05 -1.43e-05 -9.70e-03 -0.012033 1.017 2.90e-05
467 -1.31e-03  1.90e-03 -4.48e-05 -4.48e-05  1.83e-02  0.022855 1.017 1.05e-04
468  4.13e-03 -5.97e-03  1.41e-04  1.41e-04 -2.21e-02 -0.028140 1.017 1.59e-04
469  3.86e-03 -5.59e-03  1.32e-04  1.32e-04 -1.28e-02 -0.016860 1.018 5.69e-05
470  1.27e-02 -1.84e-02  4.34e-04  4.34e-04 -3.03e-02 -0.042102 1.018 3.55e-04
471  1.90e-02 -2.75e-02  6.49e-04  6.49e-04 -3.54e-02 -0.052215 1.019 5.46e-04
472  2.57e-02 -3.71e-02  8.77e-04  8.77e-04 -4.31e-02 -0.065656 1.018 8.63e-04
473  1.09e-03 -1.57e-03  3.70e-05  3.70e-05  1.78e-03  0.002686 1.022 1.45e-06
474 -1.01e-02  1.46e-02 -3.45e-04 -3.45e-04 -2.04e-02 -0.028939 1.020 1.68e-04
475 -1.06e-02  1.54e-02 -3.63e-04 -3.63e-04 -2.76e-02 -0.037291 1.018 2.79e-04
476 -5.49e-03  7.93e-03 -1.87e-04 -1.87e-04 -2.02e-02 -0.026153 1.018 1.37e-04
477 -2.68e-03  3.88e-03 -9.16e-05 -9.16e-05 -1.70e-02 -0.021396 1.017 9.17e-05
478 -2.40e-04  3.46e-04 -8.18e-06 -8.18e-06 -5.54e-03 -0.006871 1.017 9.46e-06
479 -2.06e-03  2.97e-03 -7.02e-05 -7.02e-05  2.87e-02  0.035790 1.016 2.57e-04
480 -4.01e-03  5.79e-03 -1.37e-04 -1.37e-04  2.14e-02  0.027271 1.017 1.49e-04
481 -1.43e-02  2.07e-02 -4.89e-04 -4.89e-04  4.72e-02  0.062460 1.015 7.81e-04
482 -3.06e-02  4.43e-02 -1.05e-03 -1.05e-03  7.30e-02  0.101499 1.011 2.06e-03
483 -5.86e-02  8.47e-02 -2.00e-03 -2.00e-03  1.09e-01  0.160963 1.001 5.17e-03
484 -6.90e-02  9.97e-02 -2.35e-03 -2.35e-03  1.16e-01  0.176350 1.000 6.20e-03
485  1.09e-03 -1.57e-03  3.70e-05  3.70e-05  1.78e-03  0.002686 1.022 1.45e-06
486 -3.83e-03  5.54e-03 -1.31e-04 -1.31e-04 -7.71e-03 -0.010962 1.020 2.41e-05
487 -5.78e-03  8.36e-03 -1.97e-04 -1.97e-04 -1.50e-02 -0.020287 1.019 8.25e-05
488  1.33e-03 -1.92e-03  4.53e-05  4.53e-05  4.90e-03  0.006332 1.018 8.03e-06
489  6.52e-03 -9.43e-03  2.23e-04  2.23e-04  4.14e-02  0.052018 1.015 5.42e-04
490  2.55e-03 -3.69e-03  8.71e-05  8.71e-05  5.90e-02  0.073166 1.012 1.07e-03
491 -5.63e-03  8.14e-03 -1.92e-04 -1.92e-04  7.86e-02  0.097952 1.008 1.92e-03
492 -9.43e-03  1.36e-02 -3.22e-04 -3.22e-04  5.04e-02  0.064233 1.014 8.26e-04
493 -9.93e-03  1.43e-02 -3.39e-04 -3.39e-04  3.27e-02  0.043305 1.017 3.76e-04
494  5.78e-04 -8.35e-04  1.97e-05  1.97e-05 -1.38e-03 -0.001914 1.019 7.34e-07
495  1.90e-02 -2.75e-02  6.49e-04  6.49e-04 -3.54e-02 -0.052215 1.019 5.46e-04
496  3.06e-02 -4.42e-02  1.04e-03  1.04e-03 -5.14e-02 -0.078220 1.017 1.22e-03
497  1.09e-03 -1.57e-03  3.70e-05  3.70e-05  1.78e-03  0.002686 1.022 1.45e-06
498 -8.03e-03  1.16e-02 -2.74e-04 -2.74e-04 -1.61e-02 -0.022946 1.020 1.05e-04
499 -9.01e-03  1.30e-02 -3.08e-04 -3.08e-04 -2.34e-02 -0.031622 1.018 2.00e-04
500 -4.35e-03  6.29e-03 -1.48e-04 -1.48e-04 -1.60e-02 -0.020738 1.018 8.62e-05
501 -2.68e-03  3.88e-03 -9.16e-05 -9.16e-05 -1.70e-02 -0.021396 1.017 9.17e-05
502 -9.60e-04  1.39e-03 -3.28e-05 -3.28e-05 -2.22e-02 -0.027518 1.017 1.52e-04
503  2.85e-03 -4.12e-03  9.73e-05  9.73e-05 -3.98e-02 -0.049576 1.015 4.92e-04
504  9.95e-03 -1.44e-02  3.40e-04  3.40e-04 -5.32e-02 -0.067745 1.013 9.19e-04
505  2.27e-02 -3.28e-02  7.74e-04  7.74e-04 -7.49e-02 -0.098979 1.009 1.96e-03
506  4.57e-02 -6.61e-02  1.56e-03  1.56e-03 -1.09e-01 -0.151448 1.000 4.58e-03
507 -2.04e-04  2.94e-04 -6.94e-06 -6.94e-06 -3.34e-04 -0.000503 1.022 5.08e-08
508 -9.07e-03  1.31e-02 -3.10e-04 -3.10e-04 -1.82e-02 -0.025943 1.020 1.35e-04
509 -1.22e-02  1.77e-02 -4.18e-04 -4.18e-04 -3.18e-02 -0.042960 1.018 3.70e-04
510 -8.90e-03  1.29e-02 -3.04e-04 -3.04e-04 -3.28e-02 -0.042400 1.017 3.60e-04
511 -4.33e-03  6.26e-03 -1.48e-04 -1.48e-04 -2.75e-02 -0.034507 1.016 2.39e-04
512 -1.05e-03  1.52e-03 -3.58e-05 -3.58e-05 -2.43e-02 -0.030099 1.016 1.81e-04
513  1.66e-03 -2.40e-03  5.66e-05  5.66e-05 -2.32e-02 -0.028876 1.017 1.67e-04
514  8.79e-03 -1.27e-02  3.00e-04  3.00e-04 -4.70e-02 -0.059820 1.014 7.16e-04
515  2.14e-02 -3.10e-02  7.31e-04  7.31e-04 -7.07e-02 -0.093497 1.010 1.75e-03
516  2.14e-02 -3.09e-02  7.30e-04  7.30e-04 -5.10e-02 -0.070827 1.015 1.00e-03
517  2.12e-02 -3.07e-02  7.24e-04  7.24e-04 -3.95e-02 -0.058301 1.018 6.81e-04
518  3.55e-02 -5.14e-02  1.21e-03  1.21e-03 -5.96e-02 -0.090790 1.016 1.65e-03
519 -1.49e-03  2.16e-03 -5.09e-05 -5.09e-05 -2.45e-03 -0.003693 1.022 2.73e-06
520 -6.98e-03  1.01e-02 -2.38e-04 -2.38e-04 -1.40e-02 -0.019950 1.020 7.97e-05
521 -1.14e-02  1.65e-02 -3.90e-04 -3.90e-04 -2.97e-02 -0.040125 1.018 3.22e-04
522 -6.62e-03  9.57e-03 -2.26e-04 -2.26e-04 -2.44e-02 -0.031568 1.017 2.00e-04
523 -3.34e-03  4.83e-03 -1.14e-04 -1.14e-04 -2.12e-02 -0.026640 1.017 1.42e-04
524 -7.80e-04  1.13e-03 -2.66e-05 -2.66e-05 -1.80e-02 -0.022356 1.017 1.00e-04
525  3.22e-04 -4.65e-04  1.10e-05  1.10e-05 -4.49e-03 -0.005596 1.017 6.27e-06
526  2.97e-03 -4.29e-03  1.01e-04  1.01e-04 -1.59e-02 -0.020224 1.017 8.19e-05
527  5.12e-03 -7.40e-03  1.75e-04  1.75e-04 -1.69e-02 -0.022329 1.018 9.99e-05
528 -9.82e-03  1.42e-02 -3.35e-04 -3.35e-04  2.34e-02  0.032531 1.019 2.12e-04
529 -1.64e-02  2.37e-02 -5.60e-04 -5.60e-04  3.06e-02  0.045110 1.019 4.08e-04
530 -1.61e-02  2.32e-02 -5.48e-04 -5.48e-04  2.70e-02  0.041052 1.020 3.38e-04
531 -2.04e-04  2.94e-04 -6.94e-06 -6.94e-06 -3.34e-04 -0.000503 1.022 5.08e-08
532 -5.93e-03  8.57e-03 -2.02e-04 -2.02e-04 -1.19e-02 -0.016954 1.020 5.76e-05
533 -8.21e-03  1.19e-02 -2.80e-04 -2.80e-04 -2.13e-02 -0.028788 1.018 1.66e-04
534 -8.33e-03  1.20e-02 -2.84e-04 -2.84e-04 -3.07e-02 -0.039692 1.017 3.16e-04
535 -3.67e-03  5.31e-03 -1.25e-04 -1.25e-04 -2.33e-02 -0.029262 1.017 1.72e-04
536 -5.10e-04  7.37e-04 -1.74e-05 -1.74e-05 -1.18e-02 -0.014613 1.017 4.28e-05
537 -2.73e-04  3.95e-04 -9.32e-06 -9.32e-06  3.81e-03  0.004749 1.017 4.52e-06
538  2.58e-03 -3.73e-03  8.81e-05  8.81e-05 -1.38e-02 -0.017585 1.017 6.20e-05
539  8.25e-03 -1.19e-02  2.82e-04  2.82e-04 -2.72e-02 -0.036004 1.017 2.60e-04
540  1.18e-02 -1.71e-02  4.04e-04  4.04e-04 -2.82e-02 -0.039231 1.018 3.08e-04
541  6.83e-03 -9.87e-03  2.33e-04  2.33e-04 -1.27e-02 -0.018756 1.020 7.05e-05
542  2.45e-02 -3.54e-02  8.35e-04  8.35e-04 -4.11e-02 -0.062515 1.019 7.83e-04
543 -2.78e-03  4.02e-03 -9.49e-05 -9.49e-05 -4.57e-03 -0.006883 1.022 9.49e-06
544 -9.07e-03  1.31e-02 -3.10e-04 -3.10e-04 -1.82e-02 -0.025943 1.020 1.35e-04
545 -1.14e-02  1.65e-02 -3.90e-04 -3.90e-04 -2.97e-02 -0.040125 1.018 3.22e-04
546 -7.76e-03  1.12e-02 -2.65e-04 -2.65e-04 -2.86e-02 -0.036984 1.017 2.74e-04
547 -2.35e-03  3.40e-03 -8.04e-05 -8.04e-05 -1.49e-02 -0.018775 1.017 7.06e-05
548 -3.30e-04  4.77e-04 -1.13e-05 -1.13e-05 -7.62e-03 -0.009452 1.017 1.79e-05
549 -1.17e-03  1.68e-03 -3.98e-05 -3.98e-05  1.63e-02  0.020268 1.017 8.23e-05
550 -2.46e-03  3.55e-03 -8.38e-05 -8.38e-05  1.31e-02  0.016716 1.018 5.60e-05
551 -2.56e-02  3.70e-02 -8.74e-04 -8.74e-04  8.45e-02  0.111784 1.007 2.50e-03
552 -5.42e-02  7.83e-02 -1.85e-03 -1.85e-03  1.29e-01  0.179382 0.993 6.41e-03
553 -9.66e-02  1.40e-01 -3.30e-03 -3.30e-03  1.80e-01  0.265413 0.969 1.40e-02
554 -1.20e-01  1.73e-01 -4.09e-03 -4.09e-03  2.01e-01  0.306597 0.958 1.86e-02
555 -1.49e-03  2.16e-03 -5.09e-05 -5.09e-05 -2.45e-03 -0.003693 1.022 2.73e-06
556 -5.93e-03  8.57e-03 -2.02e-04 -2.02e-04 -1.19e-02 -0.016954 1.020 5.76e-05
557 -9.82e-03  1.42e-02 -3.35e-04 -3.35e-04 -2.55e-02 -0.034457 1.018 2.38e-04
558 -4.92e-03  7.11e-03 -1.68e-04 -1.68e-04 -1.81e-02 -0.023445 1.018 1.10e-04
559 -1.04e-03  1.50e-03 -3.55e-05 -3.55e-05 -6.60e-03 -0.008288 1.018 1.38e-05
560 -5.97e-05  8.62e-05 -2.04e-06 -2.04e-06 -1.38e-03 -0.001710 1.017 5.86e-07
561 -8.68e-04  1.25e-03 -2.96e-05 -2.96e-05  1.21e-02  0.015095 1.017 4.56e-05
562 -9.05e-04  1.31e-03 -3.09e-05 -3.09e-05  4.84e-03  0.006161 1.018 7.61e-06
563 -1.78e-03  2.57e-03 -6.06e-05 -6.06e-05  5.86e-03  0.007750 1.018 1.20e-05
564 -3.75e-03  5.43e-03 -1.28e-04 -1.28e-04  8.95e-03  0.012437 1.019 3.10e-05
565 -1.86e-02  2.69e-02 -6.36e-04 -6.36e-04  3.47e-02  0.051195 1.019 5.25e-04
566 -1.48e-02  2.14e-02 -5.06e-04 -5.06e-04  2.49e-02  0.037914 1.020 2.88e-04
567 -2.04e-04  2.94e-04 -6.94e-06 -6.94e-06 -3.34e-04 -0.000503 1.022 5.08e-08
568 -4.88e-03  7.06e-03 -1.67e-04 -1.67e-04 -9.82e-03 -0.013958 1.020 3.90e-05
569 -7.40e-03  1.07e-02 -2.52e-04 -2.52e-04 -1.92e-02 -0.025955 1.019 1.35e-04
570 -5.49e-03  7.93e-03 -1.87e-04 -1.87e-04 -2.02e-02 -0.026153 1.018 1.37e-04
571 -2.03e-03  2.93e-03 -6.91e-05 -6.91e-05 -1.29e-02 -0.016153 1.017 5.23e-05
572 -6.00e-04  8.67e-04 -2.05e-05 -2.05e-05 -1.39e-02 -0.017194 1.017 5.92e-05
573 -1.31e-03  1.90e-03 -4.48e-05 -4.48e-05  1.83e-02  0.022855 1.017 1.05e-04
574 -4.39e-03  6.35e-03 -1.50e-04 -1.50e-04  2.35e-02  0.029910 1.017 1.79e-04
575 -1.49e-02  2.16e-02 -5.10e-04 -5.10e-04  4.93e-02  0.065197 1.015 8.51e-04
576 -3.06e-02  4.43e-02 -1.05e-03 -1.05e-03  7.30e-02  0.101499 1.011 2.06e-03
577 -5.30e-02  7.67e-02 -1.81e-03 -1.81e-03  9.88e-02  0.145679 1.005 4.24e-03
578 -4.80e-02  6.94e-02 -1.64e-03 -1.64e-03  8.06e-02  0.122758 1.011 3.01e-03
        hat inf
1   0.00872    
2   0.00728    
3   0.00614    
4   0.00531    
5   0.00478    
6   0.00455    
7   0.00463    
8   0.00502    
9   0.00570    
10  0.00669    
11  0.00799    
12  0.00875    
13  0.00872    
14  0.00728    
15  0.00614    
16  0.00531    
17  0.00478    
18  0.00455    
19  0.00463    
20  0.00502    
21  0.00570    
22  0.00669    
23  0.00799    
24  0.00875    
25  0.00872    
26  0.00728    
27  0.00614    
28  0.00531    
29  0.00478    
30  0.00455    
31  0.00463    
32  0.00502    
33  0.00570    
34  0.00669    
35  0.00799    
36  0.00875    
37  0.00872    
38  0.00728    
39  0.00614    
40  0.00531    
41  0.00478    
42  0.00455    
43  0.00463    
44  0.00502    
45  0.00570    
46  0.00669    
47  0.00799    
48  0.00875    
49  0.00872    
50  0.00728    
51  0.00614    
52  0.00531    
53  0.00478    
54  0.00455    
55  0.00463    
56  0.00502    
57  0.00570    
58  0.00669    
59  0.00799    
60  0.00875    
61  0.00872    
62  0.00728    
63  0.00614    
64  0.00531    
65  0.00478    
66  0.00455    
67  0.00463    
68  0.00502    
69  0.00570    
70  0.00669    
71  0.00799    
72  0.00875    
73  0.00872    
74  0.00728    
75  0.00614    
76  0.00531    
77  0.00478    
78  0.00455    
79  0.00463    
80  0.00502    
81  0.00570    
82  0.00669   *
83  0.00799   *
84  0.00875   *
85  0.00872    
86  0.00728    
87  0.00614    
88  0.00531    
89  0.00478    
90  0.00455    
91  0.00463    
92  0.00502    
93  0.00570    
94  0.00669    
95  0.00799    
96  0.00872    
97  0.00728    
98  0.00614    
99  0.00531    
100 0.00478    
101 0.00455    
102 0.00463    
103 0.00502    
104 0.00570    
105 0.00669    
106 0.00799   *
107 0.00875   *
108 0.00872    
109 0.00728    
110 0.00614    
111 0.00531    
112 0.00478    
113 0.00455    
114 0.00463    
115 0.00502    
116 0.00570    
117 0.00669    
118 0.00799    
119 0.00875    
120 0.00872    
121 0.00728    
122 0.00614    
123 0.00531    
124 0.00478    
125 0.00455    
126 0.00463    
127 0.00502    
128 0.00570    
129 0.00669    
130 0.00799    
131 0.00875    
132 0.00872    
133 0.00728    
134 0.00614    
135 0.00531    
136 0.00478    
137 0.00455    
138 0.00463    
139 0.00502    
140 0.00570    
141 0.00669    
142 0.00799    
143 0.00875    
144 0.00872    
145 0.00728    
146 0.00614    
147 0.00531    
148 0.00478    
149 0.00455    
150 0.00463    
151 0.00502    
152 0.00570   *
153 0.00669   *
154 0.00799   *
155 0.00875   *
156 0.00872    
157 0.00728    
158 0.00614    
159 0.00531    
160 0.00478    
161 0.00455    
162 0.00463    
163 0.00502    
164 0.00570    
165 0.00669   *
166 0.00799    
167 0.00875    
168 0.00872    
169 0.00728    
170 0.00614    
171 0.00531    
172 0.00478    
173 0.00455    
174 0.00463    
175 0.00502    
176 0.00872    
177 0.00728    
178 0.00614    
179 0.00531    
180 0.00478    
181 0.00455    
182 0.00463    
183 0.00872    
184 0.00728    
185 0.00614    
186 0.00531    
187 0.00478    
188 0.00455    
189 0.00463    
190 0.00502    
191 0.00570    
192 0.00669    
193 0.00799    
194 0.00875    
195 0.00872    
196 0.00728    
197 0.00872    
198 0.00728    
199 0.00614    
200 0.00531    
201 0.00478    
202 0.00455    
203 0.00463    
204 0.00502    
205 0.00570    
206 0.00669    
207 0.00799    
208 0.00875    
209 0.00872    
210 0.00728    
211 0.00614    
212 0.00531    
213 0.00478    
214 0.00455    
215 0.00463    
216 0.00502    
217 0.00570    
218 0.00669    
219 0.00799    
220 0.00875    
221 0.01286    
222 0.01135    
223 0.01015    
224 0.00925    
225 0.00866    
226 0.00837    
227 0.00838   *
228 0.00869   *
229 0.00931   *
230 0.01024   *
231 0.01147   *
232 0.01219   *
233 0.01286    
234 0.01135    
235 0.01015    
236 0.00925    
237 0.00866    
238 0.00837    
239 0.00838    
240 0.00869    
241 0.00931    
242 0.01024    
243 0.01147    
244 0.01219    
245 0.01286    
246 0.01135    
247 0.01015    
248 0.00925    
249 0.00866    
250 0.00837    
251 0.00838    
252 0.00869    
253 0.00931    
254 0.01024    
255 0.01147    
256 0.01219    
257 0.01286    
258 0.01135    
259 0.01015    
260 0.00925    
261 0.00866    
262 0.00837    
263 0.00838    
264 0.00869    
265 0.00931   *
266 0.01024   *
267 0.01147   *
268 0.01219   *
269 0.01286    
270 0.01135    
271 0.01015    
272 0.00925    
273 0.00866    
274 0.00837    
275 0.00838    
276 0.00869    
277 0.00931    
278 0.01024    
279 0.01147    
280 0.01219    
281 0.01286    
282 0.01135    
283 0.01015    
284 0.00925    
285 0.00866    
286 0.00837    
287 0.00838    
288 0.00869    
289 0.00931    
290 0.01024    
291 0.01147    
292 0.01219    
293 0.01286    
294 0.01135    
295 0.01015    
296 0.00925    
297 0.00866    
298 0.00837    
299 0.00838    
300 0.00869    
301 0.00931    
302 0.01024    
303 0.01147    
304 0.01219    
305 0.01286    
306 0.01135    
307 0.01015    
308 0.00925    
309 0.00866    
310 0.00837    
311 0.00838    
312 0.00869    
313 0.00931    
314 0.01024    
315 0.01147    
316 0.01219    
317 0.01286    
318 0.01135    
319 0.01015    
320 0.00925    
321 0.00866    
322 0.00837    
323 0.00838    
324 0.00869    
325 0.00931    
326 0.01024    
327 0.01147    
328 0.01219   *
329 0.01286    
330 0.01135    
331 0.01015    
332 0.00925    
333 0.00866    
334 0.00837    
335 0.00838    
336 0.00869    
337 0.00931    
338 0.01024    
339 0.01147    
340 0.01219    
341 0.01286    
342 0.01135    
343 0.01015    
344 0.00925    
345 0.00866    
346 0.00837    
347 0.00838    
348 0.00869    
349 0.00931    
350 0.01024    
351 0.01147    
352 0.01219    
353 0.01286    
354 0.01135    
355 0.01015    
356 0.00925    
357 0.00866    
358 0.00837    
359 0.00838    
360 0.00869    
361 0.00931    
362 0.01024    
363 0.01147    
364 0.01219    
365 0.01286    
366 0.01135    
367 0.01015    
368 0.00925    
369 0.00866    
370 0.00837    
371 0.00838    
372 0.00869    
373 0.00931    
374 0.01024    
375 0.01147    
376 0.01219   *
377 0.01286    
378 0.01135    
379 0.01015    
380 0.00925    
381 0.00866    
382 0.00837    
383 0.00838    
384 0.00869    
385 0.00931    
386 0.01024   *
387 0.01147   *
388 0.01219   *
389 0.01286    
390 0.01135    
391 0.01015    
392 0.00925    
393 0.00866    
394 0.00837    
395 0.00838    
396 0.00869    
397 0.00931   *
398 0.01024   *
399 0.01147   *
400 0.01219   *
401 0.01286    
402 0.01135    
403 0.01015    
404 0.00925    
405 0.00866    
406 0.00837    
407 0.00838    
408 0.00869    
409 0.00931    
410 0.01024    
411 0.01147    
412 0.01219    
413 0.01286    
414 0.01135    
415 0.01015    
416 0.00925    
417 0.00866    
418 0.00837    
419 0.00838    
420 0.00869    
421 0.00931    
422 0.01024    
423 0.01147    
424 0.01219    
425 0.01286    
426 0.01135    
427 0.01015    
428 0.00925    
429 0.00866    
430 0.00837    
431 0.00838    
432 0.00869    
433 0.00931    
434 0.01024    
435 0.01147    
436 0.01219    
437 0.01286    
438 0.01135    
439 0.01015    
440 0.00925    
441 0.00866    
442 0.00837    
443 0.00838    
444 0.00869    
445 0.00931    
446 0.01024    
447 0.01147    
448 0.01219    
449 0.01286    
450 0.01135    
451 0.01015    
452 0.00925    
453 0.00866    
454 0.00837    
455 0.00838    
456 0.00869    
457 0.00931    
458 0.01024    
459 0.01147    
460 0.01219   *
461 0.01287    
462 0.01138    
463 0.01021    
464 0.00933    
465 0.00876    
466 0.00850    
467 0.00853    
468 0.00887    
469 0.00952    
470 0.01047    
471 0.01172    
472 0.01246    
473 0.01287    
474 0.01138    
475 0.01021    
476 0.00933    
477 0.00876    
478 0.00850    
479 0.00853    
480 0.00887    
481 0.00952    
482 0.01047    
483 0.01172    
484 0.01246    
485 0.01287    
486 0.01138    
487 0.01021    
488 0.00933    
489 0.00876    
490 0.00850    
491 0.00853    
492 0.00887    
493 0.00952    
494 0.01047    
495 0.01172    
496 0.01246    
497 0.01287    
498 0.01138    
499 0.01021    
500 0.00933    
501 0.00876    
502 0.00850    
503 0.00853    
504 0.00887    
505 0.00952    
506 0.01047    
507 0.01287    
508 0.01138    
509 0.01021    
510 0.00933    
511 0.00876    
512 0.00850    
513 0.00853    
514 0.00887    
515 0.00952    
516 0.01047    
517 0.01172    
518 0.01246    
519 0.01287    
520 0.01138    
521 0.01021    
522 0.00933    
523 0.00876    
524 0.00850    
525 0.00853    
526 0.00887    
527 0.00952    
528 0.01047    
529 0.01172    
530 0.01246    
531 0.01287    
532 0.01138    
533 0.01021    
534 0.00933    
535 0.00876    
536 0.00850    
537 0.00853    
538 0.00887    
539 0.00952    
540 0.01047    
541 0.01172    
542 0.01246    
543 0.01287    
544 0.01138    
545 0.01021    
546 0.00933    
547 0.00876    
548 0.00850    
549 0.00853    
550 0.00887    
551 0.00952    
552 0.01047    
553 0.01172   *
554 0.01246   *
555 0.01287    
556 0.01138    
557 0.01021    
558 0.00933    
559 0.00876    
560 0.00850    
561 0.00853    
562 0.00887    
563 0.00952    
564 0.01047    
565 0.01172    
566 0.01246    
567 0.01287    
568 0.01138    
569 0.01021    
570 0.00933    
571 0.00876    
572 0.00850    
573 0.00853    
574 0.00887    
575 0.00952    
576 0.01047    
577 0.01172    
578 0.01246    
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#综合到一个图上}
\FunctionTok{influencePlot}\NormalTok{(model3,}\AttributeTok{main=}\StringTok{"异常点识别"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第四章-残差分析及异常值分析_files/figure-pdf/unnamed-chunk-4-1.pdf}}

\begin{verbatim}
        StudRes        Hat        CookD
268 -3.87192825 0.01219439 3.613230e-02
399  3.92028110 0.01146653 3.478156e-02
400  4.01629595 0.01219439 3.880169e-02
461  0.02352886 0.01286652 1.445690e-06
473  0.02352886 0.01286652 1.445690e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#纵坐标超过2或者{-}2可以认为是离群点，水平轴超过0.2或者0.3认为是高杠杆值。圆圈大小与影响成正比，圆圈很大的可以认为是强影响点  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(performance)}
\FunctionTok{check\_model}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

全局统计量：粗略估计结果变量和预测变量是否符合线性关系，结果是不符合\\
偏度和峰度：检验残差分布，结果是符合\\
连接函数：检测结果变量是连续型还是二分类，结果是不符合\\
异方差：检验方差齐性，结果是符合

\part{违反基本假定}

\chapter{Hello, Quarto}\label{hello-quarto-2}

\chapter{5
异方差和加权最小二乘}\label{ux5f02ux65b9ux5deeux548cux52a0ux6743ux6700ux5c0fux4e8cux4e58}

\section{5.1
建立简单线性回归模型}\label{ux5efaux7acbux7b80ux5355ux7ebfux6027ux56deux5f52ux6a21ux578b}

读取数据

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data5}\FloatTok{.0}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/应用回归分析{-}{-}基于R和Python/data/li3.2 北京开发区.sav"}\NormalTok{)}
\CommentTok{\#View(data5.0)}
\NormalTok{data5}\OtherTok{=}\NormalTok{data5}\FloatTok{.0}\NormalTok{[}\SpecialCharTok{{-}}\DecValTok{16}\NormalTok{,]}\CommentTok{\# 删除第16行的空行}
\CommentTok{\#View(data5)}
\FunctionTok{str}\NormalTok{(data5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [15 x 3] (S3: tbl_df/tbl/data.frame)
 $ x1: num [1:15] 25 20 6 1001 525 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ x2: num [1:15] 3548 896 750 2087 1639 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ y : num [1:15] 554 208.6 3.1 2815.4 1052.1 ...
  ..- attr(*, "format.spss")= chr "F8.2"
\end{verbatim}

建立模型

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm5}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{data5)}
\CommentTok{\#e=lm4.1$residuals}
\NormalTok{e}\OtherTok{=}\FunctionTok{resid}\NormalTok{(lm5)}
\end{Highlighting}
\end{Shaded}

\section{5.2 图示检验}\label{ux56feux793aux68c0ux9a8c}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#异方差检验 }
\FunctionTok{ncvTest}\NormalTok{(lm5) }\CommentTok{\#P值\textgreater{}0.05 则接受原假设，即不存在明显的异方差现象。 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 4.121994, Df = 1, p = 0.042329
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{spreadLevelPlot}\NormalTok{(lm5) }\CommentTok{\#分布水平图，检测异方差性}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in spreadLevelPlot.lm(lm5): 
1 negative fitted value removed
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第五章-异方差_files/figure-pdf/unnamed-chunk-3-1.pdf}}

\begin{verbatim}

Suggested power transformation:  0.6415664 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{( }\FunctionTok{fitted}\NormalTok{ (lm5), }\FunctionTok{resid}\NormalTok{ (lm5), }\AttributeTok{xlab=}\StringTok{\textquotesingle{} Fitted Values拟合值 \textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{} Residuals 残差值\textquotesingle{}}\NormalTok{) }\CommentTok{\#拟合值和残差的散点图}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第五章-异方差_files/figure-pdf/unnamed-chunk-3-2.pdf}}

\section{5.3 量化检验}\label{ux91cfux5316ux68c0ux9a8c}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{ (lmtest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: zoo
\end{verbatim}

\begin{verbatim}

Attaching package: 'zoo'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    as.Date, as.Date.numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bptest}\NormalTok{(lm5) }\CommentTok{\#Breusch{-}Pagan test 原假设：同方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  lm5
BP = 13.563, df = 2, p-value = 0.001135
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gqtest}\NormalTok{(lm5) }\CommentTok{\#Goldfeld{-}Quandt检验 原假设：同方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Goldfeld-Quandt test

data:  lm5
GQ = 0.22666, df1 = 5, df2 = 4, p-value = 0.9325
alternative hypothesis: variance increases from segment 1 to 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bptest}\NormalTok{(lm5,}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{*}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{x1}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}\AttributeTok{data=}\NormalTok{data5) }\CommentTok{\# White检验（在BP检验里面添加二次交叉项和平方项）}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  lm5
BP = 14.023, df = 3, p-value = 0.002874
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#leveneTest(lm5)}
\end{Highlighting}
\end{Shaded}

glejser检验，残差项对自变量做回归

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.g1}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e)}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2,}\AttributeTok{data=}\NormalTok{data5)}
\NormalTok{lm.g2}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(e)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{sqrt}\NormalTok{(x1)}\SpecialCharTok{+}\FunctionTok{sqrt}\NormalTok{(x2),}\AttributeTok{data=}\NormalTok{data5)}
\FunctionTok{summary}\NormalTok{(lm.g1)}\SpecialCharTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               Estimate  Std. Error   t value     Pr(>|t|)
(Intercept) -66.7376381 51.01608185 -1.308169 2.153200e-01
x1           -0.2723965  0.10249261 -2.657719 2.087922e-02
x2            0.2721707  0.02884981  9.434056 6.695768e-07
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.g2)}\SpecialCharTok{$}\NormalTok{coef }\CommentTok{\#若beta显著不为零，说明残差项和某个变量相关，说明存在异方差性}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               Estimate Std. Error   t value     Pr(>|t|)
(Intercept) -452.246509 101.179560 -4.469742 7.660043e-04
sqrt(x1)      -8.993862   4.069851 -2.209875 4.729104e-02
sqrt(x2)      22.928075   2.883416  7.951704 4.000141e-06
\end{verbatim}

\section{5.4 可行加权最小二乘法
FWLS}\label{ux53efux884cux52a0ux6743ux6700ux5c0fux4e8cux4e58ux6cd5-fwls}

残差对拟合值回归，使用回归后的残差拟合值的倒数作为权重，abs表示取绝对值

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{lm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(lm5}\SpecialCharTok{$}\NormalTok{residuals) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lm5}\SpecialCharTok{$}\NormalTok{fitted.values)}\SpecialCharTok{$}\NormalTok{fitted.values}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{wls\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ data5, }\AttributeTok{weights =}\NormalTok{ wt)}
\FunctionTok{bptest}\NormalTok{(wls\_model) }\CommentTok{\#原假设：同方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  wls_model
BP = 7.5723e-05, df = 2, p-value = 1
\end{verbatim}

残差对自变量回归，使用回归后的残差拟合值的倒数作为权重

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wt2 }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{lm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(lm5}\SpecialCharTok{$}\NormalTok{residuals) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1}\SpecialCharTok{+}\NormalTok{x2,}\AttributeTok{data =}\NormalTok{ data5)}\SpecialCharTok{$}\NormalTok{fitted.values}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{wls\_model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ data5, }\AttributeTok{weights =}\NormalTok{ wt)}
\FunctionTok{bptest}\NormalTok{(wls\_model2) }\CommentTok{\#原假设：同方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  wls_model2
BP = 7.5723e-05, df = 2, p-value = 1
\end{verbatim}

\subsection{5.5
稳健标准误回归}\label{ux7a33ux5065ux6807ux51c6ux8befux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#首先建立普通线性回归}
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data4}\FloatTok{.0}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/应用回归分析{-}{-}基于R和Python/data/li3.2 北京开发区.sav"}\NormalTok{)}
\NormalTok{data4}\OtherTok{=}\NormalTok{data4}\FloatTok{.0}\NormalTok{[}\SpecialCharTok{{-}}\DecValTok{16}\NormalTok{,]}\CommentTok{\# 删除第16行的空行}
\NormalTok{lm4}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{data4)}

\CommentTok{\# 使用稳健标准误}
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}
\CommentTok{\# HC0: 基本怀特估计量}
\CommentTok{\#coeftest(lm4, vcov = vcovHC(lm4, type = "HC0"))}
\CommentTok{\# HC1: 小样本调整 (默认)}
\CommentTok{\#coeftest(lm4, vcov = vcovHC(lm4, type = "HC1"))}
\CommentTok{\# HC2: 改进的小样本调整}
\FunctionTok{coeftest}\NormalTok{(lm4, }\AttributeTok{vcov =} \FunctionTok{vcovHC}\NormalTok{(lm4, }\AttributeTok{type =} \StringTok{"HC2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

t test of coefficients:

              Estimate Std. Error t value Pr(>|t|)   
(Intercept) -327.03947  175.05689 -1.8682 0.086337 . 
x1             2.03601    0.65978  3.0859 0.009434 **
x2             0.46839    0.25539  1.8340 0.091551 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# HC3: 推荐用于小样本}
\CommentTok{\#coeftest(lm4, vcov = vcovHC(lm4, type = "HC3"))}
\CommentTok{\# HC4: 更保守的估计}
\CommentTok{\#coeftest(lm4, vcov = vcovHC(lm4, type = "HC4"))}
\end{Highlighting}
\end{Shaded}

\chapter{Hello, Quarto}\label{hello-quarto-3}

\chapter{6
自相关和迭代法}\label{ux81eaux76f8ux5173ux548cux8fedux4ee3ux6cd5}

\section{6.1
建立简单相关模型}\label{ux5efaux7acbux7b80ux5355ux76f8ux5173ux6a21ux578b}

读取数据

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data6}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li2.2.sav"}\NormalTok{)}
\CommentTok{\#View(data6)}
\FunctionTok{str}\NormalTok{(data6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [23 x 3] (S3: tbl_df/tbl/data.frame)
 $ 年份: num [1:23] 1990 1991 1992 1993 1994 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ y   : num [1:23] 1279 1454 1672 2111 2851 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ x   : num [1:23] 1510 1701 2027 2577 3496 ...
  ..- attr(*, "format.spss")= chr "F8.2"
\end{verbatim}

建立回归模型

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model6}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{data6)}
\FunctionTok{summary}\NormalTok{(model6)}\CommentTok{\#(1)建立回归方程}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = data6)

Residuals:
    Min      1Q  Median      3Q     Max 
-471.35 -120.86   65.89  134.58  269.99 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.092e+02  7.584e+01   8.033 7.71e-08 ***
x           6.732e-01  6.762e-03  99.554  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 211.1 on 21 degrees of freedom
Multiple R-squared:  0.9979,    Adjusted R-squared:  0.9978 
F-statistic:  9911 on 1 and 21 DF,  p-value: < 2.2e-16
\end{verbatim}

\section{6.2 图示检验}\label{ux56feux793aux68c0ux9a8c-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model6.res}\OtherTok{=}\FunctionTok{resid}\NormalTok{(model6)}
\CommentTok{\#自相关性}
\CommentTok{\#图形诊断自相关性}
\NormalTok{model6.res1}\OtherTok{\textless{}{-}}\NormalTok{model6.res[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(model6.res)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{model6.res2}\OtherTok{\textless{}{-}}\NormalTok{model6.res[}\DecValTok{2}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(model6.res)]}
\FunctionTok{plot}\NormalTok{(model6.res1,model6.res2)}\CommentTok{\#e(t)与e(t{-}1)作图}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第六章-自相关_files/figure-pdf/unnamed-chunk-3-1.pdf}}

\section{6.3 量化检验}\label{ux91cfux5316ux68c0ux9a8c-1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#自相关DW检验}
\FunctionTok{durbinWatsonTest}\NormalTok{(model6) }\CommentTok{\#P值\textgreater{}0.05 拒绝原假设，即不存在自相关现象}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 lag Autocorrelation D-W Statistic p-value
   1       0.6753743     0.2831283       0
 Alternative hypothesis: rho != 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmtest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: zoo
\end{verbatim}

\begin{verbatim}

Attaching package: 'zoo'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    as.Date, as.Date.numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dwtest}\NormalTok{(model6) }\CommentTok{\# Durbin{-}Watson Test 原假设：不存在一阶自相关}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Durbin-Watson test

data:  model6
DW = 0.28313, p-value = 7.712e-10
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bgtest}\NormalTok{(model6) }\CommentTok{\# Breusch{-}Godfrey Test:拉格朗日乘数检验 原假设：自相关}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Breusch-Godfrey test for serial correlation of order up to 1

data:  model6
LM test = 15.498, df = 1, p-value = 8.261e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e6}\OtherTok{=}\FunctionTok{resid}\NormalTok{(model6)}
\FunctionTok{Box.test}\NormalTok{(e6) }\CommentTok{\# Q检验:Box{-}{-}Pierce or Ljung{-}{-}Box test 该命令在包stats}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Box-Pierce test

data:  e6
X-squared = 10.491, df = 1, p-value = 0.0012
\end{verbatim}

\subsection{6.4
Cochrane-Orcutt（CO）估计法和Prais-Winsten（Pw）估计法}\label{cochrane-orcuttcoux4f30ux8ba1ux6cd5ux548cprais-winstenpwux4f30ux8ba1ux6cd5}

包orcutt安装失败，就注释掉了，和4.4.1中的迭代法一致
CO估计比PW估计少了第一个估计方程所以不是BLUE

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cochrane{-}Orcutt估计}
\CommentTok{\#install.packages(orcutt)}
\CommentTok{\#library(orcutt)}
\CommentTok{\#co\_model \textless{}{-} cochrane.orcutt(lm4.3)}
\CommentTok{\#summary(co\_model)}
\CommentTok{\#co\_model\# 查看详细的迭代过程}
\end{Highlighting}
\end{Shaded}

Prais-Winsten（Pw）估计
相比CO在第一个方程两边同乘以（1-rho\^{}2）的开方，以保证同方差，估计是BLUE

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data6}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li2.2.sav"}\NormalTok{)}
\CommentTok{\#View(data6)}
\FunctionTok{str}\NormalTok{(data6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [23 x 3] (S3: tbl_df/tbl/data.frame)
 $ 年份: num [1:23] 1990 1991 1992 1993 1994 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ y   : num [1:23] 1279 1454 1672 2111 2851 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ x   : num [1:23] 1510 1701 2027 2577 3496 ...
  ..- attr(*, "format.spss")= chr "F8.2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prais{-}Winsten估计}
\CommentTok{\#install.packages(prais)}
\FunctionTok{library}\NormalTok{(prais)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: sandwich
\end{verbatim}

\begin{verbatim}
Loading required package: pcse
\end{verbatim}

\begin{verbatim}

Attaching package: 'pcse'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:sandwich':

    vcovPC
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pw\_model }\OtherTok{\textless{}{-}} \FunctionTok{prais\_winsten}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{data6,}\AttributeTok{index =} \StringTok{"年份"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Iteration 0: rho = 0
Iteration 1: rho = 0.8857
Iteration 2: rho = 0.8964
Iteration 3: rho = 0.9014
Iteration 4: rho = 0.9038
Iteration 5: rho = 0.905
Iteration 6: rho = 0.9056
Iteration 7: rho = 0.9059
Iteration 8: rho = 0.9061
Iteration 9: rho = 0.9062
Iteration 10: rho = 0.9062
Iteration 11: rho = 0.9062
Iteration 12: rho = 0.9063
Iteration 13: rho = 0.9063
Iteration 14: rho = 0.9063
Iteration 15: rho = 0.9063
Iteration 16: rho = 0.9063
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pw\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
prais_winsten(formula = y ~ x, data = data6, index = "年份")

Residuals:
    Min      1Q  Median      3Q     Max 
-372.61   53.02  177.25  354.61  447.29 

AR(1) coefficient rho after 16 iterations: 0.9063

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 669.01200  222.37873   3.008  0.00669 ** 
x             0.65058    0.01292  50.342  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 104.8 on 21 degrees of freedom
Multiple R-squared:  0.9837,    Adjusted R-squared:  0.9829 
F-statistic:  1264 on 1 and 21 DF,  p-value: < 2.2e-16

Durbin-Watson statistic (original): 0.2831 
Durbin-Watson statistic (transformed): 1.371
\end{verbatim}

\subsection{6.5 稳健回归
Newey-West估计法}\label{ux7a33ux5065ux56deux5f52-newey-westux4f30ux8ba1ux6cd5}

使用异方差自相关稳健标准误HAC，只改变标准误的估计值，不改变回归系数的估计值

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 使用HAC标准误}
\NormalTok{hac\_results }\OtherTok{\textless{}{-}} \FunctionTok{coeftest}\NormalTok{(model6, }\AttributeTok{vcov =} \FunctionTok{vcovHAC}\NormalTok{(model6))}
\FunctionTok{print}\NormalTok{(}\StringTok{"HAC标准误结果:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "HAC标准误结果:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(hac\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

t test of coefficients:

              Estimate Std. Error t value  Pr(>|t|)    
(Intercept) 609.218302 158.152050  3.8521 0.0009247 ***
x             0.673179   0.012831 52.4670 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}
\NormalTok{NeweyWest\_results }\OtherTok{\textless{}{-}} \FunctionTok{coeftest}\NormalTok{(model6, }\AttributeTok{vcov =}\NormalTok{ NeweyWest)}
\FunctionTok{print}\NormalTok{(}\StringTok{"NeweyWest标准误结果:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "NeweyWest标准误结果:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(NeweyWest\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

t test of coefficients:

              Estimate Std. Error t value  Pr(>|t|)    
(Intercept) 609.218302 180.350245   3.378  0.002841 ** 
x             0.673179   0.014551  46.262 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\chapter{Hello, Quarto}\label{hello-quarto-4}

\chapter{7
变量选择与模型构建}\label{ux53d8ux91cfux9009ux62e9ux4e0eux6a21ux578bux6784ux5efa}

\section{7.1 逐步回归}\label{ux9010ux6b65ux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ISLR)}
\NormalTok{fit.lm}\OtherTok{=}\FunctionTok{lm}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Hitters)}
\NormalTok{fit.step}\OtherTok{=}\FunctionTok{step}\NormalTok{(fit.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=3046.02
Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + Years + 
    CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League + 
    Division + PutOuts + Assists + Errors + NewLeague

            Df Sum of Sq      RSS    AIC
- CHmRun     1      1138 24201837 3044.0
- CHits      1      3930 24204629 3044.1
- Years      1      7869 24208569 3044.1
- NewLeague  1      9784 24210484 3044.1
- RBI        1     16076 24216776 3044.2
- HmRun      1     48572 24249272 3044.6
- Errors     1     58324 24259023 3044.7
- League     1     62121 24262821 3044.7
- Runs       1     63291 24263990 3044.7
- CRBI       1    135439 24336138 3045.5
- CAtBat     1    159864 24360564 3045.8
<none>                   24200700 3046.0
- Assists    1    280263 24480963 3047.1
- CRuns      1    374007 24574707 3048.1
- CWalks     1    609408 24810108 3050.6
- Division   1    834491 25035190 3052.9
- AtBat      1    971288 25171987 3054.4
- Hits       1    991242 25191941 3054.6
- Walks      1   1156606 25357305 3056.3
- PutOuts    1   1319628 25520328 3058.0

Step:  AIC=3044.03
Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + Years + 
    CAtBat + CHits + CRuns + CRBI + CWalks + League + Division + 
    PutOuts + Assists + Errors + NewLeague

            Df Sum of Sq      RSS    AIC
- Years      1      7609 24209447 3042.1
- NewLeague  1     10268 24212106 3042.2
- CHits      1     14003 24215840 3042.2
- RBI        1     14955 24216793 3042.2
- HmRun      1     52777 24254614 3042.6
- Errors     1     59530 24261367 3042.7
- League     1     63407 24265244 3042.7
- Runs       1     64860 24266698 3042.7
- CAtBat     1    174992 24376830 3043.9
<none>                   24201837 3044.0
- Assists    1    285766 24487603 3045.1
- CRuns      1    611358 24813196 3048.6
- CWalks     1    645627 24847464 3049.0
- Division   1    834637 25036474 3050.9
- CRBI       1    864220 25066057 3051.3
- AtBat      1    970861 25172699 3052.4
- Hits       1   1025981 25227819 3052.9
- Walks      1   1167378 25369216 3054.4
- PutOuts    1   1325273 25527110 3056.1

Step:  AIC=3042.12
Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + CAtBat + 
    CHits + CRuns + CRBI + CWalks + League + Division + PutOuts + 
    Assists + Errors + NewLeague

            Df Sum of Sq      RSS    AIC
- NewLeague  1      9931 24219377 3040.2
- RBI        1     15989 24225436 3040.3
- CHits      1     18291 24227738 3040.3
- HmRun      1     54144 24263591 3040.7
- Errors     1     57312 24266759 3040.7
- Runs       1     63172 24272619 3040.8
- League     1     65732 24275178 3040.8
<none>                   24209447 3042.1
- CAtBat     1    266205 24475652 3043.0
- Assists    1    293479 24502926 3043.3
- CRuns      1    646350 24855797 3047.1
- CWalks     1    649269 24858716 3047.1
- Division   1    827511 25036958 3049.0
- CRBI       1    872121 25081568 3049.4
- AtBat      1    968713 25178160 3050.4
- Hits       1   1018379 25227825 3050.9
- Walks      1   1164536 25373983 3052.5
- PutOuts    1   1334525 25543972 3054.2

Step:  AIC=3040.22
Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + CAtBat + 
    CHits + CRuns + CRBI + CWalks + League + Division + PutOuts + 
    Assists + Errors

           Df Sum of Sq      RSS    AIC
- RBI       1     15800 24235177 3038.4
- CHits     1     15859 24235237 3038.4
- Errors    1     54505 24273883 3038.8
- HmRun     1     54938 24274316 3038.8
- Runs      1     62294 24281671 3038.9
- League    1    107479 24326856 3039.4
<none>                  24219377 3040.2
- CAtBat    1    261336 24480713 3041.1
- Assists   1    295536 24514914 3041.4
- CWalks    1    648860 24868237 3045.2
- CRuns     1    661449 24880826 3045.3
- Division  1    824672 25044049 3047.0
- CRBI      1    880429 25099806 3047.6
- AtBat     1    999057 25218434 3048.9
- Hits      1   1034463 25253840 3049.2
- Walks     1   1157205 25376583 3050.5
- PutOuts   1   1335173 25554550 3052.3

Step:  AIC=3038.4
Salary ~ AtBat + Hits + HmRun + Runs + Walks + CAtBat + CHits + 
    CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + 
    Errors

           Df Sum of Sq      RSS    AIC
- CHits     1     13483 24248660 3036.5
- HmRun     1     44586 24279763 3036.9
- Runs      1     54057 24289234 3037.0
- Errors    1     57656 24292833 3037.0
- League    1    108644 24343821 3037.6
<none>                  24235177 3038.4
- CAtBat    1    252756 24487934 3039.1
- Assists   1    294674 24529851 3039.6
- CWalks    1    639690 24874868 3043.2
- CRuns     1    693535 24928712 3043.8
- Division  1    808984 25044161 3045.0
- CRBI      1    893830 25129008 3045.9
- Hits      1   1034884 25270061 3047.4
- AtBat     1   1042798 25277975 3047.5
- Walks     1   1145013 25380191 3048.5
- PutOuts   1   1340713 25575890 3050.6

Step:  AIC=3036.54
Salary ~ AtBat + Hits + HmRun + Runs + Walks + CAtBat + CRuns + 
    CRBI + CWalks + League + Division + PutOuts + Assists + Errors

           Df Sum of Sq      RSS    AIC
- HmRun     1     40487 24289148 3035.0
- Errors    1     51930 24300590 3035.1
- Runs      1     79343 24328003 3035.4
- League    1    114742 24363402 3035.8
<none>                  24248660 3036.5
- Assists   1    283442 24532103 3037.6
- CAtBat    1    613356 24862016 3041.1
- Division  1    801474 25050134 3043.1
- CRBI      1    903248 25151908 3044.2
- CWalks    1   1011953 25260613 3045.3
- Walks     1   1246164 25494824 3047.7
- AtBat     1   1339620 25588280 3048.7
- CRuns     1   1390808 25639469 3049.2
- PutOuts   1   1406023 25654684 3049.4
- Hits      1   1607990 25856650 3051.4

Step:  AIC=3034.98
Salary ~ AtBat + Hits + Runs + Walks + CAtBat + CRuns + CRBI + 
    CWalks + League + Division + PutOuts + Assists + Errors

           Df Sum of Sq      RSS    AIC
- Errors    1     44085 24333232 3033.5
- Runs      1     49068 24338215 3033.5
- League    1    103837 24392985 3034.1
<none>                  24289148 3035.0
- Assists   1    247002 24536150 3035.6
- CAtBat    1    652746 24941894 3040.0
- Division  1    795643 25084791 3041.5
- CWalks    1    982896 25272044 3043.4
- Walks     1   1205823 25494971 3045.7
- AtBat     1   1300972 25590120 3046.7
- CRuns     1   1351200 25640348 3047.2
- CRBI      1   1353507 25642655 3047.2
- PutOuts   1   1429006 25718154 3048.0
- Hits      1   1574140 25863288 3049.5

Step:  AIC=3033.46
Salary ~ AtBat + Hits + Runs + Walks + CAtBat + CRuns + CRBI + 
    CWalks + League + Division + PutOuts + Assists

           Df Sum of Sq      RSS    AIC
- Runs      1     54113 24387345 3032.0
- League    1     91269 24424501 3032.4
<none>                  24333232 3033.5
- Assists   1    220010 24553242 3033.8
- CAtBat    1    650513 24983746 3038.4
- Division  1    799455 25132687 3040.0
- CWalks    1    971260 25304493 3041.8
- Walks     1   1239533 25572765 3044.5
- CRBI      1   1331672 25664904 3045.5
- CRuns     1   1361070 25694302 3045.8
- AtBat     1   1378592 25711824 3045.9
- PutOuts   1   1391660 25724892 3046.1
- Hits      1   1649291 25982523 3048.7

Step:  AIC=3032.04
Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + 
    League + Division + PutOuts + Assists

           Df Sum of Sq      RSS    AIC
- League    1    113056 24500402 3031.3
<none>                  24387345 3032.0
- Assists   1    280689 24668034 3033.1
- CAtBat    1    596622 24983967 3036.4
- Division  1    780369 25167714 3038.3
- CWalks    1    946687 25334032 3040.1
- Walks     1   1212997 25600342 3042.8
- CRuns     1   1334397 25721742 3044.1
- CRBI      1   1361339 25748684 3044.3
- PutOuts   1   1455210 25842555 3045.3
- AtBat     1   1522760 25910105 3046.0
- Hits      1   1718870 26106215 3047.9

Step:  AIC=3031.26
Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + 
    Division + PutOuts + Assists

           Df Sum of Sq      RSS    AIC
<none>                  24500402 3031.3
- Assists   1    313650 24814051 3032.6
- CAtBat    1    534156 25034558 3034.9
- Division  1    798473 25298875 3037.7
- CWalks    1    965875 25466276 3039.4
- CRuns     1   1265082 25765484 3042.5
- Walks     1   1290168 25790569 3042.8
- CRBI      1   1326770 25827172 3043.1
- PutOuts   1   1551523 26051925 3045.4
- AtBat     1   1589780 26090181 3045.8
- Hits      1   1716068 26216469 3047.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fit.step) }\CommentTok{\#根据AIC准则选择变量  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + 
    CRBI + CWalks + Division + PutOuts + Assists, data = Hitters)

Residuals:
    Min      1Q  Median      3Q     Max 
-939.11 -176.87  -34.08  130.90 1910.55 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  162.53544   66.90784   2.429 0.015830 *  
AtBat         -2.16865    0.53630  -4.044 7.00e-05 ***
Hits           6.91802    1.64665   4.201 3.69e-05 ***
Walks          5.77322    1.58483   3.643 0.000327 ***
CAtBat        -0.13008    0.05550  -2.344 0.019858 *  
CRuns          1.40825    0.39040   3.607 0.000373 ***
CRBI           0.77431    0.20961   3.694 0.000271 ***
CWalks        -0.83083    0.26359  -3.152 0.001818 ** 
DivisionW   -112.38006   39.21438  -2.866 0.004511 ** 
PutOuts        0.29737    0.07444   3.995 8.50e-05 ***
Assists        0.28317    0.15766   1.796 0.073673 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 311.8 on 252 degrees of freedom
  (59 observations deleted due to missingness)
Multiple R-squared:  0.5405,    Adjusted R-squared:  0.5223 
F-statistic: 29.64 on 10 and 252 DF,  p-value: < 2.2e-16
\end{verbatim}

\section{7.2最优子集回归 Best Subset
Selection}\label{ux6700ux4f18ux5b50ux96c6ux56deux5f52-best-subset-selection}

当自变量很多时，有时候需要选择一部分自变量参与到模型，评判标准一般是三个：R方、BIC、CP值\\
使用包leaps中的regsubsets()来筛选最优变量子集，语法类似lm()函数

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaps)}
\NormalTok{regfit.full}\OtherTok{=}\FunctionTok{regsubsets}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,Hitters)   }\CommentTok{\#默认只给出包含1{-}8个变量的最优模型   }
\FunctionTok{summary}\NormalTok{(regfit.full)  }\CommentTok{\# *号表示列变量在方程中}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Subset selection object
Call: regsubsets.formula(Salary ~ ., Hitters)
19 Variables  (and intercept)
           Forced in Forced out
AtBat          FALSE      FALSE
Hits           FALSE      FALSE
HmRun          FALSE      FALSE
Runs           FALSE      FALSE
RBI            FALSE      FALSE
Walks          FALSE      FALSE
Years          FALSE      FALSE
CAtBat         FALSE      FALSE
CHits          FALSE      FALSE
CHmRun         FALSE      FALSE
CRuns          FALSE      FALSE
CRBI           FALSE      FALSE
CWalks         FALSE      FALSE
LeagueN        FALSE      FALSE
DivisionW      FALSE      FALSE
PutOuts        FALSE      FALSE
Assists        FALSE      FALSE
Errors         FALSE      FALSE
NewLeagueN     FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive
         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
1  ( 1 ) " "   " "  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
2  ( 1 ) " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
3  ( 1 ) " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
4  ( 1 ) " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
5  ( 1 ) "*"   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
6  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*" 
7  ( 1 ) " "   "*"  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " " 
8  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   " "    " "   "*"    "*"   " " 
         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
1  ( 1 ) " "    " "     " "       " "     " "     " "    " "       
2  ( 1 ) " "    " "     " "       " "     " "     " "    " "       
3  ( 1 ) " "    " "     " "       "*"     " "     " "    " "       
4  ( 1 ) " "    " "     "*"       "*"     " "     " "    " "       
5  ( 1 ) " "    " "     "*"       "*"     " "     " "    " "       
6  ( 1 ) " "    " "     "*"       "*"     " "     " "    " "       
7  ( 1 ) " "    " "     "*"       "*"     " "     " "    " "       
8  ( 1 ) "*"    " "     "*"       "*"     " "     " "    " "       
\end{verbatim}

使用函数中的nvmax选项，设置预测变量个数

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full}\OtherTok{=}\FunctionTok{regsubsets}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{nvmax=}\DecValTok{19}\NormalTok{) }\CommentTok{\#1{-}19个自变量的最优模型}
\NormalTok{reg.summary}\OtherTok{=}\FunctionTok{summary}\NormalTok{(regfit.full)}
\CommentTok{\#reg.summary}
\CommentTok{\#names(reg.summary) \#查看summary()返回的值}
\NormalTok{reg.summary}\SpecialCharTok{$}\NormalTok{which}\CommentTok{\#产生最小残差平方和的变量集合}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   (Intercept) AtBat  Hits HmRun  Runs   RBI Walks Years CAtBat CHits CHmRun
1         TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE
2         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE
3         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE
4         TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE
5         TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  FALSE FALSE  FALSE
6         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE  FALSE
7         TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE  TRUE   TRUE
8         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  FALSE FALSE   TRUE
9         TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE
10        TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE
11        TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE   TRUE FALSE  FALSE
12        TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE   TRUE FALSE  FALSE
13        TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE   TRUE FALSE  FALSE
14        TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE   TRUE FALSE  FALSE
15        TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE   TRUE  TRUE  FALSE
16        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE   TRUE  TRUE  FALSE
17        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE   TRUE  TRUE  FALSE
18        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE  TRUE  FALSE
19        TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE   TRUE  TRUE   TRUE
   CRuns  CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
1  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE
2  FALSE  TRUE  FALSE   FALSE     FALSE   FALSE   FALSE  FALSE      FALSE
3  FALSE  TRUE  FALSE   FALSE     FALSE    TRUE   FALSE  FALSE      FALSE
4  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
5  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
6  FALSE  TRUE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
7  FALSE FALSE  FALSE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
8   TRUE FALSE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
9   TRUE  TRUE   TRUE   FALSE      TRUE    TRUE   FALSE  FALSE      FALSE
10  TRUE  TRUE   TRUE   FALSE      TRUE    TRUE    TRUE  FALSE      FALSE
11  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE  FALSE      FALSE
12  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE  FALSE      FALSE
13  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE      FALSE
14  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE      FALSE
15  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE      FALSE
16  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE      FALSE
17  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE       TRUE
18  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE       TRUE
19  TRUE  TRUE   TRUE    TRUE      TRUE    TRUE    TRUE   TRUE       TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.summary}\SpecialCharTok{$}\NormalTok{rsq  }\CommentTok{\#查看R\^{}2的值 ，一个变量32\%，19个变量54.61159\%  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227
 [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164
[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159
\end{verbatim}

使用函数plot()和points()画图,两者用法差不多，后者在原图上添加点。

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{\# 生成的图像的展示格式为2行2列}
\FunctionTok{plot}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{rss,}\AttributeTok{xlab=}\StringTok{"Number of Variables"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"RSS"}\NormalTok{,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{adjr2,}\AttributeTok{xlab=}\StringTok{"Number of Variables"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"Adjusted RSq"}\NormalTok{,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{)   }
\CommentTok{\# type="l" 表示实线连接 }
\FunctionTok{which.max}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{adjr2) }\CommentTok{\# 最大的是包含11个}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{points}\NormalTok{(}\DecValTok{11}\NormalTok{,reg.summary}\SpecialCharTok{$}\NormalTok{adjr2[}\DecValTok{11}\NormalTok{],}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{cex=}\DecValTok{2}\NormalTok{,}\AttributeTok{pch=}\DecValTok{20}\NormalTok{) }
\FunctionTok{plot}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{cp,}\AttributeTok{xlab=}\StringTok{"Number of Variables"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"Cp"}\NormalTok{,}\AttributeTok{type=}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\FunctionTok{which.min}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{cp)  }\CommentTok{\# CP值最小的是包含10个}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{points}\NormalTok{(}\DecValTok{10}\NormalTok{,reg.summary}\SpecialCharTok{$}\NormalTok{cp[}\DecValTok{10}\NormalTok{],}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{cex=}\DecValTok{2}\NormalTok{,}\AttributeTok{pch=}\DecValTok{20}\NormalTok{)  }\CommentTok{\# 添加}
\FunctionTok{which.min}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{bic) }\CommentTok{\# BIC值最小的是包含6个}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(reg.summary}\SpecialCharTok{$}\NormalTok{bic,}\AttributeTok{xlab=}\StringTok{"Number of Variables"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"BIC"}\NormalTok{,}\AttributeTok{type=}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\DecValTok{6}\NormalTok{,reg.summary}\SpecialCharTok{$}\NormalTok{bic[}\DecValTok{6}\NormalTok{],}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{cex=}\DecValTok{2}\NormalTok{,}\AttributeTok{pch=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第七章-变量选择与模型构建_files/figure-pdf/unnamed-chunk-4-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.full,}\DecValTok{6}\NormalTok{)  }\CommentTok{\# 包含6个变量的最优模型的系数，应该是依据BIC选择的  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW 
  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 
     PutOuts 
   0.2643076 
\end{verbatim}

向前或者向后逐步选择 添加选项 method=''
``,不同的方法，对变量的选择也不同，即使个数一致。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.fwd}\OtherTok{=}\FunctionTok{regsubsets}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{nvmax=}\DecValTok{19}\NormalTok{,}\AttributeTok{method=}\StringTok{"forward"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(regfit.fwd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Subset selection object
Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
19 Variables  (and intercept)
           Forced in Forced out
AtBat          FALSE      FALSE
Hits           FALSE      FALSE
HmRun          FALSE      FALSE
Runs           FALSE      FALSE
RBI            FALSE      FALSE
Walks          FALSE      FALSE
Years          FALSE      FALSE
CAtBat         FALSE      FALSE
CHits          FALSE      FALSE
CHmRun         FALSE      FALSE
CRuns          FALSE      FALSE
CRBI           FALSE      FALSE
CWalks         FALSE      FALSE
LeagueN        FALSE      FALSE
DivisionW      FALSE      FALSE
PutOuts        FALSE      FALSE
Assists        FALSE      FALSE
Errors         FALSE      FALSE
NewLeagueN     FALSE      FALSE
1 subsets of each size up to 19
Selection Algorithm: forward
          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
1  ( 1 )  " "   " "  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
2  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
3  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
4  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
5  ( 1 )  "*"   "*"  " "   " "  " " " "   " "   " "    " "   " "    " "   "*" 
6  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*" 
7  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*" 
8  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    "*"   "*" 
9  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
10  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
11  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
12  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
13  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
14  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
15  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    "*"   " "    "*"   "*" 
16  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*" 
17  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*" 
18  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   " "    "*"   "*" 
19  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   "*"    "*"   "*" 
          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
1  ( 1 )  " "    " "     " "       " "     " "     " "    " "       
2  ( 1 )  " "    " "     " "       " "     " "     " "    " "       
3  ( 1 )  " "    " "     " "       "*"     " "     " "    " "       
4  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "       
5  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "       
6  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "       
7  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
8  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
9  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
10  ( 1 ) "*"    " "     "*"       "*"     "*"     " "    " "       
11  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "       
12  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "       
13  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
14  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
15  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
16  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
17  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
18  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
19  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.bwd}\OtherTok{=}\FunctionTok{regsubsets}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{nvmax=}\DecValTok{19}\NormalTok{,}\AttributeTok{method=}\StringTok{"backward"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(regfit.bwd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Subset selection object
Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
19 Variables  (and intercept)
           Forced in Forced out
AtBat          FALSE      FALSE
Hits           FALSE      FALSE
HmRun          FALSE      FALSE
Runs           FALSE      FALSE
RBI            FALSE      FALSE
Walks          FALSE      FALSE
Years          FALSE      FALSE
CAtBat         FALSE      FALSE
CHits          FALSE      FALSE
CHmRun         FALSE      FALSE
CRuns          FALSE      FALSE
CRBI           FALSE      FALSE
CWalks         FALSE      FALSE
LeagueN        FALSE      FALSE
DivisionW      FALSE      FALSE
PutOuts        FALSE      FALSE
Assists        FALSE      FALSE
Errors         FALSE      FALSE
NewLeagueN     FALSE      FALSE
1 subsets of each size up to 19
Selection Algorithm: backward
          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
1  ( 1 )  " "   " "  " "   " "  " " " "   " "   " "    " "   " "    "*"   " " 
2  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    "*"   " " 
3  ( 1 )  " "   "*"  " "   " "  " " " "   " "   " "    " "   " "    "*"   " " 
4  ( 1 )  "*"   "*"  " "   " "  " " " "   " "   " "    " "   " "    "*"   " " 
5  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    "*"   " " 
6  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    "*"   " " 
7  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    "*"   " " 
8  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   " "    "*"   "*" 
9  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
10  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
11  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*" 
12  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
13  ( 1 ) "*"   "*"  " "   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
14  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    " "   " "    "*"   "*" 
15  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   " "   "*"    "*"   " "    "*"   "*" 
16  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*" 
17  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   " "   "*"    "*"   " "    "*"   "*" 
18  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   " "    "*"   "*" 
19  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   "*"    "*"   "*" 
          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
1  ( 1 )  " "    " "     " "       " "     " "     " "    " "       
2  ( 1 )  " "    " "     " "       " "     " "     " "    " "       
3  ( 1 )  " "    " "     " "       "*"     " "     " "    " "       
4  ( 1 )  " "    " "     " "       "*"     " "     " "    " "       
5  ( 1 )  " "    " "     " "       "*"     " "     " "    " "       
6  ( 1 )  " "    " "     "*"       "*"     " "     " "    " "       
7  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
8  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
9  ( 1 )  "*"    " "     "*"       "*"     " "     " "    " "       
10  ( 1 ) "*"    " "     "*"       "*"     "*"     " "    " "       
11  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "       
12  ( 1 ) "*"    "*"     "*"       "*"     "*"     " "    " "       
13  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
14  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
15  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
16  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    " "       
17  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
18  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
19  ( 1 ) "*"    "*"     "*"       "*"     "*"     "*"    "*"       
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(}\FunctionTok{summary}\NormalTok{(regfit.bwd))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.full,}\DecValTok{7}\NormalTok{) }\CommentTok{\# 默认情况，包含7个变量的最优模型}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun 
  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 
   DivisionW      PutOuts 
-129.9866432    0.2366813 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.fwd,}\DecValTok{7}\NormalTok{) }\CommentTok{\# 向前逐步选择情况下，包含7个变量的最优模型}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 (Intercept)        AtBat         Hits        Walks         CRBI       CWalks 
 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 
   DivisionW      PutOuts 
-127.1223928    0.2533404 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.bwd,}\DecValTok{7}\NormalTok{) }\CommentTok{\# 向后逐步选择情况下，包含7个变量的最优模型}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 (Intercept)        AtBat         Hits        Walks        CRuns       CWalks 
 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 
   DivisionW      PutOuts 
-116.1692169    0.3028847 
\end{verbatim}

\section{7.3 BOX-COX变换}\label{box-coxux53d8ux6362}

计算lamda值\\

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)          }\CommentTok{\#加载MASS 包，每次启动R要重新加载}
\NormalTok{data7}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li4.3.csv"}\NormalTok{,}\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{) }
\CommentTok{\#读入数据文件并赋给data4.3，head = TRUE表示数据第一行是变量名称}
\FunctionTok{str}\NormalTok{(data7)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   29 obs. of  2 variables:
 $ y: num  2724 2850 1557 2016 2200 ...
 $ x: num  17885 32070 13050 18128 22247 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg43}\OtherTok{=}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{data7)}
\NormalTok{re4}\FloatTok{.3}\OtherTok{\textless{}{-}}\FunctionTok{boxcox}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data =}\NormalTok{ data7,}\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\FloatTok{0.1}\NormalTok{)) ;re4}\FloatTok{.3}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第七章-变量选择与模型构建_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\begin{verbatim}
$x
  [1] -3.00000000 -2.93939394 -2.87878788 -2.81818182 -2.75757576 -2.69696970
  [7] -2.63636364 -2.57575758 -2.51515152 -2.45454545 -2.39393939 -2.33333333
 [13] -2.27272727 -2.21212121 -2.15151515 -2.09090909 -2.03030303 -1.96969697
 [19] -1.90909091 -1.84848485 -1.78787879 -1.72727273 -1.66666667 -1.60606061
 [25] -1.54545455 -1.48484848 -1.42424242 -1.36363636 -1.30303030 -1.24242424
 [31] -1.18181818 -1.12121212 -1.06060606 -1.00000000 -0.93939394 -0.87878788
 [37] -0.81818182 -0.75757576 -0.69696970 -0.63636364 -0.57575758 -0.51515152
 [43] -0.45454545 -0.39393939 -0.33333333 -0.27272727 -0.21212121 -0.15151515
 [49] -0.09090909 -0.03030303  0.03030303  0.09090909  0.15151515  0.21212121
 [55]  0.27272727  0.33333333  0.39393939  0.45454545  0.51515152  0.57575758
 [61]  0.63636364  0.69696970  0.75757576  0.81818182  0.87878788  0.93939394
 [67]  1.00000000  1.06060606  1.12121212  1.18181818  1.24242424  1.30303030
 [73]  1.36363636  1.42424242  1.48484848  1.54545455  1.60606061  1.66666667
 [79]  1.72727273  1.78787879  1.84848485  1.90909091  1.96969697  2.03030303
 [85]  2.09090909  2.15151515  2.21212121  2.27272727  2.33333333  2.39393939
 [91]  2.45454545  2.51515152  2.57575758  2.63636364  2.69696970  2.75757576
 [97]  2.81818182  2.87878788  2.93939394  3.00000000

$y
  [1] -181.761293 -178.042197 -174.336745 -170.645500 -166.969020 -163.307900
  [7] -159.662760 -156.034255 -152.423063 -148.829897 -145.255502 -141.700653
 [13] -138.166162 -134.652872 -131.161660 -127.693439 -124.249154 -120.829785
 [19] -117.436340 -114.069859 -110.731410 -107.422083 -104.142989 -100.895251
 [25]  -97.680003  -94.498377  -91.351494  -88.240457  -85.166333  -82.130143
 [31]  -79.132838  -76.175285  -73.258246  -70.382340  -67.548046  -64.755630
 [37]  -62.005155  -59.296428  -56.628947  -54.001921  -51.414157  -48.864097
 [43]  -46.349758  -43.868683  -41.418012  -38.994388  -36.594089  -34.213066
 [49]  -31.847123  -29.492198  -27.144776  -24.802792  -22.466287  -20.140038
 [55]  -17.835070  -15.573152  -13.392814  -11.351903   -9.540089   -8.071206
 [61]   -7.084523   -6.706167   -7.030719   -8.063211   -9.742894  -11.940939
 [67]  -14.518948  -17.344678  -20.318873  -23.367307  -26.442263  -29.513719
 [73]  -32.563511  -35.582660  -38.566734  -41.514879  -44.428094  -47.308420
 [79]  -50.158511  -52.981195  -55.779348  -58.555727  -61.312949  -64.053417
 [85]  -66.779339  -69.492725  -72.195368  -74.888903  -77.574776  -80.254283
 [91]  -82.928587  -85.598707  -88.265564  -90.929967  -93.592631  -96.254196
 [97]  -98.915224 -101.576210 -104.237573 -106.899742
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#参数λ在区间[{-}3,3]上以步长为0.1 取值，re4.3中保存了λ和对应的对数似然函的数值}
\FunctionTok{summary}\NormalTok{(re4}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Length Class  Mode   
x 100    -none- numeric
y 100    -none- numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda}\OtherTok{\textless{}{-}}\NormalTok{ re4}\FloatTok{.3}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.max}\NormalTok{(re4}\FloatTok{.3}\SpecialCharTok{$}\NormalTok{y)]   }\CommentTok{\#将使对数似然函数值达到最大的λ赋给lambda}
\NormalTok{lambda }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6969697
\end{verbatim}

带入lamda值，异方差、自相关检验

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_bc}\OtherTok{=}\NormalTok{ (data7}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{\^{}}\NormalTok{lambda }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{lambda }
\NormalTok{reg4.}\FloatTok{3.2}\OtherTok{=}\FunctionTok{lm}\NormalTok{(y\_bc}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data =}\NormalTok{ data7)}
\CommentTok{\#reg4.3.2}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)) }\CommentTok{\# 图形排列为3列一行}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{fitted}\NormalTok{ (reg43), }\FunctionTok{resid}\NormalTok{ (reg43), }\AttributeTok{xlab=}\StringTok{\textquotesingle{} Fitted Values拟合值 \textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{} Residuals 残差值\textquotesingle{}}\NormalTok{,}\AttributeTok{main =} \StringTok{"原始回归"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{( }\FunctionTok{fitted}\NormalTok{ (reg4.}\FloatTok{3.2}\NormalTok{), }\FunctionTok{resid}\NormalTok{ (reg4.}\FloatTok{3.2}\NormalTok{), }\AttributeTok{xlab=}\StringTok{\textquotesingle{} Fitted Values拟合值 \textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{} Residuals 残差值\textquotesingle{}}\NormalTok{,}\AttributeTok{main =} \StringTok{"boxcox回归"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{( data7}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{resid}\NormalTok{ (reg4.}\FloatTok{3.2}\NormalTok{), }\AttributeTok{xlab=}\StringTok{\textquotesingle{} x \textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{} Residuals 残差值\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第七章-变量选择与模型构建_files/figure-pdf/unnamed-chunk-8-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{ (lmtest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: zoo
\end{verbatim}

\begin{verbatim}

Attaching package: 'zoo'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    as.Date, as.Date.numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bptest}\NormalTok{(reg43) }\CommentTok{\#Breusch{-}Pagan test 原假设：同方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  reg43
BP = 12.689, df = 1, p-value = 0.0003678
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bptest}\NormalTok{(reg4.}\FloatTok{3.2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    studentized Breusch-Pagan test

data:  reg4.3.2
BP = 4.5499, df = 1, p-value = 0.03292
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dwtest}\NormalTok{(reg43) }\CommentTok{\#原假设：不存在一阶自相关}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Durbin-Watson test

data:  reg43
DW = 2.0149, p-value = 0.4937
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dwtest}\NormalTok{(reg4.}\FloatTok{3.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Durbin-Watson test

data:  reg4.3.2
DW = 1.7444, p-value = 0.2244
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gqtest}\NormalTok{(reg43)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Goldfeld-Quandt test

data:  reg43
GQ = 0.78764, df1 = 13, df2 = 12, p-value = 0.6632
alternative hypothesis: variance increases from segment 1 to 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gqtest}\NormalTok{(reg4.}\FloatTok{3.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Goldfeld-Quandt test

data:  reg4.3.2
GQ = 0.52724, df1 = 13, df2 = 12, p-value = 0.8668
alternative hypothesis: variance increases from segment 1 to 2
\end{verbatim}

\section{7.4 BoxTidwell变换}\label{boxtidwellux53d8ux6362}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxTidwell}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{data7) }\CommentTok{\#函数位于包car}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 MLE of lambda Score Statistic (t) Pr(>|t|)  
        1.2796              1.8388   0.0774 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

iterations =  4 
\end{verbatim}

\chapter{Hello, Quarto}\label{hello-quarto-5}

\chapter{8 多重共线性
岭回归、LASSO、弹性网、偏最小二乘、主成分回归}\label{ux591aux91cdux5171ux7ebfux6027-ux5cadux56deux5f52lassoux5f39ux6027ux7f51ux504fux6700ux5c0fux4e8cux4e58ux4e3bux6210ux5206ux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ISLR)}
\FunctionTok{data}\NormalTok{(Hitters)}
\CommentTok{\#head(Hitters)}
\NormalTok{Hitters}\OtherTok{=}\FunctionTok{na.omit}\NormalTok{(Hitters) }\CommentTok{\# 用na.omit()剔除缺失值}
\CommentTok{\#str(Hitters)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid}\OtherTok{=}\DecValTok{10}\SpecialCharTok{\^{}}\FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\AttributeTok{length=}\DecValTok{100}\NormalTok{)  }\CommentTok{\#构造lambda的取值范围，从10\^{}10到10\^{}({-}2)次方 }
\CommentTok{\#model.matrix()用于构造回归设计矩阵，第一列都是1，自动把定性变量转换为哑变量（虚拟变量）。}
\NormalTok{x}\OtherTok{=}\FunctionTok{model.matrix}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,Hitters)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }
\NormalTok{y}\OtherTok{=}\NormalTok{Hitters}\SpecialCharTok{$}\NormalTok{Salary}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)  }
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(Hitters), }\FloatTok{0.7}\SpecialCharTok{*}\FunctionTok{nrow}\NormalTok{(Hitters)) }\CommentTok{\#随机取70\%，可以认为index是数据顺序的下标}
\end{Highlighting}
\end{Shaded}

\section{8.1
多重共线性检验}\label{ux591aux91cdux5171ux7ebfux6027ux68c0ux9a8c}

读取数据\\

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data8}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li3.3 民航客运量.sav"}\NormalTok{)}
\CommentTok{\#View(data8)}
\FunctionTok{str}\NormalTok{(data8)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [20 x 7] (S3: tbl_df/tbl/data.frame)
 $ 年份: num [1:20] 1997 1998 1999 2000 2001 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ Y   : num [1:20] 5630 5755 6094 6722 7524 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X1  : num [1:20] 78803 83818 89367 99066 109276 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ x2  : num [1:20] 1219 1319 1453 1609 1802 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X3  : num [1:20] 910927 994130 998921 994000 1036737 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X4  : num [1:20] 93308 95085 100164 105073 105155 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X5  : num [1:20] 644 695 719 744 784 ...
  ..- attr(*, "format.spss")= chr "F8.2"
\end{verbatim}

计算条件数kappa和VIF

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: carData
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kappa}\NormalTok{(data8[,}\DecValTok{3{-}7}\NormalTok{])}\CommentTok{\#条件数}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37401.33
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model8}\OtherTok{=}\FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{X3}\SpecialCharTok{+}\NormalTok{X4}\SpecialCharTok{+}\NormalTok{X5,}\AttributeTok{data=}\NormalTok{data8)}
\FunctionTok{vif}\NormalTok{(model8) }\CommentTok{\#给出vif值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      X1       x2       X3       X4       X5 
153.5352 462.8045 105.1168 158.8839 484.0188 
\end{verbatim}

\section{8.2 岭回归}\label{ux5cadux56deux5f52}

\subsection{8.2.1
用包MASS做岭回归}\label{ux7528ux5305massux505aux5cadux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{ridge0}\OtherTok{=}\FunctionTok{lm.ridge}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{X3}\SpecialCharTok{+}\NormalTok{X4}\SpecialCharTok{+}\NormalTok{X5,}\AttributeTok{data=}\NormalTok{data8,}\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\FloatTok{0.1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ridge0)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#matplot(ridge$lambda,coef(ridge),type="l",xlab=expression(lambda),ylab=expression(hat(beta)),col=1)}
\end{Highlighting}
\end{Shaded}

选择合适的λ值

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#选择适当的岭参数λ值}
\FunctionTok{select}\NormalTok{(ridge0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
modified HKB estimator is 0.02883007 
modified L-W estimator is 0.01748175 
smallest value of GCV  at 0.1 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#也可以直接使用广义交叉验证(generalized cross validation,GCV)确定λ的值。}
\FunctionTok{which.min}\NormalTok{(ridge0}\SpecialCharTok{$}\NormalTok{GCV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.1 
  2 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge}\OtherTok{=}\FunctionTok{lm.ridge}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{X3}\SpecialCharTok{+}\NormalTok{X4}\SpecialCharTok{+}\NormalTok{X5,}\AttributeTok{data=}\NormalTok{data8,}\AttributeTok{lambda =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{ridge}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                         X1            x2            X3            X4 
-3.059526e+03  2.592094e-02  2.338894e-01 -2.884699e-03  7.733209e-02 
           X5 
 3.608723e+00 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(model8) }\CommentTok{\#对比一下普通回归和岭回归的参数}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)            X1            x2            X3            X4 
-5.322037e+03  2.468667e-02 -2.100619e-01 -3.636669e-03  1.026201e-01 
           X5 
 5.156215e+00 
\end{verbatim}

\subsection{8.2.2
包ridge做岭回归}\label{ux5305ridgeux505aux5cadux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ridge)}
\NormalTok{ridge1}\OtherTok{=}\FunctionTok{linearRidge}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{X3}\SpecialCharTok{+}\NormalTok{X4}\SpecialCharTok{+}\NormalTok{X5,}\AttributeTok{data=}\NormalTok{data8)}
\NormalTok{ridge1}\SpecialCharTok{$}\NormalTok{lambda }\CommentTok{\#提取lambda值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003703637
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge1}\SpecialCharTok{$}\NormalTok{coef  }\CommentTok{\#提取系数值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         [,1]
X1  25512.406
x2   4679.708
X3 -10677.116
X4  20350.250
X5  19712.760
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ridge1)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-9-1.pdf}}

\section{8.3 LASSO}\label{lasso}

\subsection{8.3.1
使用包glmnet做LASSO}\label{ux4f7fux7528ux5305glmnetux505alasso}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
Loaded glmnet 4.1-10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.mod}\OtherTok{=}\FunctionTok{glmnet}\NormalTok{(x,y,}\AttributeTok{alpha=}\DecValTok{1}\NormalTok{,}\AttributeTok{lambda=}\NormalTok{grid)}
\FunctionTok{plot}\NormalTok{(lasso.mod)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-10-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.out}\OtherTok{=}\FunctionTok{cv.glmnet}\NormalTok{(x,y,}\AttributeTok{alpha=}\DecValTok{1}\NormalTok{)}
\NormalTok{bestlambda}\OtherTok{=}\NormalTok{cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{bestlambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.436791
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.final}\OtherTok{=}\FunctionTok{glmnet}\NormalTok{(x,y,}\AttributeTok{alpha=}\DecValTok{1}\NormalTok{,}\AttributeTok{lambda=}\FloatTok{11.5}\NormalTok{)}
\FunctionTok{coef}\NormalTok{(lasso.final)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
20 x 1 sparse Matrix of class "dgCMatrix"
                       s0
(Intercept)    2.65348128
AtBat          .         
Hits           1.97020663
HmRun          .         
Runs           .         
RBI            .         
Walks          2.24917671
Years          .         
CAtBat         .         
CHits          .         
CHmRun         0.02585679
CRuns          0.21645126
CRBI           0.40912641
CWalks         .         
LeagueN       14.74558923
DivisionW   -112.67249816
PutOuts        0.23263513
Assists        .         
Errors        -0.46316051
NewLeagueN     .         
\end{verbatim}

\subsection{8.3.2
使用包lars进行LASSO}\label{ux4f7fux7528ux5305larsux8fdbux884classo}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loaded lars 1.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#lars()函数只用于矩阵型数据}
\CommentTok{\#下面就把数据中的自变量和因变量变为矩阵形式}
\CommentTok{\#?lars \#查看命令  }
\NormalTok{fit.lASSO}\OtherTok{=}\FunctionTok{lars}\NormalTok{(x[index,],y[index])}
\FunctionTok{plot}\NormalTok{(fit.lASSO) }\CommentTok{\# 画图}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fit.lASSO) }\CommentTok{\#给出RSS、Cp值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LARS/LASSO
Call: lars(x = x[index, ], y = y[index])
   Df      Rss      Cp
0   1 41183745 222.645
1   2 36507899 178.703
2   3 34230116 158.323
3   4 32608908 144.394
4   5 27154517  92.803
5   6 26409899  87.487
6   7 25813841  83.630
7   8 19756252  26.112
8   9 19704560  27.604
9  10 19539989  27.988
10 11 19473781  29.337
11 12 19391991  30.533
12 13 19368916  32.307
13 14 19046450  31.138
14 15 18941678  32.109
15 16 18779822  32.519
16 17 18175135  28.577
17 16 17530594  20.244
18 17 16779792  14.867
19 18 16692227  16.007
20 19 16691647  18.002
21 20 16691496  20.000
\end{verbatim}

10折交叉验证

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.lasso}\OtherTok{=}\FunctionTok{cv.lars}\NormalTok{(x[index,],y[index],}\AttributeTok{K=}\DecValTok{10}\NormalTok{) }\CommentTok{\#10折交叉验证,给出图形}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-13-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.lasso  }\CommentTok{\# 给出数据}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$index
  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111
 [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323
 [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141
 [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960
 [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172
 [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384
 [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990
 [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 [97] 0.96969697 0.97979798 0.98989899 1.00000000

$cv
  [1] 227894.9 213649.3 202925.9 193159.6 183685.5 174810.0 167540.0 161275.5
  [9] 155477.1 150457.2 145865.5 141590.6 137923.3 134831.5 131841.8 129121.1
 [17] 127320.8 126074.7 125211.2 124668.2 124373.7 124419.5 124819.6 125302.2
 [25] 125662.9 125945.7 126087.7 126179.8 126283.1 126397.9 126562.4 126643.6
 [33] 126724.4 126826.5 126945.2 127024.5 127085.2 127157.8 127243.3 127320.4
 [41] 127409.3 127509.9 127621.8 127733.9 127836.6 127759.1 127693.7 127629.7
 [49] 127560.6 127503.0 127446.1 127365.2 127284.6 127229.9 127223.9 127226.1
 [57] 127175.8 127155.4 127144.6 127144.1 127153.9 127174.0 127200.7 127227.6
 [65] 127234.2 127259.2 127329.5 127402.1 127459.9 127529.3 127608.7 127696.4
 [73] 127792.4 127889.9 127992.8 128130.3 128284.3 128414.2 128586.5 128799.2
 [81] 129017.4 129238.7 129464.3 129711.6 129961.9 130217.4 130445.2 130674.2
 [89] 130940.3 131242.8 131550.2 131862.4 132181.5 132517.8 132858.5 133196.4
 [97] 133534.2 133872.8 134210.3 134549.1

$cv.error
  [1] 42425.24 42233.50 42071.15 41994.27 41466.95 40874.43 40135.01 39400.36
  [9] 38746.66 38090.18 37475.36 36929.26 36401.23 35877.06 35121.88 34458.07
 [17] 33790.69 33143.46 32499.27 31887.86 31318.18 30773.65 30493.37 30407.18
 [25] 30346.91 30308.02 30294.74 30332.36 30391.31 30453.78 30554.35 30590.21
 [33] 30628.90 30684.17 30748.24 30823.98 30907.34 30991.62 31075.26 31160.63
 [41] 31246.88 31334.02 31422.13 31513.39 31593.15 31506.65 31421.09 31338.12
 [49] 31258.63 31180.08 31102.53 31013.02 30908.35 30804.84 30701.65 30599.06
 [57] 30494.12 30386.79 30280.26 30174.52 30069.57 29965.44 29856.92 29746.50
 [65] 29636.95 29530.62 29439.25 29349.34 29264.05 29180.92 29098.71 29017.39
 [73] 28936.95 28857.93 28779.82 28700.65 28621.81 28519.05 28454.01 28423.25
 [81] 28393.37 28364.61 28336.87 28302.95 28267.21 28232.36 28166.62 28097.99
 [89] 28065.43 28063.66 28062.53 28062.06 28062.03 28061.28 28061.29 28063.99
 [97] 28065.96 28069.30 28074.68 28081.15

$mode
[1] "fraction"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best}\OtherTok{=}\NormalTok{cv.lasso}\SpecialCharTok{$}\NormalTok{index[}\FunctionTok{which.min}\NormalTok{(cv.lasso}\SpecialCharTok{$}\NormalTok{cv)]  }\CommentTok{\#选适合的值(随机性使得结果不同)}
\NormalTok{best}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2020202
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef}\OtherTok{=}\FunctionTok{coef.lars}\NormalTok{(fit.lASSO,}\AttributeTok{mode=}\StringTok{"fraction"}\NormalTok{,}\AttributeTok{s=}\NormalTok{best) }\CommentTok{\#使得CV最小步时的系数}
\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        AtBat          Hits         HmRun          Runs           RBI 
   0.00000000    2.40244177   -0.43689806    0.00000000    0.00000000 
        Walks         Years        CAtBat         CHits        CHmRun 
   1.71497735    0.00000000    0.00000000    0.00000000    0.62414597 
        CRuns          CRBI        CWalks       LeagueN     DivisionW 
   0.12199074    0.37831757    0.00000000    0.00000000 -168.75244520 
      PutOuts       Assists        Errors    NewLeagueN 
   0.26111186    0.02132686    0.00000000    0.00000000 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#min(fit.lASSO$Cp)  \#哪个Cp最小, 结果是第15个}
\NormalTok{lASSO.cp}\OtherTok{=}\FunctionTok{which.min}\NormalTok{(fit.lASSO}\SpecialCharTok{$}\NormalTok{Cp)  }\CommentTok{\#选Cp值小的哪一个}
\NormalTok{lASSO.cp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
18 
19 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef1}\OtherTok{=}\FunctionTok{coef.lars}\NormalTok{(fit.lASSO,}\AttributeTok{mode=}\StringTok{"step"}\NormalTok{,lASSO.cp) }\CommentTok{\#使fit.lASSO$Cp最小的step的系数}
\NormalTok{coef1[}\FunctionTok{which}\NormalTok{(coef1}\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)]  }\CommentTok{\#输出参数值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  [1]   -0.24907193   -0.33169739   -0.42535642   -0.80591961   -1.32739272
  [6]   -2.28109631   -2.41434800   -2.42258034   -2.42250948    0.33727628
 [11]    0.59217794    1.31358353    1.43841793    1.50979319    2.35750971
 [16]    2.36220241    2.49377733    2.55574379    2.64134388    2.64339103
 [21]    3.33310185    3.56148895    3.75545257    4.41121824    5.30998513
 [26]    7.95998163    8.60065533    8.63811284    8.63361915   -1.42857238
 [31]   -2.03622575   -2.88504991   -2.99948135   -3.19900741   -3.24609720
 [36]   -3.26295613   -4.35867415   -5.86366246   -6.65193218   -5.90300623
 [41]   -5.85979735   -5.86130896   -2.60039632   -3.69326084   -3.75547829
 [46]   -3.76988210    0.61198856    1.45233505    2.64482801    2.55732151
 [51]    2.55217576    2.55594052    0.96519406    1.11370685    1.20096182
 [56]    1.71042961    1.70996957    1.72634404    1.72517399    1.72187100
 [61]    1.71543319    1.82592379    1.86084750    2.04512820    2.72166928
 [66]    3.65002814    6.22929557    6.81845922    6.85228544    6.86647412
 [71]   -0.49200841   -2.70021821   -3.49703473   -3.96156925   -5.70193619
 [76]   -8.08280884  -13.63931177  -10.94799904  -10.79886043  -10.63846703
 [81]   -0.06594038   -0.06971885   -0.07473800    0.01618170    0.08809440
 [86]    0.32933233    0.39621216    0.40853752    0.52812477    0.55748536
 [91]    0.77545268    0.88072955    1.03251357    1.03990759    1.12906271
 [96]    1.15678236    1.19228393    1.36023738    1.59013801    2.11103268
[101]    1.73153945    1.71019824    1.72143927    0.01323787    0.01624577
[106]    0.12046817    0.12283940    0.12006444    0.12038082    0.12106812
[111]    0.12408101    0.14773866    0.15603587    0.18700314    0.32151721
[116]    0.50622968    0.93916210    1.31214578    1.33345901    1.33390289
[121]    0.16702415    0.21494757    0.23139386    0.30931907    0.30423922
[126]    0.31208394    0.38983506    0.38733943    0.35783966    0.34098823
[131]    0.31626319    0.31969187    0.31052964    0.30791341    0.30564113
[136]    0.26733253    0.21473209    0.04731399    0.25668913    0.26870756
[141]    0.26665985   -0.03949794   -0.18782280   -0.39153661   -0.82382710
[146]   -0.92729791   -0.93314483   -0.93294406    2.72741125    4.64622450
[151]    4.74154057    1.95450607    1.22042441    0.92240817   -0.82421344
[156]   -1.18295986   -9.82201937 -163.81129166 -167.31516795 -172.01478733
[161] -174.01978677 -176.79547823 -177.30905408 -177.34548872 -177.34759131
[166] -177.62003036 -177.50007946 -177.35379374 -183.12384345 -181.36909582
[171] -181.25564911 -181.17347670    0.25088799    0.25748735    0.26933881
[176]    0.27432449    0.28128232    0.28208745    0.28885039    0.29132076
[181]    0.29536621    0.31157715    0.33382639    0.36352304    0.36819080
[186]    0.36846202    0.36836496    0.01810815    0.02863272    0.03284989
[191]    0.03922219    0.04006940    0.06362358    0.07679494    0.09416634
[196]    0.16014165    0.25053538    0.41254167    0.47765255    0.48149401
[201]    0.48463133   -0.17034613   -0.36332403   -1.13114314   -2.19018126
[206]   -4.23752049   -4.72082403   -4.74519965   -4.76587099    2.25236613
[211]    2.44418047    6.81773447    8.12935782    8.74386434   10.51533636
[216]   11.86242024   10.64300064   14.28255623   15.19768588   15.58357258
\end{verbatim}

根据十折交叉验证发的结果来确定，根据cp值来选\\
\#\#\# 8.3.3 适应性LASSO\\
这里用到包msgps

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(msgps)}\CommentTok{\#加载包}
\CommentTok{\#?msgps\#查看命令  }
\NormalTok{fit.alasso}\OtherTok{=}\FunctionTok{msgps}\NormalTok{(x[index,],y[index],}\AttributeTok{penalty=}\StringTok{"alasso"}\NormalTok{,}\AttributeTok{gamma=}\DecValTok{1}\NormalTok{,}\AttributeTok{lambda=}\DecValTok{0}\NormalTok{) }\CommentTok{\#做适应性lasso回归}
\FunctionTok{summary}\NormalTok{(fit.alasso)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call: msgps(X = x[index, ], y = y[index], penalty = "alasso", gamma = 1,      lambda = 0) 

Penalty: "alasso" 

gamma: 1 

lambda: 0 

df:
      tuning     df
 [1,]  0.000  0.000
 [2,]  1.977  1.119
 [3,]  3.761  2.059
 [4,]  5.649  3.351
 [5,]  6.360  5.300
 [6,]  6.683  6.467
 [7,]  6.591  7.386
 [8,]  6.677  8.066
 [9,]  7.674  8.847
[10,]  8.978  9.578
[11,] 10.288 10.266
[12,] 11.199 11.128
[13,] 12.018 11.973
[14,] 13.272 13.347
[15,] 14.494 14.052
[16,] 15.710 14.615
[17,] 16.954 15.204
[18,] 17.837 16.070
[19,] 18.399 17.167
[20,] 18.891 17.730

tuning.max: 18.89 

ms.coef:
                   Cp      AICC       GCV       BIC
(Intercept)  227.0613  225.9157  227.0613  108.2384
AtBat         -2.2714   -2.2587   -2.2714   -1.0903
Hits           8.3968    8.3804    8.3968    6.7668
HmRun         -7.6260   -7.6551   -7.6260   -7.2338
Runs          -3.0467   -3.0323   -3.0467   -2.0793
RBI            2.8182    2.8085    2.8182    1.1536
Walks          6.0284    5.9838    6.0284    3.2680
Years        -14.3782  -14.3261  -14.3782   -7.7882
CAtBat         0.0000    0.0000    0.0000    0.0000
CHits          0.0000    0.0000    0.0000    0.0000
CHmRun         2.2337    2.2337    2.2337    2.2337
CRuns          0.8802    0.8720    0.8802    0.2613
CRBI           0.0000    0.0000    0.0000    0.0000
CWalks        -0.7089   -0.6984   -0.7089    0.0000
LeagueN       -1.0006   -1.0006   -1.0006    9.7558
DivisionW   -183.4865 -183.4865 -183.4865 -182.2417
PutOuts        0.3322    0.3308    0.3322    0.1703
Assists        0.3194    0.3142    0.3194    0.0000
Errors        -3.0845   -3.0259   -3.0845    0.0000
NewLeagueN    14.7439   14.7439   14.7439   19.7418

ms.tuning:
        Cp  AICC   GCV   BIC
[1,] 15.71 15.63 15.71 9.295

ms.df:
        Cp  AICC   GCV   BIC
[1,] 14.62 14.58 14.62 9.731
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit.alasso)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-16-1.pdf}}

\section{8.4 弹性网}\label{ux5f39ux6027ux7f51}

\section{8.5 主成分回归 PCR：Principal Components
Regression}\label{ux4e3bux6210ux5206ux56deux5f52-pcrprincipal-components-regression}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pls)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'pls'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:stats':

    loadings
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\CommentTok{\#?pcr  \#查看命令  }
\NormalTok{pcr.fit}\OtherTok{=}\FunctionTok{pcr}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pcr.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 263 19 
    Y dimension: 263 1
Fit method: svdpc
Number of components considered: 19

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV             452    352.8    352.1    351.9    350.1    345.9    342.6
adjCV          452    352.4    351.6    351.4    349.6    345.3    341.8
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV       343.3    346.0    346.9     347.3     349.9     352.7     357.0
adjCV    342.6    345.1    345.9     346.1     348.6     351.3     355.5
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV        350.5     349.6     340.3     341.8     340.8     345.0
adjCV     348.7     347.9     338.7     339.9     338.8     342.7

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
X         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96
Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75
        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
X         96.28     97.26     97.98     98.65     99.15     99.47     99.75
Salary    46.86     47.76     47.82     47.85     48.10     50.40     50.55
        16 comps  17 comps  18 comps  19 comps
X          99.89     99.97     99.99    100.00
Salary     53.01     53.85     54.61     54.61
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{validationplot}\NormalTok{(pcr.fit,}\AttributeTok{val.type=}\StringTok{"MSEP"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-18-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{pcr.fit}\OtherTok{=}\FunctionTok{pcr}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{subset=}\NormalTok{index,}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{validation=}\StringTok{"CV"}\NormalTok{)}
\FunctionTok{validationplot}\NormalTok{(pcr.fit,}\AttributeTok{val.type=}\StringTok{"MSEP"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-18-2.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcr.fit}\OtherTok{=}\FunctionTok{pcr}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{subset=}\NormalTok{index,}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{validation=}\StringTok{"CV"}\NormalTok{,}\AttributeTok{ncomp=}\DecValTok{6}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pcr.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 184 19 
    Y dimension: 184 1
Fit method: svdpc
Number of components considered: 6

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV           475.7    369.6    366.7    367.0    367.5    363.9    357.3
adjCV        475.7    369.2    366.1    366.4    366.8    363.0    356.1

TRAINING: % variance explained
        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
X         37.74    60.12    70.78    79.16    84.37    88.76
Salary    41.40    43.78    44.04    44.23    46.19    49.09
\end{verbatim}

\section{8.6 偏最小二乘回归 PLSR：Partial Least
Squares}\label{ux504fux6700ux5c0fux4e8cux4e58ux56deux5f52-plsrpartial-least-squares}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pls)}
\CommentTok{\#?plsr \#查看命令  }
\NormalTok{fit.pls}\OtherTok{=}\FunctionTok{plsr}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{Hitters,}\DecValTok{19}\NormalTok{,}\AttributeTok{subset=}\NormalTok{index,}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{validation=}\StringTok{"CV"}\NormalTok{) }\CommentTok{\#求出全部19个因子，没有19这个参数应该也可以  }
\NormalTok{fit.pls}\SpecialCharTok{$}\NormalTok{loadings  }\CommentTok{\#看代表性, 前10个因子可以代表87\%的方差}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
           Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 Comp 6 Comp 7 Comp 8 Comp 9
AtBat       0.233  0.410 -0.356         0.138 -0.126 -0.173 -0.282  0.128
Hits        0.232  0.404 -0.332         0.198               -0.154  0.414
HmRun       0.223  0.127 -0.329        -0.378  0.451 -0.181        -0.110
Runs        0.234  0.401 -0.354                                     0.170
RBI         0.265  0.268 -0.331                0.317                     
Walks       0.237  0.321 -0.163 -0.122                0.271  0.461 -0.543
Years       0.250 -0.426  0.244 -0.134 -0.114 -0.220                0.248
CAtBat      0.312 -0.331  0.196 -0.121  0.109 -0.188                0.153
CHits       0.314 -0.313  0.193 -0.104  0.143 -0.167                0.232
CHmRun      0.312 -0.242  0.119  0.175         0.278        -0.134 -0.399
CRuns       0.324 -0.294  0.184               -0.135  0.100         0.121
CRBI        0.327 -0.302  0.178         0.176                            
CWalks      0.305 -0.285  0.223 -0.143 -0.111 -0.156  0.124        -0.311
LeagueN                   0.215 -0.999  0.508  0.199 -0.219              
DivisionW         -0.358 -0.408         0.529 -0.236 -0.470  0.583 -0.278
PutOuts     0.120  0.364  0.105  0.236        -0.279 -0.745  0.858 -0.318
Assists            0.230 -0.101 -0.137  0.471 -0.741  0.634 -0.411       
Errors             0.258 -0.212 -0.265  0.227 -0.612  0.754        -0.201
NewLeagueN         0.108  0.204 -1.003  0.487  0.191 -0.294              
           Comp 10 Comp 11 Comp 12 Comp 13 Comp 14 Comp 15 Comp 16 Comp 17
AtBat      -0.363                   0.176                   0.145         
Hits       -0.302   0.203   0.225          -0.154  -0.396   0.330         
HmRun       0.860  -0.364                          -0.106  -0.528   0.289 
Runs       -0.328   0.285          -0.726   0.153   0.345  -0.443   0.154 
RBI         0.324  -0.428  -0.277   0.517                   0.482  -0.265 
Walks      -0.457   0.615  -0.384   0.189  -0.182           0.355  -0.195 
Years       0.150                          -0.404   0.353   1.240  -0.598 
CAtBat     -0.130                                  -0.108  -0.520         
CHits      -0.174                           0.122  -0.101  -0.390   0.150 
CHmRun      0.402  -0.118   0.331  -0.147  -0.138   0.266          -0.148 
CRuns      -0.161                           0.470   0.123  -0.559   0.270 
CRBI               -0.176          -0.167  -0.242           0.515         
CWalks              0.208           0.205   0.125  -0.451  -0.236   0.246 
LeagueN     0.186                  -0.402   0.698  -0.455   0.803  -0.337 
DivisionW           0.118                                                 
PutOuts     0.417  -0.297                                                 
Assists     0.635   0.171  -0.561   0.132                  -0.294   0.128 
Errors      0.220  -0.710   0.702  -0.170                   0.174         
NewLeagueN                          0.358  -0.703   0.484  -0.805   0.333 
           Comp 18 Comp 19
AtBat       0.635  -0.365 
Hits       -0.110   0.117 
HmRun       0.528  -0.270 
Runs       -0.676   0.307 
RBI        -0.632   0.326 
Walks       0.454  -0.207 
Years                     
CAtBat      0.360   0.185 
CHits       0.305  -0.593 
CHmRun                    
CRuns                     
CRBI                      
CWalks     -0.869   0.366 
LeagueN     0.117         
DivisionW                 
PutOuts                   
Assists                   
Errors                    
NewLeagueN -0.158         

               Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 Comp 6 Comp 7 Comp 8 Comp 9
SS loadings     1.021  1.789  1.191  2.284  1.364  1.695  2.068  1.613  1.176
Proportion Var  0.054  0.094  0.063  0.120  0.072  0.089  0.109  0.085  0.062
Cumulative Var  0.054  0.148  0.211  0.331  0.403  0.492  0.601  0.686  0.747
               Comp 10 Comp 11 Comp 12 Comp 13 Comp 14 Comp 15 Comp 16 Comp 17
SS loadings      2.319   1.562   1.229   1.312   1.566   1.181   4.977   1.012
Proportion Var   0.122   0.082   0.065   0.069   0.082   0.062   0.262   0.053
Cumulative Var   0.870   0.952   1.016   1.086   1.168   1.230   1.492   1.545
               Comp 18 Comp 19
SS loadings      2.804   1.000
Proportion Var   0.148   0.053
Cumulative Var   1.693   1.746
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.pls}\SpecialCharTok{$}\NormalTok{coef  }\CommentTok{\#看各个因子作为原变量的线性组合的系数}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
, , 1 comps

                Salary
AtBat       28.8577717
Hits        32.0613193
HmRun       23.1309581
Runs        29.6179036
RBI         32.7792110
Walks       31.5402513
Years       24.3870895
CAtBat      34.2508012
CHits       35.8029952
CHmRun      37.4082596
CRuns       37.1077619
CRBI        38.1762008
CWalks      32.7274136
LeagueN     -1.4299301
DivisionW  -19.7907232
PutOuts     22.4019081
Assists      1.6615160
Errors      -0.8965406
NewLeagueN  -0.3048561

, , 2 comps

                Salary
AtBat       34.1054270
Hits        50.5889437
HmRun        9.9899304
Runs        37.4791247
RBI         38.7016540
Walks       45.5152507
Years        3.5249134
CAtBat      23.6022955
CHits       30.2078050
CHmRun      39.2561374
CRuns       32.0921615
CRBI        36.3454059
CWalks      19.4287051
LeagueN      6.7214956
DivisionW  -62.8148188
PutOuts     55.4944210
Assists     13.8367411
Errors       0.6701918
NewLeagueN   8.2895905

, , 3 comps

                Salary
AtBat        12.727417
Hits         53.193722
HmRun       -24.366439
Runs         21.341327
RBI          28.073258
Walks        45.731552
Years        -6.458405
CAtBat       24.540976
CHits        38.890882
CHmRun       56.219977
CRuns        40.375514
CRBI         50.767824
CWalks       12.583843
LeagueN      15.746111
DivisionW  -113.685440
PutOuts      87.834494
Assists      19.609663
Errors      -14.312839
NewLeagueN   16.255978

, , 4 comps

                Salary
AtBat        -8.892259
Hits         78.091216
HmRun       -53.222807
Runs         11.001499
RBI          36.566477
Walks        43.563472
Years       -35.518149
CAtBat       17.428829
CHits        46.974806
CHmRun       88.260695
CRuns        47.496692
CRBI         72.327572
CWalks      -19.163513
LeagueN      -9.285108
DivisionW  -117.630316
PutOuts     104.202795
Assists      29.221827
Errors      -31.295404
NewLeagueN  -11.531167

, , 5 comps

                Salary
AtBat       -51.344165
Hits        131.220344
HmRun      -105.380812
Runs         -6.643378
RBI          58.021176
Walks        47.951499
Years       -89.617496
CAtBat        9.375706
CHits        70.008393
CHmRun      142.342125
CRuns        66.687946
CRBI        114.183276
CWalks      -80.877883
LeagueN       4.982816
DivisionW   -91.257545
PutOuts     106.530139
Assists      60.263370
Errors      -42.718896
NewLeagueN   -2.965171

, , 6 comps

                 Salary
AtBat       -84.4580764
Hits        147.8103746
HmRun      -103.2160681
Runs        -10.2863037
RBI          74.0321934
Walks        61.9508394
Years      -108.8339117
CAtBat        0.3058904
CHits        75.9393119
CHmRun      159.8054302
CRuns        75.3316030
CRBI        123.5805284
CWalks     -105.7447473
LeagueN      10.9497084
DivisionW   -96.7095720
PutOuts      92.2904120
Assists      49.2324863
Errors      -53.3140811
NewLeagueN    1.3217619

, , 7 comps

                Salary
AtBat      -151.762091
Hits        174.664618
HmRun      -104.993461
Runs        -16.328264
RBI          85.980056
Walks        98.750756
Years      -120.980298
CAtBat       -7.930457
CHits        94.853081
CHmRun      162.113741
CRuns       101.851825
CRBI        125.712972
CWalks     -135.899251
LeagueN       5.753176
DivisionW  -108.244869
PutOuts      70.287162
Assists      58.257942
Errors      -31.033837
NewLeagueN   -5.286823

, , 8 comps

                 Salary
AtBat      -263.4263696
Hits        228.0598512
HmRun       -97.8965952
Runs        -25.8792164
RBI          99.8007427
Walks       151.4421834
Years      -121.9909187
CAtBat      -20.4444913
CHits       129.9725311
CHmRun      140.1041181
CRuns       150.1021912
CRBI        113.6217631
CWalks     -187.0506098
LeagueN       0.9482658
DivisionW   -90.7832747
PutOuts      96.9335610
Assists      46.3337271
Errors      -18.6609326
NewLeagueN   -4.9006993

, , 9 comps

                Salary
AtBat      -325.230650
Hits        289.083116
HmRun       -89.097932
Runs        -25.689793
RBI         108.881640
Walks       146.164168
Years      -101.768817
CAtBat      -26.888185
CHits       158.081081
CHmRun      113.791796
CRuns       189.788174
CRBI         96.241230
CWalks     -233.692556
LeagueN      -3.394750
DivisionW   -97.298272
PutOuts      95.583572
Assists      50.297807
Errors      -26.419538
NewLeagueN   -1.179348

, , 10 comps

                Salary
AtBat      -352.054761
Hits        312.171620
HmRun       -64.478912
Runs        -35.410283
RBI         102.347851
Walks       142.997119
Years       -87.122957
CAtBat      -45.652148
CHits       152.193839
CHmRun      129.449807
CRuns       208.036151
CRBI         81.164361
CWalks     -240.640391
LeagueN       1.498286
DivisionW   -96.691291
PutOuts     102.140437
Assists      72.231079
Errors      -32.339877
NewLeagueN   -2.049494

, , 11 comps

                Salary
AtBat      -368.380899
Hits        346.017415
HmRun       -57.299430
Runs        -39.476097
RBI          78.536053
Walks       155.131180
Years       -73.528728
CAtBat      -69.594244
CHits       142.644776
CHmRun      146.194453
CRuns       233.957437
CRBI         57.950004
CWalks     -238.953464
LeagueN       3.766207
DivisionW   -93.074294
PutOuts      98.282422
Assists      84.082715
Errors      -47.779955
NewLeagueN   -2.043361

, , 12 comps

                Salary
AtBat      -377.706166
Hits        394.126314
HmRun       -50.150086
Runs        -57.728507
RBI          50.096113
Walks       149.225862
Years       -61.823988
CAtBat     -105.647150
CHits       129.044367
CHmRun      180.024187
CRuns       279.257192
CRBI         26.591733
CWalks     -237.800865
LeagueN       2.780006
DivisionW   -92.339147
PutOuts      98.249177
Assists      67.382927
Errors      -27.922965
NewLeagueN    3.099989

, , 13 comps

                Salary
AtBat      -370.153515
Hits        406.360774
HmRun       -53.474179
Runs        -92.463021
RBI          66.062182
Walks       153.443242
Years       -59.057589
CAtBat     -127.824059
CHits       121.622860
CHmRun      184.187465
CRuns       315.724925
CRBI          4.238974
CWalks     -231.359684
LeagueN      -6.772245
DivisionW   -92.122928
PutOuts      96.013024
Assists      67.828450
Errors      -29.450151
NewLeagueN   12.460141

, , 14 comps

                Salary
AtBat      -364.168317
Hits        397.107151
HmRun       -56.724786
Runs        -93.427456
RBI          73.600184
Walks       151.065623
Years       -62.599401
CAtBat     -142.845047
CHits       117.960433
CHmRun      183.338065
CRuns       351.974328
CRBI         -6.189724
CWalks     -234.757400
LeagueN       4.441543
DivisionW   -91.559463
PutOuts      96.740619
Assists      68.497265
Errors      -31.098994
NewLeagueN    1.677148

, , 15 comps

                 Salary
AtBat      -359.1111821
Hits        382.5242107
HmRun       -60.7977487
Runs        -86.1021772
RBI          76.6711166
Walks       152.5492546
Years       -53.5271857
CAtBat     -159.9219214
CHits       110.1555868
CHmRun      185.6491237
CRuns       377.8673409
CRBI         -4.1902690
CWalks     -248.2258475
LeagueN      -0.6745396
DivisionW   -90.7369286
PutOuts      97.9406686
Assists      70.1208941
Errors      -30.8794003
NewLeagueN    7.7823155

, , 16 comps

                Salary
AtBat      -353.020806
Hits        383.980883
HmRun       -59.156080
Runs        -91.570009
RBI          73.985849
Walks       154.054621
Years       -43.193937
CAtBat     -196.715267
CHits        91.629810
CHmRun      163.345774
CRuns       401.214472
CRBI         38.947127
CWalks     -249.380860
LeagueN       3.520271
DivisionW   -90.661568
PutOuts      97.375614
Assists      69.052287
Errors      -29.790575
NewLeagueN    3.719384

, , 17 comps

                  Salary
AtBat      -350.32593227
Hits        383.94813768
HmRun       -52.40685433
Runs        -92.62892049
RBI          66.99186717
Walks       151.70755273
Years       -47.99122216
CAtBat     -218.10036708
CHits        81.96075615
CHmRun      146.67329576
CRuns       421.11185283
CRBI         67.28682002
CWalks     -246.28391345
LeagueN      -0.06530316
DivisionW   -90.63465732
PutOuts      97.20068660
Assists      70.75795368
Errors      -30.60419947
NewLeagueN    7.29241007

, , 18 comps

                 Salary
AtBat      -348.9477915
Hits        384.3473121
HmRun       -50.6557774
Runs        -94.9352449
RBI          64.9874113
Walks       153.0956250
Years       -48.3909534
CAtBat     -213.9767280
CHits        77.2838601
CHmRun      145.3706491
CRuns       423.8166519
CRBI         69.3410727
CWalks     -249.3544917
LeagueN       0.1649899
DivisionW   -90.9193138
PutOuts      97.1230614
Assists      70.7289114
Errors      -30.7209658
NewLeagueN    6.9431029

, , 19 comps

                Salary
AtBat      -356.359186
Hits        393.344207
HmRun       -50.363942
Runs        -97.760851
RBI          65.541442
Walks       153.680702
Years       -50.977555
CAtBat     -172.163351
CHits        10.612528
CHmRun      134.858978
CRuns       449.242845
CRBI         85.501840
CWalks     -255.248505
LeagueN      -0.590249
DivisionW   -90.828540
PutOuts      97.707098
Assists      70.076691
Errors      -30.470673
NewLeagueN    7.783451
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{validationplot}\NormalTok{(fit.pls,}\AttributeTok{val.type=}\StringTok{"MSEP"}\NormalTok{)  }\CommentTok{\#此图可用来根据CV所用准则最优的原则挑选因子数量}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapters/第八章-多重共线性_files/figure-pdf/unnamed-chunk-19-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{RMSEP}\NormalTok{(fit.pls);}\FunctionTok{MSEP}\NormalTok{(fit.pls);}\FunctionTok{R2}\NormalTok{(fit.pls)   }\CommentTok{\#不同准则(RMESP，MSEP,R2)在不同因子数量时的值}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV           475.7    359.6    356.6    355.1    353.9    352.5    350.6
adjCV        475.7    359.2    355.9    353.7    352.7    350.5    348.8
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV       349.9    351.0    350.0     347.3     346.6     344.8     346.4
adjCV    348.0    348.6    347.4     344.9     344.3     342.6     344.1
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV        347.7     348.0     349.1     350.4     350.2     353.6
adjCV     345.2     345.5     346.5     347.7     347.6     350.6
\end{verbatim}

\begin{verbatim}
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
CV          226278   129292   127197   126069   125266   124227   122887
adjCV       226278   129028   126675   125089   124382   122869   121678
       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
CV      122438   123231   122492    120601    120123    118917    120018
adjCV   121086   121530   120659    118984    118553    117386    118372
       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
CV       120869    121120    121874    122763    122656    125007
adjCV    119148    119377    120081    120891    120808    122949
\end{verbatim}

\begin{verbatim}
(Intercept)      1 comps      2 comps      3 comps      4 comps      5 comps  
   -0.01096      0.42235      0.43171      0.43675      0.44034      0.44498  
    6 comps      7 comps      8 comps      9 comps     10 comps     11 comps  
    0.45097      0.45298      0.44943      0.45273      0.46118      0.46332  
   12 comps     13 comps     14 comps     15 comps     16 comps     17 comps  
    0.46870      0.46379      0.45999      0.45886      0.45549      0.45152  
   18 comps     19 comps  
    0.45200      0.44150  
\end{verbatim}

根据CV确定因子个数,理论上选12个是CV值是最小的，这里为简化计算，选3个

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pls.fit}\OtherTok{=}\FunctionTok{plsr}\NormalTok{(Salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{Hitters,}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{ncomp=}\DecValTok{3}\NormalTok{)   }\CommentTok{\# 选3个因子进行偏最小二乘回归 }
\FunctionTok{summary}\NormalTok{(pls.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data:   X dimension: 263 19 
    Y dimension: 263 1
Fit method: kernelpls
Number of components considered: 3
TRAINING: % variance explained
        1 comps  2 comps  3 comps
X         38.08    51.03    65.98
Salary    43.05    46.40    47.72
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pls.fit)  }\CommentTok{\# 查看建模给出哪些结果}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "coefficients"    "scores"          "loadings"        "loading.weights"
 [5] "Yscores"         "Yloadings"       "projection"      "Xmeans"         
 [9] "Ymeans"          "fitted.values"   "residuals"       "Xvar"           
[13] "Xtotvar"         "fit.time"        "ncomp"           "method"         
[17] "center"          "scale"           "call"            "terms"          
[21] "model"          
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(pls.fit)}\SpecialCharTok{$}\NormalTok{coefficients  }\CommentTok{\#给出估计参数}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
, , 1 comps

                Salary
AtBat       25.0420570
Hits        27.8270677
HmRun       21.7597795
Runs        26.6334747
RBI         28.5110396
Walks       28.1564522
Years       25.4154350
CAtBat      33.3750764
CHits       34.8197471
CHmRun      33.2986538
CRuns       35.6931216
CRBI        35.9651267
CWalks      31.0715657
LeagueN     -0.9059591
DivisionW  -12.2120349
PutOuts     19.0607903
Assists      1.6135259
Errors      -0.3425902
NewLeagueN  -0.1798022

, , 2 comps

               Salary
AtBat       26.988567
Hits        43.689562
HmRun       12.561740
Runs        36.166065
RBI         30.845058
Walks       42.533221
Years        8.957764
CAtBat      26.430999
CHits       33.829611
CHmRun      30.279265
CRuns       34.715277
CRBI        35.195944
CWalks      20.084235
LeagueN     17.985777
DivisionW  -48.032748
PutOuts     56.282325
Assists      4.199492
Errors      -4.327875
NewLeagueN  15.096458

, , 3 comps

                Salary
AtBat       11.5612560
Hits        43.0738184
HmRun       -3.0788552
Runs        29.4670885
RBI         21.0426670
Walks       43.6363824
Years        5.4373897
CAtBat      28.9571188
CHits       41.9403523
CHmRun      35.6444168
CRuns       42.6088887
CRBI        43.5878120
CWalks      17.0901059
LeagueN     25.0699041
DivisionW  -69.4031815
PutOuts     74.5802997
Assists      0.8434621
Errors     -16.4113867
NewLeagueN  17.3645626
\end{verbatim}

\chapter{Hello, Quarto}\label{hello-quarto-6}

\chapter{9 非线性回归}\label{ux975eux7ebfux6027ux56deux5f52}

\section{9.1 多项式回归}\label{ux591aux9879ux5f0fux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data90}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li9.2 多项式.sav"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(data90)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [18 x 7] (S3: tbl_df/tbl/data.frame)
 $ K  : num [1:18] 1 2 3 4 5 6 7 8 9 10 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ X1 : num [1:18] 66.3 41 73 45 57.2 ...
  ..- attr(*, "format.spss")= chr "F8.3"
 $ X2 : num [1:18] 7 5 10 6 4 5 4 6 9 5 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ Y  : num [1:18] 196 63 252 84 126 14 49 49 266 49 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ X11: num [1:18] 4394 1678 5328 2026 3272 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X22: num [1:18] 49 25 100 36 16 25 16 36 81 25 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ X12: num [1:18] 464 205 730 270 229 ...
  ..- attr(*, "format.spss")= chr "F8.2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data9}\OtherTok{=}\NormalTok{data90[,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)]}
\FunctionTok{str}\NormalTok{(data9)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [18 x 3] (S3: tbl_df/tbl/data.frame)
 $ X1: num [1:18] 66.3 41 73 45 57.2 ...
  ..- attr(*, "format.spss")= chr "F8.3"
 $ X2: num [1:18] 7 5 10 6 4 5 4 6 9 5 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ Y : num [1:18] 196 63 252 84 126 14 49 49 266 49 ...
  ..- attr(*, "format.spss")= chr "F8.0"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poly1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X1}\SpecialCharTok{+}\NormalTok{X2}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(X2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(X1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\NormalTok{(X1}\SpecialCharTok{*}\NormalTok{X2),}\AttributeTok{data =}\NormalTok{ data9)}
\FunctionTok{summary}\NormalTok{(poly1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ X1 + X2 + I(X2^2) + I(X1^2) + (X1 * X2), data = data9)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9521 -1.0445  0.2148  0.8888  3.0599 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -65.38561    6.12304 -10.679 1.75e-07 ***
X1            1.01718    0.22807   4.460 0.000779 ***
X2            5.21714    1.34874   3.868 0.002235 ** 
I(X2^2)       0.16616    0.12015   1.383 0.191889    
I(X1^2)       0.03579    0.00219  16.342 1.45e-09 ***
X1:X2        -0.01959    0.01398  -1.401 0.186463    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.743 on 12 degrees of freedom
Multiple R-squared:  0.9997,    Adjusted R-squared:  0.9995 
F-statistic:  7110 on 5 and 12 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#poly2 \textless{}{-} lm(Y \textasciitilde{} poly(data9$X1,data9$X2,degree=2,raw=TRUE),data = data9)  \# poly函数表示关于x的多项式函数，最高次数为2。raw 参数表示使用正交多项式}
\CommentTok{\#summary(poly2)}
\end{Highlighting}
\end{Shaded}

\chapter{Hello, Quarto}\label{hello-quarto-7}

\chapter{10 广义回归}\label{ux5e7fux4e49ux56deux5f52}

\section{10.1 虚拟变量}\label{ux865aux62dfux53d8ux91cf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.1 储蓄.sav"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(data10)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
      k     y    x1    x2
  <dbl> <dbl> <dbl> <dbl>
1     1   235   2.3     0
2     2   346   3.2     1
3     3   365   2.8     0
4     4   468   3.5     1
5     5   658   2.6     0
6     6   867   3.2     1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumm}\OtherTok{=}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2,}\AttributeTok{data=}\NormalTok{data10)}
\FunctionTok{summary}\NormalTok{(dumm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x1 + x2, data = data10)

Residuals:
    Min      1Q  Median      3Q     Max 
-2658.1  -706.9  -114.5   600.1  2309.0 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -7976.8     1093.4  -7.295 1.55e-07 ***
x1            3826.1      304.6  12.562 4.82e-12 ***
x2           -3700.3      513.4  -7.207 1.90e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1289 on 24 degrees of freedom
  (1 observation deleted due to missingness)
Multiple R-squared:  0.8793,    Adjusted R-squared:  0.8692 
F-statistic: 87.43 on 2 and 24 DF,  p-value: 9.555e-12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(dumm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)          x1          x2 
  -7976.809    3826.129   -3700.330 
\end{verbatim}

\section{10.2
分段回归=断点回归}\label{ux5206ux6bb5ux56deux5f52ux65adux70b9ux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data102}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.2 折线回归.sav"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(data102)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
      I     Y     X    X2
  <dbl> <dbl> <dbl> <dbl>
1     1  2.57   650   150
2     2  4.4    340     0
3     3  4.52   400     0
4     4  1.39   800   300
5     5  4.75   300     0
6     6  3.55   570    70
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m102}\OtherTok{=}\FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X}\SpecialCharTok{+}\NormalTok{X2,}\AttributeTok{data=}\NormalTok{data102)}
\FunctionTok{summary}\NormalTok{(m102)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ X + X2, data = data102)

Residuals:
       1        2        3        4        5        6        7        8 
-0.17160 -0.15117  0.20605 -0.17463  0.04068  0.18068  0.29765 -0.22765 
attr(,"format.spss")
[1] "F8.2"

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  5.895447   0.604213   9.757 0.000192 ***
X           -0.003954   0.001492  -2.650 0.045432 *  
X2          -0.003893   0.002310  -1.685 0.152774    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.2449 on 5 degrees of freedom
Multiple R-squared:  0.9693,    Adjusted R-squared:  0.9571 
F-statistic: 79.06 on 2 and 5 DF,  p-value: 0.0001645
\end{verbatim}

\section{10.3 Logistic回归}\label{logisticux56deux5f52}

分组数据

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\FloatTok{.4}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.4 二分因变量分组数据.sav"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(data10}\FloatTok{.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [9 x 6] (S3: tbl_df/tbl/data.frame)
 $ x : num [1:9] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5
  ..- attr(*, "format.spss")= chr "F8.2"
 $ ni: num [1:9] 25 32 58 52 43 39 28 21 15
  ..- attr(*, "format.spss")= chr "F8.0"
 $ mi: num [1:9] 8 13 26 22 20 22 16 12 10
  ..- attr(*, "format.spss")= chr "F8.0"
 $ pi: num [1:9] 0.32 0.406 0.448 0.423 0.465 ...
  ..- attr(*, "format.spss")= chr "F8.5"
  ..- attr(*, "display_width")= int 10
 $ y : num [1:9] -0.754 -0.379 -0.208 -0.31 -0.14 ...
  ..- attr(*, "format.spss")= chr "F8.6"
  ..- attr(*, "display_width")= int 10
 $ wi: num [1:9] 5.44 7.72 14.34 12.69 10.7 ...
  ..- attr(*, "format.spss")= chr "F8.2"
  ..- attr(*, "display_width")= int 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm10}\FloatTok{.4}\OtherTok{=}\FunctionTok{lm}\NormalTok{(pi}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{weights =}\NormalTok{ wi,}\AttributeTok{data=}\NormalTok{data10}\FloatTok{.4}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pi ~ x, data = data10.4, weights = wi)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-0.11984 -0.07031  0.01421  0.06222  0.10693 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.292781   0.027479   10.65 1.41e-05 ***
x           0.036430   0.005011    7.27 0.000167 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.09344 on 7 degrees of freedom
Multiple R-squared:  0.8831,    Adjusted R-squared:  0.8663 
F-statistic: 52.85 on 1 and 7 DF,  p-value: 0.000167
\end{verbatim}

未分组数据

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\FloatTok{.5}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.5 二分因变量原始数据.sav"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(data10}\FloatTok{.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [28 x 5] (S3: tbl_df/tbl/data.frame)
 $ x3   : num [1:28] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ x1   : num [1:28] 18 21 23 23 28 31 36 42 46 48 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ x2   : num [1:28] 850 1200 850 950 1200 850 1500 1000 950 1200 ...
  ..- attr(*, "format.spss")= chr "F8.2"
 $ y    : num [1:28] 0 0 1 1 1 0 1 1 1 0 ...
  ..- attr(*, "format.spss")= chr "F8.0"
 $ total: num [1:28] 1 1 1 1 1 1 1 1 1 1 ...
  ..- attr(*, "format.spss")= chr "F8.0"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm10}\FloatTok{.5}\OtherTok{=}\FunctionTok{glm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2}\SpecialCharTok{+}\NormalTok{x3,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}\AttributeTok{data=}\NormalTok{data10}\FloatTok{.5}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = y ~ x1 + x2 + x3, family = binomial(link = "logit"), 
    data = data10.5)

Coefficients:
             Estimate Std. Error z value Pr(>|z|)  
(Intercept) -3.655016   2.091218  -1.748   0.0805 .
x1           0.082168   0.052119   1.577   0.1149  
x2           0.001517   0.001865   0.813   0.4160  
x3          -2.501844   1.157815  -2.161   0.0307 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38.673  on 27  degrees of freedom
Residual deviance: 25.971  on 24  degrees of freedom
AIC: 33.971

Number of Fisher Scoring iterations: 5
\end{verbatim}

probit

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\FloatTok{.6}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.4 二分因变量分组数据.sav"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(data10}\FloatTok{.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [9 x 6] (S3: tbl_df/tbl/data.frame)
 $ x : num [1:9] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5
  ..- attr(*, "format.spss")= chr "F8.2"
 $ ni: num [1:9] 25 32 58 52 43 39 28 21 15
  ..- attr(*, "format.spss")= chr "F8.0"
 $ mi: num [1:9] 8 13 26 22 20 22 16 12 10
  ..- attr(*, "format.spss")= chr "F8.0"
 $ pi: num [1:9] 0.32 0.406 0.448 0.423 0.465 ...
  ..- attr(*, "format.spss")= chr "F8.5"
  ..- attr(*, "display_width")= int 10
 $ y : num [1:9] -0.754 -0.379 -0.208 -0.31 -0.14 ...
  ..- attr(*, "format.spss")= chr "F8.6"
  ..- attr(*, "display_width")= int 10
 $ wi: num [1:9] 5.44 7.72 14.34 12.69 10.7 ...
  ..- attr(*, "format.spss")= chr "F8.2"
  ..- attr(*, "display_width")= int 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm10}\FloatTok{.6}\OtherTok{=}\FunctionTok{glm}\NormalTok{(pi}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{weights =}\NormalTok{ ni,}\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{),}\AttributeTok{data=}\NormalTok{data10}\FloatTok{.6}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pi ~ x, data = data10.4, weights = wi)

Weighted Residuals:
     Min       1Q   Median       3Q      Max 
-0.11984 -0.07031  0.01421  0.06222  0.10693 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.292781   0.027479   10.65 1.41e-05 ***
x           0.036430   0.005011    7.27 0.000167 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.09344 on 7 degrees of freedom
Multiple R-squared:  0.8831,    Adjusted R-squared:  0.8663 
F-statistic: 52.85 on 1 and 7 DF,  p-value: 0.000167
\end{verbatim}

\section{10.5
多类别logistic回归}\label{ux591aux7c7bux522blogisticux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\FloatTok{.7}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.7 telco多值因变量.sav"}\NormalTok{)}
\CommentTok{\#str(data10.7)}
\FunctionTok{library}\NormalTok{(nnet)}
\NormalTok{lm10}\FloatTok{.7}\OtherTok{=}\FunctionTok{multinom}\NormalTok{(CUSTCAT}\SpecialCharTok{\textasciitilde{}}\NormalTok{MARITAL}\SpecialCharTok{+}\NormalTok{ED}\SpecialCharTok{+}\NormalTok{RETIRE}\SpecialCharTok{+}\NormalTok{GENDER}\SpecialCharTok{+}\NormalTok{AGE}\SpecialCharTok{+}\NormalTok{ADDRESS}\SpecialCharTok{+}\NormalTok{INCOME}\SpecialCharTok{+}\NormalTok{EMPLOY}\SpecialCharTok{+}\NormalTok{RESIDE,}\AttributeTok{data=}\NormalTok{data10}\FloatTok{.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# weights:  44 (30 variable)
initial  value 1386.294361 
iter  10 value 1334.291926
iter  20 value 1284.698564
iter  30 value 1258.658734
final  value 1258.477963 
converged
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
multinom(formula = CUSTCAT ~ MARITAL + ED + RETIRE + GENDER + 
    AGE + ADDRESS + INCOME + EMPLOY + RESIDE, data = data10.7)

Coefficients:
  (Intercept)   MARITAL          ED      RETIRE      GENDER         AGE
2  -2.2041272 0.4257118  0.52703930 -0.47406402  0.12956739 -0.01349908
3  -0.1790401 0.3305860 -0.08830832  0.37565622 -0.08884163 -0.01621111
4  -3.1896929 0.2532196  0.81460612  0.09318089 -0.02247600 -0.01561131
     ADDRESS       INCOME     EMPLOY       RESIDE
2 0.04416277 0.0001796203 0.03577267  0.027948290
3 0.02728132 0.0022966976 0.04756982 -0.001011698
4 0.02824341 0.0020517567 0.03960337  0.187933399

Std. Errors:
  (Intercept)   MARITAL         ED    RETIRE    GENDER        AGE    ADDRESS
2   0.5525527 0.2514977 0.08908194 0.5960736 0.1907637 0.01296751 0.01322415
3   0.4979993 0.2362058 0.08665598 0.4830198 0.1778917 0.01170622 0.01194888
4   0.5699498 0.2511376 0.09204487 0.6090522 0.1925551 0.01340774 0.01355116
       INCOME     EMPLOY     RESIDE
2 0.001921535 0.01645807 0.09164314
3 0.001747870 0.01457933 0.08703146
4 0.001765299 0.01707086 0.08807858

Residual Deviance: 2516.956 
AIC: 2576.956 
\end{verbatim}

计算系数显著性和混淆矩阵

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coefficients}\SpecialCharTok{/}\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}\SpecialCharTok{$}\NormalTok{standard.errors}
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(z), }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{*} \DecValTok{2}
\NormalTok{p }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   (Intercept)    MARITAL           ED    RETIRE    GENDER       AGE
2 6.635520e-05 0.09051138 3.291785e-09 0.4264325 0.4970089 0.2978791
3 7.192071e-01 0.16164271 3.081708e-01 0.4367316 0.6174876 0.1661049
4 2.187908e-08 0.31331506 0.000000e+00 0.8784036 0.9070779 0.2442821
       ADDRESS    INCOME      EMPLOY     RESIDE
2 0.0008391297 0.9255242 0.029737908 0.76038997
3 0.0224203627 0.1888468 0.001103076 0.99072519
4 0.0371414469 0.2451251 0.020344103 0.03286722
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(pp }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           1         2          3          4
1 0.19903015 0.2908941 0.14942977 0.36064602
2 0.06180225 0.1840803 0.05649074 0.69762670
3 0.13897591 0.1972725 0.58078981 0.08296175
4 0.45673568 0.1853013 0.25641446 0.10154860
5 0.39037598 0.1338813 0.38357701 0.09216568
6 0.27016498 0.2252421 0.37440253 0.13019035
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre\_mult }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}
\FunctionTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
Loading required package: lattice
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(lattice)}
\NormalTok{conMat\_mult }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{factor}\NormalTok{(pre\_mult), }\FunctionTok{factor}\NormalTok{(data10}\FloatTok{.7}\SpecialCharTok{$}\NormalTok{CUSTCAT))}
\NormalTok{conMat\_mult}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   1   2   3   4
         1 126  58  82  53
         2  11  16  18  21
         3  73  62 136  36
         4  56  81  45 126

Overall Statistics
                                          
               Accuracy : 0.404           
                 95% CI : (0.3734, 0.4352)
    No Information Rate : 0.281           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.1966          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 4
Sensitivity            0.4737  0.07373   0.4840   0.5339
Specificity            0.7371  0.93614   0.7622   0.7618
Pos Pred Value         0.3950  0.24242   0.4430   0.4091
Neg Pred Value         0.7944  0.78480   0.7908   0.8410
Prevalence             0.2660  0.21700   0.2810   0.2360
Detection Rate         0.1260  0.01600   0.1360   0.1260
Detection Prevalence   0.3190  0.06600   0.3070   0.3080
Balanced Accuracy      0.6054  0.50494   0.6231   0.6478
\end{verbatim}

逐步逻辑斯蒂回归

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{steplm10}\FloatTok{.7}\OtherTok{=}\FunctionTok{step}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=2576.96
CUSTCAT ~ MARITAL + ED + RETIRE + GENDER + AGE + ADDRESS + INCOME + 
    EMPLOY + RESIDE

trying - MARITAL 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.396701
iter  20 value 1285.425667
iter  30 value 1260.165550
final  value 1260.135972 
converged
trying - ED 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1356.159086
iter  20 value 1341.806273
iter  30 value 1332.698336
final  value 1332.694225 
converged
trying - RETIRE 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.306031
iter  20 value 1286.264926
iter  30 value 1260.002809
final  value 1259.874906 
converged
trying - GENDER 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.341804
iter  20 value 1284.148170
iter  30 value 1259.211215
final  value 1259.175389 
converged
trying - AGE 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1341.209977
iter  20 value 1276.190865
iter  30 value 1259.688920
final  value 1259.672694 
converged
trying - ADDRESS 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.613857
iter  20 value 1279.065635
iter  30 value 1264.448584
final  value 1264.400137 
converged
trying - INCOME 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1315.549004
iter  20 value 1277.089208
iter  30 value 1260.851613
final  value 1260.822358 
converged
trying - EMPLOY 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.695197
iter  20 value 1276.142108
iter  30 value 1264.059436
final  value 1264.023150 
converged
trying - RESIDE 
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1335.681699
iter  20 value 1276.653978
iter  30 value 1261.648713
final  value 1261.647477 
converged
          Df      AIC
- GENDER  27 2572.351
- AGE     27 2573.345
- RETIRE  27 2573.750
- MARITAL 27 2574.272
- INCOME  27 2575.645
<none>    30 2576.956
- RESIDE  27 2577.295
- EMPLOY  27 2582.046
- ADDRESS 27 2582.800
- ED      27 2719.388
# weights:  40 (27 variable)
initial  value 1386.294361 
iter  10 value 1334.341804
iter  20 value 1284.148170
iter  30 value 1259.211215
final  value 1259.175389 
converged

Step:  AIC=2572.35
CUSTCAT ~ MARITAL + ED + RETIRE + AGE + ADDRESS + INCOME + EMPLOY + 
    RESIDE

trying - MARITAL 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1334.446530
iter  20 value 1284.544906
iter  30 value 1260.846393
final  value 1260.846380 
converged
trying - ED 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1356.210541
iter  20 value 1336.820555
iter  30 value 1333.578660
iter  30 value 1333.578660
iter  30 value 1333.578660
final  value 1333.578660 
converged
trying - RETIRE 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1334.355893
iter  20 value 1285.651547
iter  30 value 1260.414269
final  value 1260.414237 
converged
trying - AGE 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1341.708354
iter  20 value 1272.158181
iter  30 value 1260.356157
iter  30 value 1260.356156
iter  30 value 1260.356156
final  value 1260.356156 
converged
trying - ADDRESS 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1334.639708
iter  20 value 1270.830750
iter  30 value 1265.151213
iter  30 value 1265.151209
iter  30 value 1265.151209
final  value 1265.151209 
converged
trying - INCOME 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1315.947147
iter  20 value 1267.187657
iter  30 value 1261.441148
iter  30 value 1261.441139
iter  30 value 1261.441139
final  value 1261.441139 
converged
trying - EMPLOY 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1334.718729
iter  20 value 1268.608327
iter  30 value 1264.654811
iter  30 value 1264.654809
iter  30 value 1264.654809
final  value 1264.654809 
converged
trying - RESIDE 
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1335.732183
iter  20 value 1269.899919
iter  30 value 1262.366002
final  value 1262.365987 
converged
          Df      AIC
- AGE     24 2568.712
- RETIRE  24 2568.828
- MARITAL 24 2569.693
- INCOME  24 2570.882
<none>    27 2572.351
- RESIDE  24 2572.732
- EMPLOY  24 2577.310
- ADDRESS 24 2578.302
- ED      24 2715.157
# weights:  36 (24 variable)
initial  value 1386.294361 
iter  10 value 1341.708354
iter  20 value 1272.158181
iter  30 value 1260.356157
iter  30 value 1260.356156
iter  30 value 1260.356156
final  value 1260.356156 
converged

Step:  AIC=2568.71
CUSTCAT ~ MARITAL + ED + RETIRE + ADDRESS + INCOME + EMPLOY + 
    RESIDE

trying - MARITAL 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1341.719585
iter  20 value 1264.847288
final  value 1261.703470 
converged
trying - ED 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1360.248908
iter  20 value 1336.920133
final  value 1334.648796 
converged
trying - RETIRE 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1341.747810
iter  20 value 1273.380970
final  value 1261.640402 
converged
trying - ADDRESS 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1334.774339
iter  20 value 1266.932488
final  value 1265.737152 
converged
trying - INCOME 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1334.786364
iter  20 value 1267.784070
final  value 1262.474510 
converged
trying - EMPLOY 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1340.615354
iter  20 value 1266.156732
final  value 1264.793645 
converged
trying - RESIDE 
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1339.057308
iter  20 value 1265.307243
final  value 1264.010139 
converged
          Df      AIC
- RETIRE  21 2565.281
- MARITAL 21 2565.407
- INCOME  21 2566.949
<none>    24 2568.712
- RESIDE  21 2570.020
- EMPLOY  21 2571.587
- ADDRESS 21 2573.474
- ED      21 2711.298
# weights:  32 (21 variable)
initial  value 1386.294361 
iter  10 value 1341.747810
iter  20 value 1273.380970
final  value 1261.640402 
converged

Step:  AIC=2565.28
CUSTCAT ~ MARITAL + ED + ADDRESS + INCOME + EMPLOY + RESIDE

trying - MARITAL 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1341.758566
iter  20 value 1268.359678
final  value 1263.071409 
converged
trying - ED 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1360.296393
iter  20 value 1336.636813
final  value 1335.424739 
converged
trying - ADDRESS 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1334.914943
iter  20 value 1267.709947
final  value 1266.554101 
converged
trying - INCOME 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1334.990507
iter  20 value 1268.757814
final  value 1263.288946 
converged
trying - EMPLOY 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1340.927790
iter  20 value 1268.257664
final  value 1266.847481 
converged
trying - RESIDE 
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1339.104681
iter  20 value 1266.084908
final  value 1265.320290 
converged
          Df      AIC
- MARITAL 18 2562.143
- INCOME  18 2562.578
<none>    21 2565.281
- RESIDE  18 2566.641
- ADDRESS 18 2569.108
- EMPLOY  18 2569.695
- ED      18 2706.849
# weights:  28 (18 variable)
initial  value 1386.294361 
iter  10 value 1341.758566
iter  20 value 1268.359678
final  value 1263.071409 
converged

Step:  AIC=2562.14
CUSTCAT ~ ED + ADDRESS + INCOME + EMPLOY + RESIDE

trying - ED 
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1359.874711
iter  20 value 1336.929483
final  value 1336.929421 
converged
trying - ADDRESS 
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1335.267844
iter  20 value 1268.820295
final  value 1268.819593 
converged
trying - INCOME 
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1335.399148
iter  20 value 1264.763551
final  value 1264.762550 
converged
trying - EMPLOY 
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1341.226881
iter  20 value 1268.367394
final  value 1268.367373 
converged
trying - RESIDE 
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1339.078551
iter  20 value 1271.031222
final  value 1271.030712 
converged
          Df      AIC
- INCOME  15 2559.525
<none>    18 2562.143
- EMPLOY  15 2566.735
- ADDRESS 15 2567.639
- RESIDE  15 2572.061
- ED      15 2703.859
# weights:  24 (15 variable)
initial  value 1386.294361 
iter  10 value 1335.399148
iter  20 value 1264.763551
final  value 1264.762550 
converged

Step:  AIC=2559.53
CUSTCAT ~ ED + ADDRESS + EMPLOY + RESIDE

trying - ED 
# weights:  20 (12 variable)
initial  value 1386.294361 
iter  10 value 1353.639788
final  value 1346.567152 
converged
trying - ADDRESS 
# weights:  20 (12 variable)
initial  value 1386.294361 
iter  10 value 1303.061277
final  value 1270.502065 
converged
trying - EMPLOY 
# weights:  20 (12 variable)
initial  value 1386.294361 
iter  10 value 1307.733970
final  value 1278.856480 
converged
trying - RESIDE 
# weights:  20 (12 variable)
initial  value 1386.294361 
iter  10 value 1326.391057
final  value 1272.616140 
converged
          Df      AIC
<none>    15 2559.525
- ADDRESS 12 2565.004
- RESIDE  12 2569.232
- EMPLOY  12 2581.713
- ED      12 2717.134
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(steplm10}\FloatTok{.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
multinom(formula = CUSTCAT ~ ED + ADDRESS + EMPLOY + RESIDE, 
    data = data10.7)

Coefficients:
  (Intercept)          ED    ADDRESS     EMPLOY     RESIDE
2  -2.5817157  0.52590942 0.03640258 0.02643184 0.14685141
3  -0.8218691 -0.05205386 0.02097994 0.05140033 0.08522531
4  -3.7656579  0.84517714 0.02082581 0.04156680 0.26081610

Std. Errors:
  (Intercept)         ED    ADDRESS     EMPLOY     RESIDE
2   0.3587024 0.08448205 0.01092228 0.01162357 0.06875362
3   0.3132980 0.08213584 0.01014918 0.01037283 0.06529354
4   0.3866772 0.08802761 0.01164240 0.01196797 0.06766854

Residual Deviance: 2529.525 
AIC: 2559.525 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{steppre\_mult }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(steplm10}\FloatTok{.7}\NormalTok{)}
\NormalTok{stepconMat\_mult }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{factor}\NormalTok{(steppre\_mult), }\FunctionTok{factor}\NormalTok{(data10}\FloatTok{.7}\SpecialCharTok{$}\NormalTok{CUSTCAT))}
\NormalTok{stepconMat\_mult}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   1   2   3   4
         1 127  59  86  51
         2   7   9  10  13
         3  72  64 138  40
         4  60  85  47 132

Overall Statistics
                                          
               Accuracy : 0.406           
                 95% CI : (0.3754, 0.4372)
    No Information Rate : 0.281           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.1983          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 4
Sensitivity            0.4774  0.04147   0.4911   0.5593
Specificity            0.7330  0.96169   0.7552   0.7487
Pos Pred Value         0.3932  0.23077   0.4395   0.4074
Neg Pred Value         0.7947  0.78356   0.7915   0.8462
Prevalence             0.2660  0.21700   0.2810   0.2360
Detection Rate         0.1270  0.00900   0.1380   0.1320
Detection Prevalence   0.3230  0.03900   0.3140   0.3240
Balanced Accuracy      0.6052  0.50158   0.6232   0.6540
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre\_mult }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm10}\FloatTok{.7}\NormalTok{)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(lattice)}
\NormalTok{conMat\_mult }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{factor}\NormalTok{(pre\_mult), }\FunctionTok{factor}\NormalTok{(data10}\FloatTok{.7}\SpecialCharTok{$}\NormalTok{CUSTCAT))}
\NormalTok{conMat\_mult}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   1   2   3   4
         1 126  58  82  53
         2  11  16  18  21
         3  73  62 136  36
         4  56  81  45 126

Overall Statistics
                                          
               Accuracy : 0.404           
                 95% CI : (0.3734, 0.4352)
    No Information Rate : 0.281           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.1966          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 4
Sensitivity            0.4737  0.07373   0.4840   0.5339
Specificity            0.7371  0.93614   0.7622   0.7618
Pos Pred Value         0.3950  0.24242   0.4430   0.4091
Neg Pred Value         0.7944  0.78480   0.7908   0.8410
Prevalence             0.2660  0.21700   0.2810   0.2360
Detection Rate         0.1260  0.01600   0.1360   0.1260
Detection Prevalence   0.3190  0.06600   0.3070   0.3080
Balanced Accuracy      0.6054  0.50494   0.6231   0.6478
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlogit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: dfidx
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"Fishing"}\NormalTok{,}\AttributeTok{package=}\StringTok{"mlogit"}\NormalTok{)}
\NormalTok{fish}\OtherTok{=}\FunctionTok{mlogit.data}\NormalTok{(Fishing,}\AttributeTok{varying=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{9}\NormalTok{),}\AttributeTok{shape=}\StringTok{"wide"}\NormalTok{,}\AttributeTok{choice=}\StringTok{"mode"}\NormalTok{)}
\NormalTok{mmodel}\OtherTok{=}\FunctionTok{mlogit}\NormalTok{(mode}\SpecialCharTok{\textasciitilde{}}\DecValTok{0}\SpecialCharTok{|}\NormalTok{income,}\AttributeTok{data=}\NormalTok{fish)}
\FunctionTok{summary}\NormalTok{(mmodel)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
mlogit(formula = mode ~ 0 | income, data = fish, method = "nr")

Frequencies of alternatives:choice
  beach    boat charter    pier 
0.11337 0.35364 0.38240 0.15059 

nr method
4 iterations, 0h:0m:0s 
g'(-H)^-1g = 8.32E-07 
gradient close to zero 

Coefficients :
                       Estimate  Std. Error z-value  Pr(>|z|)    
(Intercept):boat     7.3892e-01  1.9673e-01  3.7560 0.0001727 ***
(Intercept):charter  1.3413e+00  1.9452e-01  6.8955 5.367e-12 ***
(Intercept):pier     8.1415e-01  2.2863e-01  3.5610 0.0003695 ***
income:boat          9.1906e-05  4.0664e-05  2.2602 0.0238116 *  
income:charter      -3.1640e-05  4.1846e-05 -0.7561 0.4495908    
income:pier         -1.4340e-04  5.3288e-05 -2.6911 0.0071223 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Log-Likelihood: -1477.2
McFadden R^2:  0.013736 
Likelihood ratio test : chisq = 41.145 (p.value = 6.0931e-09)
\end{verbatim}

\section{10.6
因变量顺序数据回归}\label{ux56e0ux53d8ux91cfux987aux5e8fux6570ux636eux56deux5f52}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)  }\CommentTok{\#读取spss文件包}
\NormalTok{data10}\FloatTok{.8}\OtherTok{=}\FunctionTok{read\_sav}\NormalTok{(}\StringTok{"C:/Users/lievi/OneDrive/RPS/R/datake/li10.8 german\_credit 有序样本量.sav"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(data10}\FloatTok{.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [1,000 x 20] (S3: tbl_df/tbl/data.frame)
 $ DURATION: num [1:1000] 6 48 12 42 24 36 24 36 12 30 ...
  ..- attr(*, "label")= chr "Duration in months"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ CAMT    : num [1:1000] 1169 5951 2096 7882 4870 ...
  ..- attr(*, "label")= chr "Credit amount"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ INSTRATE: num [1:1000] 4 2 2 2 3 2 3 2 2 4 ...
  ..- attr(*, "label")= chr "Installment rate"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ RESIDLEN: num [1:1000] 4 2 3 4 4 4 4 2 4 2 ...
  ..- attr(*, "label")= chr "Residence length"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ AGE     : num [1:1000] 67 22 49 45 53 35 53 35 61 28 ...
  ..- attr(*, "label")= chr "Age in years"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ NUMCRED : num [1:1000] 2 1 1 1 2 1 1 1 1 2 ...
  ..- attr(*, "label")= chr "# of existing credits"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ NUMLIAB : num [1:1000] 1 1 2 2 2 2 1 1 1 1 ...
  ..- attr(*, "label")= chr "# who will be liable"
  ..- attr(*, "format.spss")= chr "F11.2"
 $ CHKS    : dbl+lbl [1:1000] 1, 2, 4, 1, 1, 4, 4, 2, 4, 2, 2, 1, 2, 1, 1, 1, 4, 1,...
   ..@ label      : chr "Checking status"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:4] 1 2 3 4
   .. ..- attr(*, "names")= chr [1:4] "LT 0 DM" "0 - 200DM" ">200 DM" "NO CHKG"
 $ CHIST   : dbl+lbl [1:1000] 5, 3, 5, 3, 4, 3, 3, 3, 3, 5, 3, 3, 3, 5, 3, 3, 5, 1,...
   ..@ label      : chr "Account status"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:5] 1 2 3 4 5
   .. ..- attr(*, "names")= chr [1:5] "No debt history" "No current debt" "Payments current" "Payments delayed" ...
 $ REASON  : dbl+lbl [1:1000]  4,  4,  7,  3,  1,  7,  3,  2,  4,  1,  1, 10,  4,  ...
   ..@ label      : chr "Reason for loan"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:11] 1 2 3 4 5 6 7 8 9 10 ...
   .. ..- attr(*, "names")= chr [1:11] "NEW CAR" "USD CAR" "FURN" "RAD/TV" ...
 $ SAVNGS  : dbl+lbl [1:1000] 5, 1, 1, 1, 1, 5, 3, 1, 4, 1, 1, 1, 1, 1, 1, 2, 5, 5,...
   ..@ label      : chr "Saving plus bonds"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:5] 1 2 3 4 5
   .. ..- attr(*, "names")= chr [1:5] "LT 100DM" "100 - 500DM" "500-IK DM" "> 1K DM" ...
 $ LENEMP  : dbl+lbl [1:1000] 4, 2, 3, 3, 2, 2, 4, 2, 3, 5, 1, 1, 2, 4, 2, 2, 4, 1,...
   ..@ label      : chr "Length employed"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:5] 1 2 3 4 5
   .. ..- attr(*, "names")= chr [1:5] "LT 1YR" "1-4 YRS" "4-7 YRS" "GT 7 YRS" ...
 $ PERSTAT : dbl+lbl [1:1000] 3, 2, 3, 3, 3, 3, 3, 3, 1, 4, 2, 2, 2, 3, 2, 2, 3, 3,...
   ..@ label      : chr "Personal status"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:5] 1 2 3 4 5
   .. ..- attr(*, "names")= chr [1:5] "M DIV/SEP" "F DIV/SEP/MAR" "M SINGLE" "M MAR" ...
 $ OTHDEBT : dbl+lbl [1:1000] 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
   ..@ label      : chr "Other debtors"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:3] 1 2 3
   .. ..- attr(*, "names")= chr [1:3] "NONE" "CO-APP" "GURANTOR"
 $ PRPOWNR : dbl+lbl [1:1000] 1, 1, 1, 2, 4, 4, 2, 3, 1, 3, 3, 2, 3, 3, 3, 3, 2, 3,...
   ..@ label      : chr "Property owner"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:4] 1 2 3 4
   .. ..- attr(*, "names")= chr [1:4] "Real Estate" "BLD SAV/LIFE INS" "CAR OR OTHER" "Unknown/None"
 $ OTHNSTAL: dbl+lbl [1:1000] 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,...
   ..@ label      : chr "Other installment debts"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:3] 1 2 3
   .. ..- attr(*, "names")= chr [1:3] "BANK" "STORES" "NONE"
 $ HOUSNG  : dbl+lbl [1:1000] 2, 2, 2, 3, 3, 3, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2,...
   ..@ label      : chr "Housing"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:3] 1 2 3
   .. ..- attr(*, "names")= chr [1:3] "RENT" "OWN" "FREE"
 $ EMPTYPE : dbl+lbl [1:1000] 3, 3, 2, 3, 3, 2, 3, 4, 2, 4, 3, 3, 3, 2, 3, 2, 3, 3,...
   ..@ label      : chr "Employment type"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:4] 1 2 3 4
   .. ..- attr(*, "names")= chr [1:4] "UEMP/ UNSKL NR" "UNSKL RESIDENT" "SKL EMP/OFFICIAL" "MGT / SELF-EMP"
 $ TELEPHNE: dbl+lbl [1:1000] 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,...
   ..@ label      : chr "Telephone indicator"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:2] 0 1
   .. ..- attr(*, "names")= chr [1:2] "NO" "YES"
 $ FORWORKR: dbl+lbl [1:1000] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
   ..@ label      : chr "Foreign worker"
   ..@ format.spss: chr "F8.2"
   ..@ labels     : Named num [1:2] 0 1
   .. ..- attr(*, "names")= chr [1:2] "NO" "YES"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data10}\FloatTok{.8}\SpecialCharTok{$}\NormalTok{CHIST}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(data10}\FloatTok{.8}\SpecialCharTok{$}\NormalTok{CHIST)}
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{lm10}\FloatTok{.8}\OtherTok{=}\FunctionTok{polr}\NormalTok{(CHIST}\SpecialCharTok{\textasciitilde{}}\NormalTok{AGE}\SpecialCharTok{+}\NormalTok{DURATION}\SpecialCharTok{+}\NormalTok{NUMCRED}\SpecialCharTok{+}\NormalTok{OTHNSTAL}\SpecialCharTok{+}\NormalTok{HOUSNG,}\AttributeTok{data=}\NormalTok{data10}\FloatTok{.8}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm10}\FloatTok{.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Re-fitting to get Hessian
\end{verbatim}

\begin{verbatim}
Call:
polr(formula = CHIST ~ AGE + DURATION + NUMCRED + OTHNSTAL + 
    HOUSNG, data = data10.8)

Coefficients:
            Value Std. Error t value
AGE       0.01391   0.006097   2.281
DURATION -0.01095   0.005527  -1.982
NUMCRED   2.08086   0.141859  14.668
OTHNSTAL  0.54465   0.095468   5.705
HOUSNG    0.15874   0.132291   1.200

Intercepts:
    Value   Std. Error t value
1|2  1.2014  0.4519     2.6585
2|3  2.0742  0.4394     4.7203
3|4  5.4547  0.4690    11.6306
4|5  6.0252  0.4778    12.6112

Residual Deviance: 2060.556 
AIC: 2078.556 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre\_lm10}\FloatTok{.8} \OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm10}\FloatTok{.8}\NormalTok{)}
\NormalTok{conMat\_lm10}\FloatTok{.8} \OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{factor}\NormalTok{(pre\_lm10}\FloatTok{.8}\NormalTok{), }\FunctionTok{factor}\NormalTok{(data10}\FloatTok{.8}\SpecialCharTok{$}\NormalTok{CHIST))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in levels(reference) != levels(data): longer object length is not a
multiple of shorter object length
\end{verbatim}

\begin{verbatim}
Warning in confusionMatrix.default(factor(pre_lm10.8), factor(data10.8$CHIST)):
Levels are not in the same order for reference and data. Refactoring data to
match.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conMat\_lm10}\FloatTok{.8}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   1   2   3   4   5
         1   0   0   0   0   0
         2   0   0   0   0   0
         3  18  45 488  42  91
         4   0   0   0   0   0
         5  22   4  42  46 202

Overall Statistics
                                          
               Accuracy : 0.69            
                 95% CI : (0.6603, 0.7186)
    No Information Rate : 0.53            
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.4311          
                                          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5
Sensitivity              0.00    0.000   0.9208    0.000   0.6894
Specificity              1.00    1.000   0.5830    1.000   0.8388
Pos Pred Value            NaN      NaN   0.7135      NaN   0.6392
Neg Pred Value           0.96    0.951   0.8671    0.912   0.8670
Prevalence               0.04    0.049   0.5300    0.088   0.2930
Detection Rate           0.00    0.000   0.4880    0.000   0.2020
Detection Prevalence     0.00    0.000   0.6840    0.000   0.3160
Balanced Accuracy        0.50    0.500   0.7519    0.500   0.7641
\end{verbatim}

\part{进阶}

\chapter{Hello, Quarto}\label{hello-quarto-8}

\chapter{11 机器学习回归}\label{ux673aux5668ux5b66ux4e60ux56deux5f52}

\chapter{Hello, Quarto}\label{hello-quarto-9}

\chapter{分类}\label{ux5206ux7c7b}




\end{document}
